{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/joongho/Intelligent_Radar/gui', '/home/joongho/.conda/envs/radar/lib/python37.zip', '/home/joongho/.conda/envs/radar/lib/python3.7', '/home/joongho/.conda/envs/radar/lib/python3.7/lib-dynload', '', '/home/joongho/.local/lib/python3.7/site-packages', '/home/joongho/.conda/envs/radar/lib/python3.7/site-packages', '/home/joongho/.conda/envs/radar/lib/python3.7/site-packages/IPython/extensions', '/home/joongho/.ipython', '/home/joongho/Intelligent_Radar/gui/ml', '/home/joongho/Intelligent_Radar']\n",
      "Tensorflow version 2.7.0 detected\n",
      "Tensorflow version 2.7.0 detected\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "import natsort\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pandas import DataFrame as DF\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# sys.path.append(os.path.realpath(os.path.join('C:\\\\acconeer\\\\gui_tool\\\\acconeer-python-exploration\\\\gui\\\\comparision.ipynb', \"../\")))\n",
    "# sys.path.append('C:\\\\acconeer\\\\gui_tool\\\\acconeer-python-exploration\\\\gui\\\\ml')\n",
    "# sys.path.append('C:\\\\acconeer\\\\gui_tool\\\\acconeer-python-exploration')\n",
    "sys.path.append('/home/joongho/Intelligent_Radar/gui/ml')\n",
    "sys.path.append('/home/joongho/Intelligent_Radar')\n",
    "print(sys.path)\n",
    "# from acconeer.exptool import configs\n",
    "try:\n",
    "    # from acconeer.exptool import imock, utils\n",
    "    from gui.ml import feature_processing as feature_proc  \n",
    "    from gui.ml import keras_processing as kp\n",
    "except Exception:\n",
    "    print(\"Failed to import deeplearning libraries, please specify acconeer-exploration-folder!\")\n",
    "    exit(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_dir = 'C:\\\\acconeer\\\\gui_tool\\\\acconeer-python-exploration\\\\data\\\\2021_10_25'\n",
    "path_dir = '/home/joongho/Intelligent_Radar/data/2021_11//Train'\n",
    " \n",
    "file_list = os.listdir(path_dir)\n",
    "file_list = natsort.natsorted(file_list)\n",
    "file_path = list()\n",
    "for file in range(len(file_list)):\n",
    "    file_path.append(path_dir + '/' + file_list[file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Can1.npy', 'Can2.npy', 'Can3.npy', 'Can4.npy', 'Can5.npy', 'Can6.npy', 'Can7.npy', 'Can8.npy', 'Can9.npy', 'Glass1.npy', 'Glass2.npy', 'Glass3.npy', 'Glass4.npy', 'Glass5.npy', 'Glass6.npy', 'Glass7.npy', 'Glass8.npy', 'Glass9.npy', 'Paper1.npy', 'Paper2.npy', 'Paper3.npy', 'Paper4.npy', 'Paper5.npy', 'Paper6.npy', 'Paper7.npy', 'Paper8.npy', 'Paper9.npy', 'Plastic1.npy', 'Plastic2.npy', 'Plastic3.npy', 'Plastic4.npy', 'Plastic5.npy', 'Plastic6.npy', 'Plastic7.npy', 'Plastic8.npy', 'Plastic9.npy']\n"
     ]
    }
   ],
   "source": [
    "print(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_list = list()\n",
    "for file in file_list:\n",
    "    for i in range(30):\n",
    "        file_name_list.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "\n",
      "Initiating model with 4x1 inputs and 4 outputs\n",
      "No layers defined!\n",
      "['/home/joongho/Intelligent_Radar/data/2021_11//Train/Can1.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Train/Can2.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Train/Can3.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Train/Can4.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Train/Can5.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Train/Can6.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Train/Can7.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Train/Can8.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Train/Can9.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Train/Glass1.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Train/Glass2.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Train/Glass3.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Train/Glass4.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Train/Glass5.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Train/Glass6.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Train/Glass7.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Train/Glass8.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Train/Glass9.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Train/Paper1.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Train/Paper2.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Train/Paper3.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Train/Paper4.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Train/Paper5.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Train/Paper6.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Train/Paper7.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Train/Paper8.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Train/Paper9.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Train/Plastic1.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Train/Plastic2.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Train/Plastic3.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Train/Plastic4.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Train/Plastic5.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Train/Plastic6.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Train/Plastic7.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Train/Plastic8.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Train/Plastic9.npy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 17:53:31.267607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-02 17:53:31.292264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-02 17:53:31.292480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-02 17:53:31.293087: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-02 17:53:31.294562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-02 17:53:31.294737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-02 17:53:31.294909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-02 17:53:31.763323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-02 17:53:31.763504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-02 17:53:31.763644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-02 17:53:31.763772: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6106 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:26:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "ML = kp.MachineLearning()\n",
    "data = ML.load_train_data(file_path)\n",
    "print(file_path)\n",
    "training_data = ML.training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/joongho/Intelligent_Radar/data/2021_11//Test/ml_session_data_2021_11_29_2220_Can1.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Test/ml_session_data_2021_11_29_2214_Can2.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Test/ml_session_data_2021_11_29_2222_Paper1.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Test/ml_session_data_2021_11_29_2225_Plastic2.npy', '/home/joongho/Intelligent_Radar/data/2021_11//Test/ml_session_data_2021_11_29_2217_Plastic1.npy']\n"
     ]
    }
   ],
   "source": [
    "test_path = '/home/joongho/Intelligent_Radar/data/2021_11//Test'\n",
    "test_file_list = os.listdir(test_path)\n",
    "test_file_path = list()\n",
    "for file in range(len(test_file_list)):\n",
    "    test_file_path.append(test_path + '/' + test_file_list[file])\n",
    "print(test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "\n",
      "Initiating model with 4x1 inputs and 3 outputs\n",
      "No layers defined!\n"
     ]
    }
   ],
   "source": [
    "test_ML = kp.MachineLearning()\n",
    "test_data_all = test_ML.load_train_data(test_file_path)\n",
    "test_data = test_ML.training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(matrix,title):\n",
    "    df=DF(matrix,index=[\"Can\",\"Glass\",\"Paper\",\"Plastic\"],columns=[\"Can\",\"Glass\",\"Paper\",\"Plastic\"])\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.heatmap(df, annot=True)\n",
    "    plt.tick_params(axis='x', top=True, labeltop = True,bottom=False, labelbottom=False)\n",
    "    plt.xticks(np.arange(0.5, len(df.columns), 1), df.columns)\n",
    "    plt.yticks(np.arange(0.5, len(df.index), 1), df.index)\n",
    "    plt.xlabel(\"Prediction\",position = (0.5,1.0+0.05))\n",
    "    plt.ylabel(\"Object\")\n",
    "    plt.title(title) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization\n",
    "def normalization(arr):\n",
    "    normalized_arr = np.zeros_like(arr)\n",
    "    max = np.zeros(len(arr[0]))\n",
    "    min = np.zeros(len(arr[0]))\n",
    "    for i in range(len(arr[0])):\n",
    "        for j in range(len(arr)):\n",
    "            if arr[j][i] > max[i]:\n",
    "                max[i] = arr[j][i]\n",
    "            if arr[j][i] < min[i]:\n",
    "                min[i] = arr[j][i]\n",
    "    for i in range(len(arr[0])):\n",
    "        for j in range(len(arr)):\n",
    "            normalized_arr[j][i] = (arr[j][i] - min[i])/(max[i]-min[i])\n",
    "    return normalized_arr, min, max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization\n",
    "def normalization_test(arr, min, max):\n",
    "    normalized_arr = np.zeros_like(arr)\n",
    "    for i in range(len(arr[0])):\n",
    "        for j in range(len(arr)):\n",
    "            normalized_arr[j][i] = (arr[j][i] - min[i])/(max[i]-min[i])\n",
    "    return normalized_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_converter\n",
    "def label_convert(labels):\n",
    "    converted_labels = np.zeros(len(labels))\n",
    "    for i in range(len(labels)):\n",
    "        for idx in range(len(labels[0])):\n",
    "            if labels[i][idx] == 1:\n",
    "                converted_labels[i] = idx\n",
    "            else:\n",
    "                pass\n",
    "    return converted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Can': 0, 'Glass': 1, 'Paper': 2, 'Plastic': 3}\n"
     ]
    }
   ],
   "source": [
    "print(ML.labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = training_data['x_data']\n",
    "X_test = test_data['x_data']\n",
    "Y_train = data['model_data']['y_labels']\n",
    "Y_test = test_data_all['model_data']['y_labels']\n",
    "X_train_nn, min, max = normalization(X_train)\n",
    "# X_train_nn = normalization(X_train)\n",
    "X_test_nn = normalization_test(X_test, min, max)\n",
    "# X_test_nn = normalization(X_test)\n",
    "Y_train_cf = label_convert(Y_train)\n",
    "Y_test_cf = label_convert(Y_test)\n",
    "X_train = np.reshape(X_train, (len(X_train), len(X_train[0])))\n",
    "X_test = np.reshape(X_test, (len(X_test), len(X_test[0])))\n",
    "X_train_nn = np.reshape(X_train_nn, (len(X_train_nn), len(X_train_nn[0])))\n",
    "X_test_nn = np.reshape(X_test_nn, (len(X_test_nn), len(X_test_nn[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['Avg.Amp', 'Avg.total', 'Global.Max', 'Global.Min']\n",
    "data_df = pd.DataFrame(X_train, columns=column_names)\n",
    "data_df['Label'] = Y_train_cf\n",
    "data_df['File_name'] = file_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Avg.Amp  Avg.total  Global.Max  Global.Min  Label     File_name\n",
      "0     -126.08  -30503.56         0.0    -65542.0    0.0      Can1.npy\n",
      "1     -120.64  -25937.42      7705.0    -51350.0    0.0      Can1.npy\n",
      "2     -120.00  -28216.28      7705.0    -57277.0    0.0      Can1.npy\n",
      "3     -123.30  -26709.78     10401.0    -56379.0    0.0      Can1.npy\n",
      "4     -119.98  -24479.84      3725.0    -56056.0    0.0      Can1.npy\n",
      "...       ...        ...         ...         ...    ...           ...\n",
      "1075   -13.90   55350.04     78463.0         0.0    3.0  Plastic9.npy\n",
      "1076   -14.78   53411.10     76813.0         0.0    3.0  Plastic9.npy\n",
      "1077   -15.86   56394.46     76654.0         0.0    3.0  Plastic9.npy\n",
      "1078   -10.12   59070.34     79050.0         0.0    3.0  Plastic9.npy\n",
      "1079   -13.86   56735.62     83622.0         0.0    3.0  Plastic9.npy\n",
      "\n",
      "[1080 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_Avg_Amp = (data_df.loc[:,'Avg.Amp'] - data_df.loc[:,'Avg.Amp'].min())/(data_df.loc[:,'Avg.Amp'].max()-data_df.loc[:,'Avg.Amp'].min())\n",
    "norm_Avg_total = (data_df.loc[:,'Avg.total'] - data_df.loc[:,'Avg.total'].min())/(data_df.loc[:,'Avg.total'].max()-data_df.loc[:,'Avg.total'].min())\n",
    "norm_Glob_min = (data_df.loc[:,'Avg.total'] - data_df.loc[:,'Avg.total'].min())/(data_df.loc[:,'Avg.total'].max()-data_df.loc[:,'Avg.total'].min())\n",
    "norm_Glob_max = (data_df.loc[:,'Global.Max'] - data_df.loc[:,'Global.Max'].min())/(data_df.loc[:,'Avg.total'].max()-data_df.loc[:,'Avg.total'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['Avg.Amp.norm'] = norm_Avg_Amp\n",
    "data_df['Avg.total.norm'] = norm_Avg_total\n",
    "data_df['Glob.Max'] = norm_Glob_max\n",
    "data_df['Glob.Min'] = norm_Glob_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Avg.Amp  Avg.total  Global.Max  Global.Min  Label     File_name  \\\n",
      "0     -126.08  -30503.56         0.0    -65542.0    0.0      Can1.npy   \n",
      "1     -120.64  -25937.42      7705.0    -51350.0    0.0      Can1.npy   \n",
      "2     -120.00  -28216.28      7705.0    -57277.0    0.0      Can1.npy   \n",
      "3     -123.30  -26709.78     10401.0    -56379.0    0.0      Can1.npy   \n",
      "4     -119.98  -24479.84      3725.0    -56056.0    0.0      Can1.npy   \n",
      "...       ...        ...         ...         ...    ...           ...   \n",
      "1075   -13.90   55350.04     78463.0         0.0    3.0  Plastic9.npy   \n",
      "1076   -14.78   53411.10     76813.0         0.0    3.0  Plastic9.npy   \n",
      "1077   -15.86   56394.46     76654.0         0.0    3.0  Plastic9.npy   \n",
      "1078   -10.12   59070.34     79050.0         0.0    3.0  Plastic9.npy   \n",
      "1079   -13.86   56735.62     83622.0         0.0    3.0  Plastic9.npy   \n",
      "\n",
      "      Avg.Amp.norm  Avg.total.norm  Glob.Max  Glob.Min  \n",
      "0         0.166113        0.438642  0.000000  0.438642  \n",
      "1         0.176384        0.445257  0.011163  0.445257  \n",
      "2         0.177592        0.441956  0.011163  0.441956  \n",
      "3         0.171362        0.444138  0.015068  0.444138  \n",
      "4         0.177630        0.447369  0.005397  0.447369  \n",
      "...            ...             ...       ...       ...  \n",
      "1075      0.377917        0.563021  0.113672  0.563021  \n",
      "1076      0.376256        0.560212  0.111282  0.560212  \n",
      "1077      0.374216        0.564535  0.111052  0.564535  \n",
      "1078      0.385054        0.568411  0.114523  0.568411  \n",
      "1079      0.377993        0.565029  0.121147  0.565029  \n",
      "\n",
      "[1080 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_df = pd.DataFrame()\n",
    "train_data_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Can1_data_df = data_df[data_df['File_name']=='Can1.npy']\n",
    "Can1_test_data_df = Can1_data_df.iloc[:10, :]\n",
    "Can1_train_data_df = Can1_data_df.iloc[10:, :]\n",
    "Can2_data_df = data_df[data_df['File_name']=='Can2.npy']\n",
    "Can2_test_data_df = Can2_data_df.iloc[:10, :]\n",
    "Can2_train_data_df = Can2_data_df.iloc[10:, :]\n",
    "Can3_data_df = data_df[data_df['File_name']=='Can3.npy']\n",
    "Can3_test_data_df = Can3_data_df.iloc[:10, :]\n",
    "Can3_train_data_df = Can3_data_df.iloc[10:, :]\n",
    "Can4_data_df = data_df[data_df['File_name']=='Can4.npy']\n",
    "Can4_test_data_df = Can4_data_df.iloc[:10, :]\n",
    "Can4_train_data_df = Can4_data_df.iloc[10:, :]\n",
    "Can5_data_df = data_df[data_df['File_name']=='Can5.npy']\n",
    "Can5_test_data_df = Can5_data_df.iloc[:10, :]\n",
    "Can5_train_data_df = Can5_data_df.iloc[10:, :]\n",
    "Can6_data_df = data_df[data_df['File_name']=='Can6.npy']\n",
    "Can6_test_data_df = Can6_data_df.iloc[:10, :]\n",
    "Can6_train_data_df = Can6_data_df.iloc[10:, :]\n",
    "Glass1_data_df = data_df[data_df['File_name']=='Glass1.npy']\n",
    "Glass1_test_data_df = Glass1_data_df.iloc[:10, :]\n",
    "Glass1_train_data_df = Glass1_data_df.iloc[10:, :]\n",
    "Glass2_data_df = data_df[data_df['File_name']=='Glass2.npy']\n",
    "Glass2_test_data_df = Glass2_data_df.iloc[:10, :]\n",
    "Glass2_train_data_df = Glass2_data_df.iloc[10:, :]\n",
    "Glass3_data_df = data_df[data_df['File_name']=='Glass3.npy']\n",
    "Glass3_test_data_df = Glass3_data_df.iloc[:10, :]\n",
    "Glass3_train_data_df = Glass3_data_df.iloc[10:, :]\n",
    "Plastic1_data_df = data_df[data_df['File_name']=='Plastic1.npy']\n",
    "Plastic1_test_data_df = Plastic1_data_df.iloc[:10, :]\n",
    "Plastic1_train_data_df = Plastic1_data_df.iloc[10:, :]\n",
    "Plastic2_data_df = data_df[data_df['File_name']=='Plastic2.npy']\n",
    "Plastic2_test_data_df = Plastic2_data_df.iloc[:10, :]\n",
    "Plastic2_train_data_df = Plastic2_data_df.iloc[10:, :]\n",
    "Plastic3_data_df = data_df[data_df['File_name']=='Plastic3.npy']\n",
    "Plastic3_test_data_df = Plastic3_data_df.iloc[:10, :]\n",
    "Plastic3_train_data_df = Plastic3_data_df.iloc[10:, :]\n",
    "Paper1_data_df = data_df[data_df['File_name']=='Paper1.npy']\n",
    "Paper1_test_data_df = Paper1_data_df.iloc[:10, :]\n",
    "Paper1_train_data_df = Paper1_data_df.iloc[10:, :]\n",
    "Paper2_data_df = data_df[data_df['File_name']=='Paper2.npy']\n",
    "Paper2_test_data_df = Paper2_data_df.iloc[:10, :]\n",
    "Paper2_train_data_df = Paper2_data_df.iloc[10:, :]\n",
    "Paper3_data_df = data_df[data_df['File_name']=='Paper3.npy']\n",
    "Paper3_test_data_df = Paper3_data_df.iloc[:10, :]\n",
    "Paper3_train_data_df = Paper3_data_df.iloc[10:, :]\n",
    "test_data_df = test_data_df.append([Can1_test_data_df,\n",
    "                                    Can2_test_data_df,\n",
    "                                    Can3_test_data_df,\n",
    "                                    Can4_test_data_df,\n",
    "                                    Can5_test_data_df,\n",
    "                                    Can6_test_data_df,\n",
    "                                    Glass1_test_data_df,\n",
    "                                    Glass2_test_data_df,\n",
    "                                    Glass3_test_data_df,\n",
    "                                    Plastic1_test_data_df,\n",
    "                                    Plastic2_test_data_df,\n",
    "                                    Plastic3_test_data_df,\n",
    "                                    Paper1_test_data_df,\n",
    "                                    Paper2_test_data_df,\n",
    "                                    Paper3_test_data_df\n",
    "                                    ], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df = train_data_df.append([Can1_train_data_df,\n",
    "                                    Can2_train_data_df,\n",
    "                                    Can3_train_data_df,\n",
    "                                    Can4_train_data_df,\n",
    "                                    Can5_train_data_df,\n",
    "                                    Can6_train_data_df,\n",
    "                                    Glass1_train_data_df,\n",
    "                                    Glass2_train_data_df,\n",
    "                                    Glass3_train_data_df,\n",
    "                                    Plastic1_train_data_df,\n",
    "                                    Plastic2_train_data_df,\n",
    "                                    Plastic3_train_data_df,\n",
    "                                    Paper1_train_data_df,\n",
    "                                    Paper2_train_data_df,\n",
    "                                    Paper3_train_data_df\n",
    "                                    ], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avg.Amp</th>\n",
       "      <th>Avg.total</th>\n",
       "      <th>Global.Max</th>\n",
       "      <th>Global.Min</th>\n",
       "      <th>Label</th>\n",
       "      <th>File_name</th>\n",
       "      <th>Avg.Amp.norm</th>\n",
       "      <th>Avg.total.norm</th>\n",
       "      <th>Glob.Max</th>\n",
       "      <th>Glob.Min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-126.08</td>\n",
       "      <td>-30503.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-65542.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Can1.npy</td>\n",
       "      <td>0.166113</td>\n",
       "      <td>0.438642</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.438642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-120.64</td>\n",
       "      <td>-25937.42</td>\n",
       "      <td>7705.0</td>\n",
       "      <td>-51350.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Can1.npy</td>\n",
       "      <td>0.176384</td>\n",
       "      <td>0.445257</td>\n",
       "      <td>0.011163</td>\n",
       "      <td>0.445257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-120.00</td>\n",
       "      <td>-28216.28</td>\n",
       "      <td>7705.0</td>\n",
       "      <td>-57277.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Can1.npy</td>\n",
       "      <td>0.177592</td>\n",
       "      <td>0.441956</td>\n",
       "      <td>0.011163</td>\n",
       "      <td>0.441956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-123.30</td>\n",
       "      <td>-26709.78</td>\n",
       "      <td>10401.0</td>\n",
       "      <td>-56379.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Can1.npy</td>\n",
       "      <td>0.171362</td>\n",
       "      <td>0.444138</td>\n",
       "      <td>0.015068</td>\n",
       "      <td>0.444138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-119.98</td>\n",
       "      <td>-24479.84</td>\n",
       "      <td>3725.0</td>\n",
       "      <td>-56056.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Can1.npy</td>\n",
       "      <td>0.177630</td>\n",
       "      <td>0.447369</td>\n",
       "      <td>0.005397</td>\n",
       "      <td>0.447369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>-46.30</td>\n",
       "      <td>-32929.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-43963.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Paper3.npy</td>\n",
       "      <td>0.316743</td>\n",
       "      <td>0.435127</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.435127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>-45.86</td>\n",
       "      <td>-33961.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-42700.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Paper3.npy</td>\n",
       "      <td>0.317574</td>\n",
       "      <td>0.433632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.433632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>-48.12</td>\n",
       "      <td>-32811.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-40858.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Paper3.npy</td>\n",
       "      <td>0.313307</td>\n",
       "      <td>0.435299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.435299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>-47.06</td>\n",
       "      <td>-33289.42</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-44697.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Paper3.npy</td>\n",
       "      <td>0.315309</td>\n",
       "      <td>0.434606</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.434606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>-46.22</td>\n",
       "      <td>-34181.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-45131.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Paper3.npy</td>\n",
       "      <td>0.316894</td>\n",
       "      <td>0.433313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.433313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Avg.Amp  Avg.total  Global.Max  Global.Min  Label   File_name  \\\n",
       "0    -126.08  -30503.56         0.0    -65542.0    0.0    Can1.npy   \n",
       "1    -120.64  -25937.42      7705.0    -51350.0    0.0    Can1.npy   \n",
       "2    -120.00  -28216.28      7705.0    -57277.0    0.0    Can1.npy   \n",
       "3    -123.30  -26709.78     10401.0    -56379.0    0.0    Can1.npy   \n",
       "4    -119.98  -24479.84      3725.0    -56056.0    0.0    Can1.npy   \n",
       "..       ...        ...         ...         ...    ...         ...   \n",
       "145   -46.30  -32929.52         0.0    -43963.0    2.0  Paper3.npy   \n",
       "146   -45.86  -33961.50         0.0    -42700.0    2.0  Paper3.npy   \n",
       "147   -48.12  -32811.10         0.0    -40858.0    2.0  Paper3.npy   \n",
       "148   -47.06  -33289.42         0.0    -44697.0    2.0  Paper3.npy   \n",
       "149   -46.22  -34181.74         0.0    -45131.0    2.0  Paper3.npy   \n",
       "\n",
       "     Avg.Amp.norm  Avg.total.norm  Glob.Max  Glob.Min  \n",
       "0        0.166113        0.438642  0.000000  0.438642  \n",
       "1        0.176384        0.445257  0.011163  0.445257  \n",
       "2        0.177592        0.441956  0.011163  0.441956  \n",
       "3        0.171362        0.444138  0.015068  0.444138  \n",
       "4        0.177630        0.447369  0.005397  0.447369  \n",
       "..            ...             ...       ...       ...  \n",
       "145      0.316743        0.435127  0.000000  0.435127  \n",
       "146      0.317574        0.433632  0.000000  0.433632  \n",
       "147      0.313307        0.435299  0.000000  0.435299  \n",
       "148      0.315309        0.434606  0.000000  0.434606  \n",
       "149      0.316894        0.433313  0.000000  0.433313  \n",
       "\n",
       "[150 rows x 10 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(test_data_df)\n",
    "test_data_df\n",
    "# print(train_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df = train_data_df.append(data_df[(data_df['File_name'] != 'Can1.npy') & (data_df['File_name'] != 'Can2.npy')\n",
    "                                                        & (data_df['File_name'] != 'Can3.npy') & (data_df['File_name'] != 'Can4.npy')\n",
    "                                                        & (data_df['File_name'] != 'Can5.npy') & (data_df['File_name'] != 'Can6.npy')\n",
    "                                                        & (data_df['File_name'] != 'Glass1.npy') & (data_df['File_name'] != 'Glass2.npy')\n",
    "                                                        & (data_df['File_name'] != 'Glass3.npy') & (data_df['File_name'] != 'Plastic1.npy')\n",
    "                                                        & (data_df['File_name'] != 'Plastic2.npy') & (data_df['File_name'] != 'Plastic3.npy')\n",
    "                                                        & (data_df['File_name'] != 'Paper1.npy') & (data_df['File_name'] != 'Paper2.npy')\n",
    "                                                        & (data_df['File_name'] != 'Paper3.npy')], ignore_index=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avg.Amp</th>\n",
       "      <th>Avg.total</th>\n",
       "      <th>Global.Max</th>\n",
       "      <th>Global.Min</th>\n",
       "      <th>Label</th>\n",
       "      <th>File_name</th>\n",
       "      <th>Avg.Amp.norm</th>\n",
       "      <th>Avg.total.norm</th>\n",
       "      <th>Glob.Max</th>\n",
       "      <th>Glob.Min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-113.62</td>\n",
       "      <td>-23723.28</td>\n",
       "      <td>8034.0</td>\n",
       "      <td>-54234.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Can1.npy</td>\n",
       "      <td>0.189638</td>\n",
       "      <td>0.448465</td>\n",
       "      <td>0.011639</td>\n",
       "      <td>0.448465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-120.16</td>\n",
       "      <td>-25313.84</td>\n",
       "      <td>8034.0</td>\n",
       "      <td>-45061.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Can1.npy</td>\n",
       "      <td>0.177290</td>\n",
       "      <td>0.446160</td>\n",
       "      <td>0.011639</td>\n",
       "      <td>0.446160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-118.92</td>\n",
       "      <td>-28008.58</td>\n",
       "      <td>858.0</td>\n",
       "      <td>-55172.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Can1.npy</td>\n",
       "      <td>0.179631</td>\n",
       "      <td>0.442256</td>\n",
       "      <td>0.001243</td>\n",
       "      <td>0.442256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-113.24</td>\n",
       "      <td>-25342.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-50688.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Can1.npy</td>\n",
       "      <td>0.190356</td>\n",
       "      <td>0.446119</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.446119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-114.02</td>\n",
       "      <td>-25750.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-45565.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Can1.npy</td>\n",
       "      <td>0.188883</td>\n",
       "      <td>0.445527</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.445527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>-13.90</td>\n",
       "      <td>55350.04</td>\n",
       "      <td>78463.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Plastic9.npy</td>\n",
       "      <td>0.377917</td>\n",
       "      <td>0.563021</td>\n",
       "      <td>0.113672</td>\n",
       "      <td>0.563021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>-14.78</td>\n",
       "      <td>53411.10</td>\n",
       "      <td>76813.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Plastic9.npy</td>\n",
       "      <td>0.376256</td>\n",
       "      <td>0.560212</td>\n",
       "      <td>0.111282</td>\n",
       "      <td>0.560212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>-15.86</td>\n",
       "      <td>56394.46</td>\n",
       "      <td>76654.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Plastic9.npy</td>\n",
       "      <td>0.374216</td>\n",
       "      <td>0.564535</td>\n",
       "      <td>0.111052</td>\n",
       "      <td>0.564535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>-10.12</td>\n",
       "      <td>59070.34</td>\n",
       "      <td>79050.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Plastic9.npy</td>\n",
       "      <td>0.385054</td>\n",
       "      <td>0.568411</td>\n",
       "      <td>0.114523</td>\n",
       "      <td>0.568411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>-13.86</td>\n",
       "      <td>56735.62</td>\n",
       "      <td>83622.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Plastic9.npy</td>\n",
       "      <td>0.377993</td>\n",
       "      <td>0.565029</td>\n",
       "      <td>0.121147</td>\n",
       "      <td>0.565029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>930 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Avg.Amp  Avg.total  Global.Max  Global.Min  Label     File_name  \\\n",
       "0    -113.62  -23723.28      8034.0    -54234.0    0.0      Can1.npy   \n",
       "1    -120.16  -25313.84      8034.0    -45061.0    0.0      Can1.npy   \n",
       "2    -118.92  -28008.58       858.0    -55172.0    0.0      Can1.npy   \n",
       "3    -113.24  -25342.58         0.0    -50688.0    0.0      Can1.npy   \n",
       "4    -114.02  -25750.86         0.0    -45565.0    0.0      Can1.npy   \n",
       "..       ...        ...         ...         ...    ...           ...   \n",
       "925   -13.90   55350.04     78463.0         0.0    3.0  Plastic9.npy   \n",
       "926   -14.78   53411.10     76813.0         0.0    3.0  Plastic9.npy   \n",
       "927   -15.86   56394.46     76654.0         0.0    3.0  Plastic9.npy   \n",
       "928   -10.12   59070.34     79050.0         0.0    3.0  Plastic9.npy   \n",
       "929   -13.86   56735.62     83622.0         0.0    3.0  Plastic9.npy   \n",
       "\n",
       "     Avg.Amp.norm  Avg.total.norm  Glob.Max  Glob.Min  \n",
       "0        0.189638        0.448465  0.011639  0.448465  \n",
       "1        0.177290        0.446160  0.011639  0.446160  \n",
       "2        0.179631        0.442256  0.001243  0.442256  \n",
       "3        0.190356        0.446119  0.000000  0.446119  \n",
       "4        0.188883        0.445527  0.000000  0.445527  \n",
       "..            ...             ...       ...       ...  \n",
       "925      0.377917        0.563021  0.113672  0.563021  \n",
       "926      0.376256        0.560212  0.111282  0.560212  \n",
       "927      0.374216        0.564535  0.111052  0.564535  \n",
       "928      0.385054        0.568411  0.114523  0.568411  \n",
       "929      0.377993        0.565029  0.121147  0.565029  \n",
       "\n",
       "[930 rows x 10 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.set_option('display.max_rows', None)\n",
    "pd.options.display.max_rows = 60\n",
    "train_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train = train_data_df.loc[:,['Avg.Amp.norm', 'Avg.total.norm', 'Glob.Max', 'Glob.Min']]\n",
    "X_Test = test_data_df.loc[:,['Avg.Amp.norm', 'Avg.total.norm', 'Glob.Max', 'Glob.Min']]\n",
    "Y_Train = train_data_df.loc[:, 'Label']\n",
    "Y_Test = test_data_df.loc[:, 'Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=1000, random_state=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RFC_df = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "RFC_df.fit(X_Train, Y_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy score with 100 decision-trees : 0.9267\n",
      "[[59  0  0  1]\n",
      " [ 0 30  0  0]\n",
      " [ 1  0 29  0]\n",
      " [ 0  0  9 21]]\n"
     ]
    }
   ],
   "source": [
    "RFC_df_Y_pred = RFC_df.predict(X_Test)\n",
    "print('Model accuracy score with 100 decision-trees : {0:0.4f}'. format(accuracy_score(Y_Test, RFC_df_Y_pred)))\n",
    "rfc_cm = confusion_matrix(Y_Test, RFC_df_Y_pred)\n",
    "print(rfc_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFC_df_Y_pred_df = RFC_df_Y_pred.reshape(5,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_indices = ['Can1', 'Can2', 'Glass', 'Paper', 'Plastic']\n",
    "column_names = ['1', '2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','21','22','23','24','25','26','27','28','29','30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rfc = pd.DataFrame(RFC_df_Y_pred_df, index=row_indices, )\n",
    "result_svm = pd.DataFrame(RFC_df_Y_pred_df, index=row_indices, columns = column_names)\n",
    "result_svm\n",
    "result_svm.replace(0, 'Can', inplace=True)\n",
    "result_svm.replace(1, 'Glass', inplace=True)\n",
    "result_svm.replace(2, 'Paper', inplace=True)\n",
    "result_svm.replace(3, 'Plastic', inplace=True)\n",
    "result_svm.to_csv('/home/joongho/Intelligent_Radar/gui/result/rfc_result.csv', sep=',', na_rep='NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_df['RF_pred'] = RFC_df_Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avg.Amp</th>\n",
       "      <th>Avg.total</th>\n",
       "      <th>Global.Max</th>\n",
       "      <th>Global.Min</th>\n",
       "      <th>Label</th>\n",
       "      <th>File_name</th>\n",
       "      <th>Avg.Amp.norm</th>\n",
       "      <th>Avg.total.norm</th>\n",
       "      <th>Glob.Max</th>\n",
       "      <th>Glob.Min</th>\n",
       "      <th>RF_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-126.08</td>\n",
       "      <td>-30503.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-65542.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Can1.npy</td>\n",
       "      <td>0.166113</td>\n",
       "      <td>0.438642</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.438642</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-120.64</td>\n",
       "      <td>-25937.42</td>\n",
       "      <td>7705.0</td>\n",
       "      <td>-51350.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Can1.npy</td>\n",
       "      <td>0.176384</td>\n",
       "      <td>0.445257</td>\n",
       "      <td>0.011163</td>\n",
       "      <td>0.445257</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-120.00</td>\n",
       "      <td>-28216.28</td>\n",
       "      <td>7705.0</td>\n",
       "      <td>-57277.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Can1.npy</td>\n",
       "      <td>0.177592</td>\n",
       "      <td>0.441956</td>\n",
       "      <td>0.011163</td>\n",
       "      <td>0.441956</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-123.30</td>\n",
       "      <td>-26709.78</td>\n",
       "      <td>10401.0</td>\n",
       "      <td>-56379.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Can1.npy</td>\n",
       "      <td>0.171362</td>\n",
       "      <td>0.444138</td>\n",
       "      <td>0.015068</td>\n",
       "      <td>0.444138</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-119.98</td>\n",
       "      <td>-24479.84</td>\n",
       "      <td>3725.0</td>\n",
       "      <td>-56056.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Can1.npy</td>\n",
       "      <td>0.177630</td>\n",
       "      <td>0.447369</td>\n",
       "      <td>0.005397</td>\n",
       "      <td>0.447369</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>-46.30</td>\n",
       "      <td>-32929.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-43963.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Paper3.npy</td>\n",
       "      <td>0.316743</td>\n",
       "      <td>0.435127</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.435127</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>-45.86</td>\n",
       "      <td>-33961.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-42700.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Paper3.npy</td>\n",
       "      <td>0.317574</td>\n",
       "      <td>0.433632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.433632</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>-48.12</td>\n",
       "      <td>-32811.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-40858.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Paper3.npy</td>\n",
       "      <td>0.313307</td>\n",
       "      <td>0.435299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.435299</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>-47.06</td>\n",
       "      <td>-33289.42</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-44697.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Paper3.npy</td>\n",
       "      <td>0.315309</td>\n",
       "      <td>0.434606</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.434606</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>-46.22</td>\n",
       "      <td>-34181.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-45131.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Paper3.npy</td>\n",
       "      <td>0.316894</td>\n",
       "      <td>0.433313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.433313</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Avg.Amp  Avg.total  Global.Max  Global.Min  Label   File_name  \\\n",
       "0    -126.08  -30503.56         0.0    -65542.0    0.0    Can1.npy   \n",
       "1    -120.64  -25937.42      7705.0    -51350.0    0.0    Can1.npy   \n",
       "2    -120.00  -28216.28      7705.0    -57277.0    0.0    Can1.npy   \n",
       "3    -123.30  -26709.78     10401.0    -56379.0    0.0    Can1.npy   \n",
       "4    -119.98  -24479.84      3725.0    -56056.0    0.0    Can1.npy   \n",
       "..       ...        ...         ...         ...    ...         ...   \n",
       "145   -46.30  -32929.52         0.0    -43963.0    2.0  Paper3.npy   \n",
       "146   -45.86  -33961.50         0.0    -42700.0    2.0  Paper3.npy   \n",
       "147   -48.12  -32811.10         0.0    -40858.0    2.0  Paper3.npy   \n",
       "148   -47.06  -33289.42         0.0    -44697.0    2.0  Paper3.npy   \n",
       "149   -46.22  -34181.74         0.0    -45131.0    2.0  Paper3.npy   \n",
       "\n",
       "     Avg.Amp.norm  Avg.total.norm  Glob.Max  Glob.Min  RF_pred  \n",
       "0        0.166113        0.438642  0.000000  0.438642      0.0  \n",
       "1        0.176384        0.445257  0.011163  0.445257      0.0  \n",
       "2        0.177592        0.441956  0.011163  0.441956      0.0  \n",
       "3        0.171362        0.444138  0.015068  0.444138      0.0  \n",
       "4        0.177630        0.447369  0.005397  0.447369      0.0  \n",
       "..            ...             ...       ...       ...      ...  \n",
       "145      0.316743        0.435127  0.000000  0.435127      2.0  \n",
       "146      0.317574        0.433632  0.000000  0.433632      2.0  \n",
       "147      0.313307        0.435299  0.000000  0.435299      2.0  \n",
       "148      0.315309        0.434606  0.000000  0.434606      2.0  \n",
       "149      0.316894        0.433313  0.000000  0.433313      2.0  \n",
       "\n",
       "[150 rows x 11 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9266666666666666"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RFC_df.score(X_Test, Y_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "title = 'RandomForestClassifier'\n",
    "heatmap(rfc_cm, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #For Support Vector Machine\n",
    "from sklearn.svm import SVC\n",
    "C = 1\n",
    "# # svm_clf = SVC(kernel = 'rbf', gamma = 0.7, C=C, max_iter = 10000)\n",
    "# svm_clf = SVC(kernel='linear')\n",
    "# svm_clf.fit(X_train, Y_train_cf)\n",
    "# svm_y_pred=svm_clf.predict(X_test)\n",
    "# print('Model accuracy score with support vetor machine : {0:0.4f}'. format(accuracy_score(Y_test_cf, svm_y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy score with support vetor machine : 0.4133\n"
     ]
    }
   ],
   "source": [
    "svm_rbf_clf = SVC(kernel = 'rbf', gamma = 0.7, C=C, max_iter = 10000)\n",
    "svm_rbf_clf.fit(X_Train, Y_Train)\n",
    "svm_rbf_y_pred=svm_rbf_clf.predict(X_Test)\n",
    "print('Model accuracy score with support vetor machine : {0:0.4f}'. format(accuracy_score(Y_Test, svm_rbf_y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[31 20  0  9]\n",
      " [ 0 30  0  0]\n",
      " [ 0 30  0  0]\n",
      " [ 0 29  0  1]]\n"
     ]
    }
   ],
   "source": [
    "SVM_cm=confusion_matrix(Y_Test, svm_rbf_y_pred)\n",
    "print(SVM_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "SVM_df_Y_pred = svm_rbf_y_pred.reshape(5,30)\n",
    "result_svm = pd.DataFrame(SVM_df_Y_pred, index=row_indices, columns = column_names)\n",
    "result_svm\n",
    "result_svm.replace(0, 'Can', inplace=True)\n",
    "result_svm.replace(1, 'Glass', inplace=True)\n",
    "result_svm.replace(2, 'Paper', inplace=True)\n",
    "result_svm.replace(3, 'Plastic', inplace=True)\n",
    "result_svm.to_csv('/home/joongho/Intelligent_Radar/gui/result/svm_result.csv', sep=',', na_rep='NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "title = 'Support Vector Machine'\n",
    "heatmap(SVM_cm, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1080, 4)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv1d_model = keras.Sequential()\n",
    "# conv1d_model.add(Conv1D(filters=32, kernel_size=1, padding='same', activation = 'relu', input_shape=(1,6)))\n",
    "# conv1d_model.add(Dropout(0.1))\n",
    "# conv1d_model.add(Conv1D(filters=68, kernel_size=1, padding='valid', activation = 'relu'))\n",
    "# conv1d_model.add(Conv1D(filters=32, kernel_size=1, padding='valid', activation = 'relu'))\n",
    "# conv1d_model.add(Flatten())\n",
    "# conv1d_model.add(Dropout(0.2))\n",
    "# conv1d_model.add(Dense(len(ML.labels_dict),activation='softmax'))\n",
    "\n",
    "# conv1d_model.compile(optimizer='Adam', loss=tf.keras.losses.CategoricalCrossentropy(), metrics=[\"accuracy\"])\n",
    "# # conv1d_model.summary()\n",
    "# conv1d_model.fit(X_train, Y_train, epochs=500, verbose=0, batch_size=20)\n",
    "# loss, acc = conv1d_model.evaluate(X_test, Y_test)\n",
    "# result=conv1d_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_Train_nn = np.zeros((len(Y_Train), 4))\n",
    "for i in range(len(Y_Train)):\n",
    "    Y_Train_nn[i][int(Y_Train[i])] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_Test_nn = np.zeros((len(Y_Test), 4))\n",
    "for i in range(len(Y_Test)):\n",
    "    Y_Test_nn[i][int(Y_Test[i])] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 4)                 20        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 36)                180       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 296       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 36        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 532\n",
      "Trainable params: 532\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn_model = keras.Sequential()\n",
    "nn_model.add(keras.Input(shape=(X_train_nn.shape[1],)))\n",
    "nn_model.add(Dense(units=4, activation='relu'))\n",
    "nn_model.add(Dense(units=36, activation='relu'))\n",
    "nn_model.add(Dense(units=8, activation='relu'))\n",
    "nn_model.add(Dropout(0.2))\n",
    "nn_model.add(Dense(units=4, activation='softmax'))\n",
    "nn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "6/6 [==============================] - 1s 23ms/step - loss: 1.3906 - accuracy: 0.2554 - val_loss: 1.3824 - val_accuracy: 0.0323\n",
      "Epoch 2/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.3824 - accuracy: 0.2930 - val_loss: 1.4008 - val_accuracy: 0.0323\n",
      "Epoch 3/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.3752 - accuracy: 0.3253 - val_loss: 1.4146 - val_accuracy: 0.0323\n",
      "Epoch 4/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.3732 - accuracy: 0.3125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 17:54:12.421814: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 5ms/step - loss: 1.3733 - accuracy: 0.3051 - val_loss: 1.4255 - val_accuracy: 0.0323\n",
      "Epoch 5/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.3694 - accuracy: 0.3239 - val_loss: 1.4351 - val_accuracy: 0.0323\n",
      "Epoch 6/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.3658 - accuracy: 0.3185 - val_loss: 1.4442 - val_accuracy: 0.0323\n",
      "Epoch 7/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.3607 - accuracy: 0.3441 - val_loss: 1.4556 - val_accuracy: 0.0323\n",
      "Epoch 8/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.3608 - accuracy: 0.3212 - val_loss: 1.4676 - val_accuracy: 0.0323\n",
      "Epoch 9/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.3557 - accuracy: 0.3414 - val_loss: 1.4797 - val_accuracy: 0.0323\n",
      "Epoch 10/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.3551 - accuracy: 0.3306 - val_loss: 1.4912 - val_accuracy: 0.0323\n",
      "Epoch 11/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.3492 - accuracy: 0.3253 - val_loss: 1.5041 - val_accuracy: 0.0323\n",
      "Epoch 12/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.3436 - accuracy: 0.3320 - val_loss: 1.5193 - val_accuracy: 0.0323\n",
      "Epoch 13/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.3425 - accuracy: 0.3078 - val_loss: 1.5346 - val_accuracy: 0.0323\n",
      "Epoch 14/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.3388 - accuracy: 0.3118 - val_loss: 1.5498 - val_accuracy: 0.0323\n",
      "Epoch 15/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.3353 - accuracy: 0.3212 - val_loss: 1.5637 - val_accuracy: 0.0323\n",
      "Epoch 16/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.3369 - accuracy: 0.3212 - val_loss: 1.5782 - val_accuracy: 0.0323\n",
      "Epoch 17/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.3343 - accuracy: 0.3441 - val_loss: 1.5911 - val_accuracy: 0.0323\n",
      "Epoch 18/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.3224 - accuracy: 0.3737 - val_loss: 1.6055 - val_accuracy: 0.0323\n",
      "Epoch 19/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.3197 - accuracy: 0.4005 - val_loss: 1.6233 - val_accuracy: 0.0323\n",
      "Epoch 20/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.3154 - accuracy: 0.4046 - val_loss: 1.6439 - val_accuracy: 0.0323\n",
      "Epoch 21/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.3122 - accuracy: 0.3938 - val_loss: 1.6653 - val_accuracy: 0.0323\n",
      "Epoch 22/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.3062 - accuracy: 0.3871 - val_loss: 1.6863 - val_accuracy: 0.0323\n",
      "Epoch 23/1000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.3021 - accuracy: 0.3884 - val_loss: 1.7055 - val_accuracy: 0.0323\n",
      "Epoch 24/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.3010 - accuracy: 0.3831 - val_loss: 1.7239 - val_accuracy: 0.0323\n",
      "Epoch 25/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.2999 - accuracy: 0.3925 - val_loss: 1.7358 - val_accuracy: 0.0323\n",
      "Epoch 26/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.2912 - accuracy: 0.3898 - val_loss: 1.7565 - val_accuracy: 0.0323\n",
      "Epoch 27/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.2917 - accuracy: 0.4220 - val_loss: 1.7786 - val_accuracy: 0.0323\n",
      "Epoch 28/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.2838 - accuracy: 0.4005 - val_loss: 1.7946 - val_accuracy: 0.0323\n",
      "Epoch 29/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.2738 - accuracy: 0.4315 - val_loss: 1.8176 - val_accuracy: 0.0323\n",
      "Epoch 30/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.2755 - accuracy: 0.4032 - val_loss: 1.8443 - val_accuracy: 0.0323\n",
      "Epoch 31/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.2728 - accuracy: 0.3898 - val_loss: 1.8631 - val_accuracy: 0.0323\n",
      "Epoch 32/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.2671 - accuracy: 0.4073 - val_loss: 1.8818 - val_accuracy: 0.0323\n",
      "Epoch 33/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.2638 - accuracy: 0.3884 - val_loss: 1.8951 - val_accuracy: 0.0323\n",
      "Epoch 34/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.2546 - accuracy: 0.3508 - val_loss: 1.9104 - val_accuracy: 0.0323\n",
      "Epoch 35/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.2564 - accuracy: 0.3414 - val_loss: 1.9314 - val_accuracy: 0.0323\n",
      "Epoch 36/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.2480 - accuracy: 0.3320 - val_loss: 1.9434 - val_accuracy: 0.0323\n",
      "Epoch 37/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.2466 - accuracy: 0.3508 - val_loss: 1.9670 - val_accuracy: 0.0323\n",
      "Epoch 38/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.2462 - accuracy: 0.3387 - val_loss: 1.9824 - val_accuracy: 0.0323\n",
      "Epoch 39/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.2377 - accuracy: 0.3548 - val_loss: 2.0005 - val_accuracy: 0.0323\n",
      "Epoch 40/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.2301 - accuracy: 0.3683 - val_loss: 2.0244 - val_accuracy: 0.0323\n",
      "Epoch 41/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.2392 - accuracy: 0.3522 - val_loss: 2.0310 - val_accuracy: 0.0323\n",
      "Epoch 42/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.2322 - accuracy: 0.3441 - val_loss: 2.0363 - val_accuracy: 0.0323\n",
      "Epoch 43/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.2225 - accuracy: 0.3710 - val_loss: 2.0534 - val_accuracy: 0.0323\n",
      "Epoch 44/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.2177 - accuracy: 0.3817 - val_loss: 2.0657 - val_accuracy: 0.0323\n",
      "Epoch 45/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.2224 - accuracy: 0.3562 - val_loss: 2.0625 - val_accuracy: 0.0323\n",
      "Epoch 46/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.2249 - accuracy: 0.3642 - val_loss: 2.0761 - val_accuracy: 0.0323\n",
      "Epoch 47/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.2142 - accuracy: 0.3804 - val_loss: 2.0754 - val_accuracy: 0.0323\n",
      "Epoch 48/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.2135 - accuracy: 0.3710 - val_loss: 2.0789 - val_accuracy: 0.0323\n",
      "Epoch 49/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.2014 - accuracy: 0.3831 - val_loss: 2.0990 - val_accuracy: 0.0323\n",
      "Epoch 50/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.2008 - accuracy: 0.3898 - val_loss: 2.1141 - val_accuracy: 0.0323\n",
      "Epoch 51/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1981 - accuracy: 0.3938 - val_loss: 2.1280 - val_accuracy: 0.0323\n",
      "Epoch 52/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.1924 - accuracy: 0.3790 - val_loss: 2.1257 - val_accuracy: 0.0323\n",
      "Epoch 53/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1849 - accuracy: 0.4005 - val_loss: 2.1326 - val_accuracy: 0.0323\n",
      "Epoch 54/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.2013 - accuracy: 0.3804 - val_loss: 2.1365 - val_accuracy: 0.0323\n",
      "Epoch 55/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1750 - accuracy: 0.4073 - val_loss: 2.1339 - val_accuracy: 0.0323\n",
      "Epoch 56/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1766 - accuracy: 0.3535 - val_loss: 2.1398 - val_accuracy: 0.0323\n",
      "Epoch 57/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1732 - accuracy: 0.3817 - val_loss: 2.1455 - val_accuracy: 0.0323\n",
      "Epoch 58/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1693 - accuracy: 0.4207 - val_loss: 2.1455 - val_accuracy: 0.0323\n",
      "Epoch 59/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1674 - accuracy: 0.4194 - val_loss: 2.1418 - val_accuracy: 0.0323\n",
      "Epoch 60/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1601 - accuracy: 0.4059 - val_loss: 2.1410 - val_accuracy: 0.0323\n",
      "Epoch 61/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.1482 - accuracy: 0.4274 - val_loss: 2.1555 - val_accuracy: 0.0323\n",
      "Epoch 62/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1496 - accuracy: 0.4194 - val_loss: 2.1643 - val_accuracy: 0.0323\n",
      "Epoch 63/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1547 - accuracy: 0.4032 - val_loss: 2.1604 - val_accuracy: 0.0323\n",
      "Epoch 64/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.1440 - accuracy: 0.4140 - val_loss: 2.1689 - val_accuracy: 0.0323\n",
      "Epoch 65/1000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.1323 - accuracy: 0.4059 - val_loss: 2.1765 - val_accuracy: 0.0323\n",
      "Epoch 66/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.1324 - accuracy: 0.4247 - val_loss: 2.1789 - val_accuracy: 0.0323\n",
      "Epoch 67/1000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.1249 - accuracy: 0.4368 - val_loss: 2.1717 - val_accuracy: 0.0323\n",
      "Epoch 68/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1275 - accuracy: 0.4086 - val_loss: 2.1794 - val_accuracy: 0.0323\n",
      "Epoch 69/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1086 - accuracy: 0.4140 - val_loss: 2.1758 - val_accuracy: 0.0323\n",
      "Epoch 70/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1018 - accuracy: 0.4449 - val_loss: 2.2053 - val_accuracy: 0.0323\n",
      "Epoch 71/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1024 - accuracy: 0.4341 - val_loss: 2.2005 - val_accuracy: 0.0323\n",
      "Epoch 72/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.0992 - accuracy: 0.4395 - val_loss: 2.2226 - val_accuracy: 0.0323\n",
      "Epoch 73/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.1079 - accuracy: 0.4288 - val_loss: 2.2225 - val_accuracy: 0.0323\n",
      "Epoch 74/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0934 - accuracy: 0.4462 - val_loss: 2.1948 - val_accuracy: 0.0323\n",
      "Epoch 75/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0912 - accuracy: 0.4422 - val_loss: 2.2067 - val_accuracy: 0.0323\n",
      "Epoch 76/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.0772 - accuracy: 0.4435 - val_loss: 2.2275 - val_accuracy: 0.0323\n",
      "Epoch 77/1000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.0851 - accuracy: 0.4288 - val_loss: 2.2170 - val_accuracy: 0.0323\n",
      "Epoch 78/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0650 - accuracy: 0.4543 - val_loss: 2.2398 - val_accuracy: 0.0323\n",
      "Epoch 79/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0610 - accuracy: 0.4476 - val_loss: 2.2371 - val_accuracy: 0.0323\n",
      "Epoch 80/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0525 - accuracy: 0.4570 - val_loss: 2.2409 - val_accuracy: 0.0323\n",
      "Epoch 81/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0439 - accuracy: 0.4476 - val_loss: 2.2444 - val_accuracy: 0.0323\n",
      "Epoch 82/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0662 - accuracy: 0.4503 - val_loss: 2.2380 - val_accuracy: 0.0323\n",
      "Epoch 83/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0563 - accuracy: 0.4489 - val_loss: 2.2411 - val_accuracy: 0.0323\n",
      "Epoch 84/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.0505 - accuracy: 0.4583 - val_loss: 2.2226 - val_accuracy: 0.0323\n",
      "Epoch 85/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.0559 - accuracy: 0.4704 - val_loss: 2.2292 - val_accuracy: 0.0323\n",
      "Epoch 86/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0342 - accuracy: 0.4718 - val_loss: 2.2356 - val_accuracy: 0.0323\n",
      "Epoch 87/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.0312 - accuracy: 0.4516 - val_loss: 2.2301 - val_accuracy: 0.0323\n",
      "Epoch 88/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.0411 - accuracy: 0.4731 - val_loss: 2.2461 - val_accuracy: 0.0323\n",
      "Epoch 89/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0351 - accuracy: 0.4610 - val_loss: 2.2522 - val_accuracy: 0.0323\n",
      "Epoch 90/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0261 - accuracy: 0.4610 - val_loss: 2.2425 - val_accuracy: 0.0323\n",
      "Epoch 91/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0356 - accuracy: 0.4879 - val_loss: 2.2308 - val_accuracy: 0.0323\n",
      "Epoch 92/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.0304 - accuracy: 0.4852 - val_loss: 2.2342 - val_accuracy: 0.0323\n",
      "Epoch 93/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.0191 - accuracy: 0.4772 - val_loss: 2.2330 - val_accuracy: 0.0323\n",
      "Epoch 94/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0101 - accuracy: 0.4785 - val_loss: 2.2345 - val_accuracy: 0.0323\n",
      "Epoch 95/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.0141 - accuracy: 0.4718 - val_loss: 2.2441 - val_accuracy: 0.0323\n",
      "Epoch 96/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.0126 - accuracy: 0.4731 - val_loss: 2.2398 - val_accuracy: 0.0323\n",
      "Epoch 97/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.0095 - accuracy: 0.4852 - val_loss: 2.2437 - val_accuracy: 0.0323\n",
      "Epoch 98/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0054 - accuracy: 0.4691 - val_loss: 2.2543 - val_accuracy: 0.0323\n",
      "Epoch 99/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.9944 - accuracy: 0.4731 - val_loss: 2.2625 - val_accuracy: 0.0323\n",
      "Epoch 100/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.9922 - accuracy: 0.4946 - val_loss: 2.2552 - val_accuracy: 0.0323\n",
      "Epoch 101/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.9824 - accuracy: 0.5040 - val_loss: 2.2729 - val_accuracy: 0.0323\n",
      "Epoch 102/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.9861 - accuracy: 0.4973 - val_loss: 2.2789 - val_accuracy: 0.0323\n",
      "Epoch 103/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9875 - accuracy: 0.5161 - val_loss: 2.2831 - val_accuracy: 0.0323\n",
      "Epoch 104/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9675 - accuracy: 0.5363 - val_loss: 2.3008 - val_accuracy: 0.0323\n",
      "Epoch 105/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9803 - accuracy: 0.5148 - val_loss: 2.3084 - val_accuracy: 0.0323\n",
      "Epoch 106/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.9659 - accuracy: 0.5027 - val_loss: 2.3052 - val_accuracy: 0.0323\n",
      "Epoch 107/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9767 - accuracy: 0.5202 - val_loss: 2.3138 - val_accuracy: 0.0323\n",
      "Epoch 108/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9752 - accuracy: 0.4960 - val_loss: 2.3067 - val_accuracy: 0.0323\n",
      "Epoch 109/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.9902 - accuracy: 0.4839 - val_loss: 2.2795 - val_accuracy: 0.0323\n",
      "Epoch 110/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.9634 - accuracy: 0.5108 - val_loss: 2.2928 - val_accuracy: 0.0323\n",
      "Epoch 111/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.9682 - accuracy: 0.5108 - val_loss: 2.2772 - val_accuracy: 0.0323\n",
      "Epoch 112/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9660 - accuracy: 0.5175 - val_loss: 2.2692 - val_accuracy: 0.0323\n",
      "Epoch 113/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9480 - accuracy: 0.5202 - val_loss: 2.2645 - val_accuracy: 0.0323\n",
      "Epoch 114/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9508 - accuracy: 0.5954 - val_loss: 2.2593 - val_accuracy: 0.0323\n",
      "Epoch 115/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9560 - accuracy: 0.6048 - val_loss: 2.2734 - val_accuracy: 0.0323\n",
      "Epoch 116/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9508 - accuracy: 0.6331 - val_loss: 2.2792 - val_accuracy: 0.0323\n",
      "Epoch 117/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9411 - accuracy: 0.6384 - val_loss: 2.2654 - val_accuracy: 0.0323\n",
      "Epoch 118/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9373 - accuracy: 0.6102 - val_loss: 2.2723 - val_accuracy: 0.0323\n",
      "Epoch 119/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9533 - accuracy: 0.6129 - val_loss: 2.2730 - val_accuracy: 0.0323\n",
      "Epoch 120/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9445 - accuracy: 0.6075 - val_loss: 2.2673 - val_accuracy: 0.0323\n",
      "Epoch 121/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9293 - accuracy: 0.6277 - val_loss: 2.2807 - val_accuracy: 0.0323\n",
      "Epoch 122/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.9297 - accuracy: 0.6263 - val_loss: 2.2772 - val_accuracy: 0.0323\n",
      "Epoch 123/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.9221 - accuracy: 0.6277 - val_loss: 2.2727 - val_accuracy: 0.0323\n",
      "Epoch 124/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9168 - accuracy: 0.6452 - val_loss: 2.2798 - val_accuracy: 0.0323\n",
      "Epoch 125/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9195 - accuracy: 0.6425 - val_loss: 2.2818 - val_accuracy: 0.0323\n",
      "Epoch 126/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9208 - accuracy: 0.6492 - val_loss: 2.2993 - val_accuracy: 0.0323\n",
      "Epoch 127/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9227 - accuracy: 0.6317 - val_loss: 2.3016 - val_accuracy: 0.0323\n",
      "Epoch 128/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.9012 - accuracy: 0.6317 - val_loss: 2.2937 - val_accuracy: 0.0323\n",
      "Epoch 129/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8923 - accuracy: 0.6290 - val_loss: 2.3032 - val_accuracy: 0.0323\n",
      "Epoch 130/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8939 - accuracy: 0.6411 - val_loss: 2.3228 - val_accuracy: 0.0323\n",
      "Epoch 131/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8913 - accuracy: 0.6371 - val_loss: 2.3143 - val_accuracy: 0.0323\n",
      "Epoch 132/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9010 - accuracy: 0.6116 - val_loss: 2.3229 - val_accuracy: 0.0323\n",
      "Epoch 133/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9120 - accuracy: 0.6102 - val_loss: 2.3324 - val_accuracy: 0.0323\n",
      "Epoch 134/1000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9090 - accuracy: 0.6263 - val_loss: 2.3131 - val_accuracy: 0.0323\n",
      "Epoch 135/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.8917 - accuracy: 0.6196 - val_loss: 2.3140 - val_accuracy: 0.0323\n",
      "Epoch 136/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.8840 - accuracy: 0.6263 - val_loss: 2.3098 - val_accuracy: 0.0323\n",
      "Epoch 137/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.8890 - accuracy: 0.6317 - val_loss: 2.3053 - val_accuracy: 0.0323\n",
      "Epoch 138/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.8767 - accuracy: 0.6452 - val_loss: 2.3105 - val_accuracy: 0.0323\n",
      "Epoch 139/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.8720 - accuracy: 0.6425 - val_loss: 2.3140 - val_accuracy: 0.0323\n",
      "Epoch 140/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8909 - accuracy: 0.6210 - val_loss: 2.3032 - val_accuracy: 0.0323\n",
      "Epoch 141/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8689 - accuracy: 0.6384 - val_loss: 2.3141 - val_accuracy: 0.0323\n",
      "Epoch 142/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8836 - accuracy: 0.6317 - val_loss: 2.3211 - val_accuracy: 0.0323\n",
      "Epoch 143/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.8771 - accuracy: 0.6223 - val_loss: 2.3147 - val_accuracy: 0.0323\n",
      "Epoch 144/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8652 - accuracy: 0.6411 - val_loss: 2.3274 - val_accuracy: 0.0323\n",
      "Epoch 145/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8620 - accuracy: 0.6411 - val_loss: 2.3251 - val_accuracy: 0.0323\n",
      "Epoch 146/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.8527 - accuracy: 0.6478 - val_loss: 2.3224 - val_accuracy: 0.0323\n",
      "Epoch 147/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.8590 - accuracy: 0.6465 - val_loss: 2.3261 - val_accuracy: 0.0323\n",
      "Epoch 148/1000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.8714 - accuracy: 0.6250 - val_loss: 2.3322 - val_accuracy: 0.0323\n",
      "Epoch 149/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8500 - accuracy: 0.6505 - val_loss: 2.3373 - val_accuracy: 0.0323\n",
      "Epoch 150/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8307 - accuracy: 0.6519 - val_loss: 2.3463 - val_accuracy: 0.0323\n",
      "Epoch 151/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8369 - accuracy: 0.6478 - val_loss: 2.3478 - val_accuracy: 0.0323\n",
      "Epoch 152/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.8431 - accuracy: 0.6250 - val_loss: 2.3591 - val_accuracy: 0.0323\n",
      "Epoch 153/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8453 - accuracy: 0.6492 - val_loss: 2.3525 - val_accuracy: 0.0323\n",
      "Epoch 154/1000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.8520 - accuracy: 0.6371 - val_loss: 2.3590 - val_accuracy: 0.0323\n",
      "Epoch 155/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8316 - accuracy: 0.6438 - val_loss: 2.3665 - val_accuracy: 0.0323\n",
      "Epoch 156/1000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.8251 - accuracy: 0.6573 - val_loss: 2.3742 - val_accuracy: 0.0323\n",
      "Epoch 157/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8394 - accuracy: 0.6559 - val_loss: 2.3820 - val_accuracy: 0.0323\n",
      "Epoch 158/1000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.8297 - accuracy: 0.6626 - val_loss: 2.3899 - val_accuracy: 0.0323\n",
      "Epoch 159/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.8212 - accuracy: 0.6573 - val_loss: 2.4009 - val_accuracy: 0.0323\n",
      "Epoch 160/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.8149 - accuracy: 0.6559 - val_loss: 2.3992 - val_accuracy: 0.0323\n",
      "Epoch 161/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8119 - accuracy: 0.6478 - val_loss: 2.3987 - val_accuracy: 0.0323\n",
      "Epoch 162/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.8230 - accuracy: 0.6559 - val_loss: 2.4155 - val_accuracy: 0.0323\n",
      "Epoch 163/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8256 - accuracy: 0.6519 - val_loss: 2.4176 - val_accuracy: 0.0323\n",
      "Epoch 164/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8075 - accuracy: 0.6761 - val_loss: 2.4098 - val_accuracy: 0.0323\n",
      "Epoch 165/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.8090 - accuracy: 0.6626 - val_loss: 2.4189 - val_accuracy: 0.0323\n",
      "Epoch 166/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8090 - accuracy: 0.6532 - val_loss: 2.4392 - val_accuracy: 0.0323\n",
      "Epoch 167/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8045 - accuracy: 0.6640 - val_loss: 2.4482 - val_accuracy: 0.0323\n",
      "Epoch 168/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.8081 - accuracy: 0.6559 - val_loss: 2.4333 - val_accuracy: 0.0323\n",
      "Epoch 169/1000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.8080 - accuracy: 0.6492 - val_loss: 2.4373 - val_accuracy: 0.0323\n",
      "Epoch 170/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.8028 - accuracy: 0.6653 - val_loss: 2.4407 - val_accuracy: 0.0323\n",
      "Epoch 171/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7952 - accuracy: 0.6747 - val_loss: 2.4496 - val_accuracy: 0.0323\n",
      "Epoch 172/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.7982 - accuracy: 0.6613 - val_loss: 2.4543 - val_accuracy: 0.0323\n",
      "Epoch 173/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7941 - accuracy: 0.6720 - val_loss: 2.4471 - val_accuracy: 0.0323\n",
      "Epoch 174/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8020 - accuracy: 0.6707 - val_loss: 2.4654 - val_accuracy: 0.0323\n",
      "Epoch 175/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.7932 - accuracy: 0.6667 - val_loss: 2.4723 - val_accuracy: 0.0323\n",
      "Epoch 176/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.8051 - accuracy: 0.6411 - val_loss: 2.4785 - val_accuracy: 0.0323\n",
      "Epoch 177/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7936 - accuracy: 0.6653 - val_loss: 2.4785 - val_accuracy: 0.0323\n",
      "Epoch 178/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7875 - accuracy: 0.6546 - val_loss: 2.4875 - val_accuracy: 0.0323\n",
      "Epoch 179/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7729 - accuracy: 0.6694 - val_loss: 2.5042 - val_accuracy: 0.0323\n",
      "Epoch 180/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.7903 - accuracy: 0.6559 - val_loss: 2.5176 - val_accuracy: 0.0323\n",
      "Epoch 181/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7910 - accuracy: 0.6801 - val_loss: 2.5271 - val_accuracy: 0.0323\n",
      "Epoch 182/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7810 - accuracy: 0.6734 - val_loss: 2.5305 - val_accuracy: 0.0323\n",
      "Epoch 183/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.7881 - accuracy: 0.6599 - val_loss: 2.5210 - val_accuracy: 0.0323\n",
      "Epoch 184/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7763 - accuracy: 0.6747 - val_loss: 2.5370 - val_accuracy: 0.0323\n",
      "Epoch 185/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.7774 - accuracy: 0.6492 - val_loss: 2.5587 - val_accuracy: 0.0323\n",
      "Epoch 186/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7861 - accuracy: 0.6680 - val_loss: 2.5522 - val_accuracy: 0.0323\n",
      "Epoch 187/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.7829 - accuracy: 0.6707 - val_loss: 2.5677 - val_accuracy: 0.0323\n",
      "Epoch 188/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8032 - accuracy: 0.6452 - val_loss: 2.5917 - val_accuracy: 0.0323\n",
      "Epoch 189/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7638 - accuracy: 0.6680 - val_loss: 2.6159 - val_accuracy: 0.0323\n",
      "Epoch 190/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7532 - accuracy: 0.6680 - val_loss: 2.6172 - val_accuracy: 0.0323\n",
      "Epoch 191/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7787 - accuracy: 0.6707 - val_loss: 2.6173 - val_accuracy: 0.0323\n",
      "Epoch 192/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7783 - accuracy: 0.6734 - val_loss: 2.6238 - val_accuracy: 0.0323\n",
      "Epoch 193/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7629 - accuracy: 0.6573 - val_loss: 2.6486 - val_accuracy: 0.0323\n",
      "Epoch 194/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7727 - accuracy: 0.6599 - val_loss: 2.6574 - val_accuracy: 0.0323\n",
      "Epoch 195/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7639 - accuracy: 0.6720 - val_loss: 2.6622 - val_accuracy: 0.0323\n",
      "Epoch 196/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7588 - accuracy: 0.6788 - val_loss: 2.6682 - val_accuracy: 0.0323\n",
      "Epoch 197/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.7596 - accuracy: 0.6626 - val_loss: 2.6724 - val_accuracy: 0.0323\n",
      "Epoch 198/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7432 - accuracy: 0.6815 - val_loss: 2.6825 - val_accuracy: 0.0323\n",
      "Epoch 199/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7734 - accuracy: 0.6452 - val_loss: 2.6776 - val_accuracy: 0.0323\n",
      "Epoch 200/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7807 - accuracy: 0.6478 - val_loss: 2.6687 - val_accuracy: 0.0323\n",
      "Epoch 201/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.7599 - accuracy: 0.6411 - val_loss: 2.6696 - val_accuracy: 0.0323\n",
      "Epoch 202/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.7542 - accuracy: 0.6599 - val_loss: 2.6801 - val_accuracy: 0.0323\n",
      "Epoch 203/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.7661 - accuracy: 0.6599 - val_loss: 2.7014 - val_accuracy: 0.0323\n",
      "Epoch 204/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7484 - accuracy: 0.6519 - val_loss: 2.6963 - val_accuracy: 0.0323\n",
      "Epoch 205/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7426 - accuracy: 0.6788 - val_loss: 2.6935 - val_accuracy: 0.0323\n",
      "Epoch 206/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7646 - accuracy: 0.6599 - val_loss: 2.7029 - val_accuracy: 0.0323\n",
      "Epoch 207/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.7561 - accuracy: 0.6747 - val_loss: 2.7122 - val_accuracy: 0.0323\n",
      "Epoch 208/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7397 - accuracy: 0.6841 - val_loss: 2.7095 - val_accuracy: 0.0323\n",
      "Epoch 209/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.7586 - accuracy: 0.6774 - val_loss: 2.7090 - val_accuracy: 0.0323\n",
      "Epoch 210/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7362 - accuracy: 0.6761 - val_loss: 2.7150 - val_accuracy: 0.0323\n",
      "Epoch 211/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.7531 - accuracy: 0.6949 - val_loss: 2.7268 - val_accuracy: 0.0323\n",
      "Epoch 212/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7522 - accuracy: 0.6747 - val_loss: 2.7251 - val_accuracy: 0.0323\n",
      "Epoch 213/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7476 - accuracy: 0.6774 - val_loss: 2.7502 - val_accuracy: 0.0323\n",
      "Epoch 214/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.7385 - accuracy: 0.6680 - val_loss: 2.7483 - val_accuracy: 0.0323\n",
      "Epoch 215/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7424 - accuracy: 0.6774 - val_loss: 2.7661 - val_accuracy: 0.0323\n",
      "Epoch 216/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7400 - accuracy: 0.6815 - val_loss: 2.7800 - val_accuracy: 0.0323\n",
      "Epoch 217/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7443 - accuracy: 0.6667 - val_loss: 2.7932 - val_accuracy: 0.0323\n",
      "Epoch 218/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7619 - accuracy: 0.6680 - val_loss: 2.7713 - val_accuracy: 0.0323\n",
      "Epoch 219/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.7290 - accuracy: 0.6720 - val_loss: 2.7735 - val_accuracy: 0.0323\n",
      "Epoch 220/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.7416 - accuracy: 0.6720 - val_loss: 2.7820 - val_accuracy: 0.0323\n",
      "Epoch 221/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7483 - accuracy: 0.6626 - val_loss: 2.8071 - val_accuracy: 0.0323\n",
      "Epoch 222/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7462 - accuracy: 0.6653 - val_loss: 2.7866 - val_accuracy: 0.0323\n",
      "Epoch 223/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7340 - accuracy: 0.6962 - val_loss: 2.7827 - val_accuracy: 0.0323\n",
      "Epoch 224/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7424 - accuracy: 0.6761 - val_loss: 2.8181 - val_accuracy: 0.0323\n",
      "Epoch 225/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7267 - accuracy: 0.6774 - val_loss: 2.8142 - val_accuracy: 0.0323\n",
      "Epoch 226/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7288 - accuracy: 0.6788 - val_loss: 2.8064 - val_accuracy: 0.0323\n",
      "Epoch 227/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7318 - accuracy: 0.6546 - val_loss: 2.8339 - val_accuracy: 0.0323\n",
      "Epoch 228/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7538 - accuracy: 0.6465 - val_loss: 2.8468 - val_accuracy: 0.0323\n",
      "Epoch 229/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7357 - accuracy: 0.6586 - val_loss: 2.8337 - val_accuracy: 0.0323\n",
      "Epoch 230/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.7520 - accuracy: 0.6331 - val_loss: 2.8409 - val_accuracy: 0.0323\n",
      "Epoch 231/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7266 - accuracy: 0.6815 - val_loss: 2.8588 - val_accuracy: 0.0323\n",
      "Epoch 232/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.7277 - accuracy: 0.6573 - val_loss: 2.8501 - val_accuracy: 0.0323\n",
      "Epoch 233/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.7149 - accuracy: 0.6640 - val_loss: 2.8619 - val_accuracy: 0.0323\n",
      "Epoch 234/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7453 - accuracy: 0.6465 - val_loss: 2.8909 - val_accuracy: 0.0323\n",
      "Epoch 235/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7238 - accuracy: 0.6640 - val_loss: 2.8703 - val_accuracy: 0.0323\n",
      "Epoch 236/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7412 - accuracy: 0.6640 - val_loss: 2.8766 - val_accuracy: 0.0323\n",
      "Epoch 237/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7385 - accuracy: 0.6452 - val_loss: 2.8602 - val_accuracy: 0.0323\n",
      "Epoch 238/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7233 - accuracy: 0.6815 - val_loss: 2.8842 - val_accuracy: 0.0323\n",
      "Epoch 239/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.7168 - accuracy: 0.6788 - val_loss: 2.8886 - val_accuracy: 0.0323\n",
      "Epoch 240/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7393 - accuracy: 0.6680 - val_loss: 2.8985 - val_accuracy: 0.0323\n",
      "Epoch 241/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7192 - accuracy: 0.6613 - val_loss: 2.8917 - val_accuracy: 0.0323\n",
      "Epoch 242/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7322 - accuracy: 0.6747 - val_loss: 2.8766 - val_accuracy: 0.0323\n",
      "Epoch 243/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7220 - accuracy: 0.6761 - val_loss: 2.8855 - val_accuracy: 0.0323\n",
      "Epoch 244/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7193 - accuracy: 0.6922 - val_loss: 2.8723 - val_accuracy: 0.0323\n",
      "Epoch 245/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7315 - accuracy: 0.6694 - val_loss: 2.8968 - val_accuracy: 0.0323\n",
      "Epoch 246/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7250 - accuracy: 0.6774 - val_loss: 2.9136 - val_accuracy: 0.0323\n",
      "Epoch 247/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.7206 - accuracy: 0.6895 - val_loss: 2.9087 - val_accuracy: 0.0323\n",
      "Epoch 248/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7197 - accuracy: 0.6841 - val_loss: 2.9274 - val_accuracy: 0.0323\n",
      "Epoch 249/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7266 - accuracy: 0.6935 - val_loss: 2.9193 - val_accuracy: 0.0323\n",
      "Epoch 250/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7294 - accuracy: 0.6774 - val_loss: 2.9066 - val_accuracy: 0.0323\n",
      "Epoch 251/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7396 - accuracy: 0.6492 - val_loss: 2.9365 - val_accuracy: 0.0323\n",
      "Epoch 252/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7080 - accuracy: 0.6640 - val_loss: 2.9569 - val_accuracy: 0.0323\n",
      "Epoch 253/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.7094 - accuracy: 0.6747 - val_loss: 2.9570 - val_accuracy: 0.0323\n",
      "Epoch 254/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.7010 - accuracy: 0.6788 - val_loss: 2.9525 - val_accuracy: 0.0323\n",
      "Epoch 255/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.7237 - accuracy: 0.6734 - val_loss: 2.9529 - val_accuracy: 0.0323\n",
      "Epoch 256/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7199 - accuracy: 0.6788 - val_loss: 2.9604 - val_accuracy: 0.0323\n",
      "Epoch 257/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7316 - accuracy: 0.6653 - val_loss: 2.9605 - val_accuracy: 0.0323\n",
      "Epoch 258/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7085 - accuracy: 0.6734 - val_loss: 2.9723 - val_accuracy: 0.0323\n",
      "Epoch 259/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7177 - accuracy: 0.6815 - val_loss: 2.9499 - val_accuracy: 0.0323\n",
      "Epoch 260/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.7256 - accuracy: 0.6747 - val_loss: 2.9420 - val_accuracy: 0.0323\n",
      "Epoch 261/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7351 - accuracy: 0.6573 - val_loss: 2.9699 - val_accuracy: 0.0323\n",
      "Epoch 262/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7049 - accuracy: 0.6841 - val_loss: 2.9409 - val_accuracy: 0.0323\n",
      "Epoch 263/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7357 - accuracy: 0.6599 - val_loss: 2.9620 - val_accuracy: 0.0323\n",
      "Epoch 264/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7155 - accuracy: 0.6761 - val_loss: 2.9756 - val_accuracy: 0.0323\n",
      "Epoch 265/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.7176 - accuracy: 0.6734 - val_loss: 2.9737 - val_accuracy: 0.0323\n",
      "Epoch 266/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6964 - accuracy: 0.6949 - val_loss: 2.9721 - val_accuracy: 0.0323\n",
      "Epoch 267/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.7115 - accuracy: 0.6882 - val_loss: 2.9620 - val_accuracy: 0.0323\n",
      "Epoch 268/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.7041 - accuracy: 0.6935 - val_loss: 2.9798 - val_accuracy: 0.0323\n",
      "Epoch 269/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7223 - accuracy: 0.6734 - val_loss: 2.9957 - val_accuracy: 0.0323\n",
      "Epoch 270/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.7150 - accuracy: 0.6801 - val_loss: 2.9998 - val_accuracy: 0.0323\n",
      "Epoch 271/1000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.7022 - accuracy: 0.6922 - val_loss: 2.9942 - val_accuracy: 0.0323\n",
      "Epoch 272/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7307 - accuracy: 0.6707 - val_loss: 2.9911 - val_accuracy: 0.0323\n",
      "Epoch 273/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7027 - accuracy: 0.6962 - val_loss: 2.9961 - val_accuracy: 0.0323\n",
      "Epoch 274/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6973 - accuracy: 0.7016 - val_loss: 2.9953 - val_accuracy: 0.0323\n",
      "Epoch 275/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6833 - accuracy: 0.6976 - val_loss: 2.9885 - val_accuracy: 0.0323\n",
      "Epoch 276/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7003 - accuracy: 0.7003 - val_loss: 3.0032 - val_accuracy: 0.0323\n",
      "Epoch 277/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.7216 - accuracy: 0.6707 - val_loss: 3.0040 - val_accuracy: 0.0323\n",
      "Epoch 278/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6986 - accuracy: 0.6922 - val_loss: 2.9899 - val_accuracy: 0.0323\n",
      "Epoch 279/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7139 - accuracy: 0.6734 - val_loss: 3.0226 - val_accuracy: 0.0323\n",
      "Epoch 280/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7019 - accuracy: 0.7164 - val_loss: 3.0068 - val_accuracy: 0.0323\n",
      "Epoch 281/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6875 - accuracy: 0.7043 - val_loss: 3.0139 - val_accuracy: 0.0323\n",
      "Epoch 282/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6954 - accuracy: 0.7070 - val_loss: 3.0225 - val_accuracy: 0.0323\n",
      "Epoch 283/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7001 - accuracy: 0.6909 - val_loss: 3.0176 - val_accuracy: 0.0323\n",
      "Epoch 284/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6959 - accuracy: 0.7003 - val_loss: 3.0195 - val_accuracy: 0.0323\n",
      "Epoch 285/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6830 - accuracy: 0.6935 - val_loss: 3.0383 - val_accuracy: 0.0323\n",
      "Epoch 286/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6823 - accuracy: 0.7070 - val_loss: 3.0486 - val_accuracy: 0.0323\n",
      "Epoch 287/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7077 - accuracy: 0.6855 - val_loss: 3.0405 - val_accuracy: 0.0323\n",
      "Epoch 288/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6914 - accuracy: 0.6962 - val_loss: 3.0246 - val_accuracy: 0.0323\n",
      "Epoch 289/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6892 - accuracy: 0.7056 - val_loss: 3.0353 - val_accuracy: 0.0323\n",
      "Epoch 290/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.7164 - accuracy: 0.6680 - val_loss: 3.0559 - val_accuracy: 0.0323\n",
      "Epoch 291/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6853 - accuracy: 0.7030 - val_loss: 3.0658 - val_accuracy: 0.0323\n",
      "Epoch 292/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7025 - accuracy: 0.6828 - val_loss: 3.0352 - val_accuracy: 0.0323\n",
      "Epoch 293/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6781 - accuracy: 0.7070 - val_loss: 3.0423 - val_accuracy: 0.0323\n",
      "Epoch 294/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6741 - accuracy: 0.7177 - val_loss: 3.0524 - val_accuracy: 0.0323\n",
      "Epoch 295/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6876 - accuracy: 0.6976 - val_loss: 3.0755 - val_accuracy: 0.0323\n",
      "Epoch 296/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6981 - accuracy: 0.7003 - val_loss: 3.0757 - val_accuracy: 0.0323\n",
      "Epoch 297/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6852 - accuracy: 0.6976 - val_loss: 3.0661 - val_accuracy: 0.0323\n",
      "Epoch 298/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6980 - accuracy: 0.6989 - val_loss: 3.0668 - val_accuracy: 0.0323\n",
      "Epoch 299/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6848 - accuracy: 0.7043 - val_loss: 3.0570 - val_accuracy: 0.0323\n",
      "Epoch 300/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6917 - accuracy: 0.7070 - val_loss: 3.0700 - val_accuracy: 0.0323\n",
      "Epoch 301/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7051 - accuracy: 0.6815 - val_loss: 3.0930 - val_accuracy: 0.0323\n",
      "Epoch 302/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6948 - accuracy: 0.6828 - val_loss: 3.0591 - val_accuracy: 0.0323\n",
      "Epoch 303/1000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.6966 - accuracy: 0.6989 - val_loss: 3.0779 - val_accuracy: 0.0323\n",
      "Epoch 304/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6915 - accuracy: 0.6828 - val_loss: 3.0892 - val_accuracy: 0.0323\n",
      "Epoch 305/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7121 - accuracy: 0.6855 - val_loss: 3.0585 - val_accuracy: 0.0323\n",
      "Epoch 306/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6956 - accuracy: 0.6801 - val_loss: 3.0669 - val_accuracy: 0.0323\n",
      "Epoch 307/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6792 - accuracy: 0.6882 - val_loss: 3.0719 - val_accuracy: 0.0323\n",
      "Epoch 308/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6930 - accuracy: 0.6895 - val_loss: 3.0638 - val_accuracy: 0.0323\n",
      "Epoch 309/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6874 - accuracy: 0.6976 - val_loss: 3.0914 - val_accuracy: 0.0323\n",
      "Epoch 310/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6581 - accuracy: 0.7231 - val_loss: 3.0974 - val_accuracy: 0.0323\n",
      "Epoch 311/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6700 - accuracy: 0.7110 - val_loss: 3.0888 - val_accuracy: 0.0323\n",
      "Epoch 312/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6645 - accuracy: 0.7245 - val_loss: 3.1286 - val_accuracy: 0.0323\n",
      "Epoch 313/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6891 - accuracy: 0.7124 - val_loss: 3.1102 - val_accuracy: 0.0323\n",
      "Epoch 314/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6884 - accuracy: 0.7056 - val_loss: 3.0975 - val_accuracy: 0.0323\n",
      "Epoch 315/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6791 - accuracy: 0.7003 - val_loss: 3.1433 - val_accuracy: 0.0323\n",
      "Epoch 316/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6672 - accuracy: 0.6935 - val_loss: 3.1034 - val_accuracy: 0.0323\n",
      "Epoch 317/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6845 - accuracy: 0.6962 - val_loss: 3.1136 - val_accuracy: 0.0323\n",
      "Epoch 318/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6791 - accuracy: 0.6935 - val_loss: 3.1207 - val_accuracy: 0.0323\n",
      "Epoch 319/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6804 - accuracy: 0.6949 - val_loss: 3.1191 - val_accuracy: 0.0323\n",
      "Epoch 320/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6546 - accuracy: 0.7285 - val_loss: 3.1348 - val_accuracy: 0.0323\n",
      "Epoch 321/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6979 - accuracy: 0.6935 - val_loss: 3.1197 - val_accuracy: 0.0323\n",
      "Epoch 322/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6846 - accuracy: 0.6828 - val_loss: 3.1142 - val_accuracy: 0.0323\n",
      "Epoch 323/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6762 - accuracy: 0.7016 - val_loss: 3.1284 - val_accuracy: 0.0323\n",
      "Epoch 324/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6788 - accuracy: 0.6962 - val_loss: 3.1391 - val_accuracy: 0.0323\n",
      "Epoch 325/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6627 - accuracy: 0.7177 - val_loss: 3.0992 - val_accuracy: 0.0323\n",
      "Epoch 326/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6535 - accuracy: 0.7097 - val_loss: 3.1437 - val_accuracy: 0.0323\n",
      "Epoch 327/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6615 - accuracy: 0.7097 - val_loss: 3.1285 - val_accuracy: 0.0323\n",
      "Epoch 328/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6660 - accuracy: 0.7043 - val_loss: 3.1484 - val_accuracy: 0.0323\n",
      "Epoch 329/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6884 - accuracy: 0.6895 - val_loss: 3.1826 - val_accuracy: 0.0323\n",
      "Epoch 330/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6609 - accuracy: 0.7204 - val_loss: 3.1658 - val_accuracy: 0.0323\n",
      "Epoch 331/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6566 - accuracy: 0.7110 - val_loss: 3.1491 - val_accuracy: 0.0323\n",
      "Epoch 332/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6665 - accuracy: 0.7097 - val_loss: 3.1637 - val_accuracy: 0.0323\n",
      "Epoch 333/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6716 - accuracy: 0.6922 - val_loss: 3.1424 - val_accuracy: 0.0323\n",
      "Epoch 334/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6738 - accuracy: 0.6962 - val_loss: 3.1680 - val_accuracy: 0.0323\n",
      "Epoch 335/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6852 - accuracy: 0.6989 - val_loss: 3.1595 - val_accuracy: 0.0323\n",
      "Epoch 336/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6537 - accuracy: 0.7231 - val_loss: 3.1646 - val_accuracy: 0.0323\n",
      "Epoch 337/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6847 - accuracy: 0.6895 - val_loss: 3.1884 - val_accuracy: 0.0323\n",
      "Epoch 338/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6483 - accuracy: 0.7446 - val_loss: 3.1686 - val_accuracy: 0.0323\n",
      "Epoch 339/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6605 - accuracy: 0.7204 - val_loss: 3.1174 - val_accuracy: 0.0323\n",
      "Epoch 340/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6485 - accuracy: 0.7110 - val_loss: 3.1819 - val_accuracy: 0.0323\n",
      "Epoch 341/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6691 - accuracy: 0.6895 - val_loss: 3.1942 - val_accuracy: 0.0323\n",
      "Epoch 342/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6703 - accuracy: 0.6774 - val_loss: 3.1850 - val_accuracy: 0.0323\n",
      "Epoch 343/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6640 - accuracy: 0.6989 - val_loss: 3.1743 - val_accuracy: 0.0323\n",
      "Epoch 344/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6840 - accuracy: 0.7070 - val_loss: 3.1755 - val_accuracy: 0.0323\n",
      "Epoch 345/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6773 - accuracy: 0.6882 - val_loss: 3.1736 - val_accuracy: 0.0323\n",
      "Epoch 346/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6434 - accuracy: 0.7379 - val_loss: 3.2283 - val_accuracy: 0.0323\n",
      "Epoch 347/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6305 - accuracy: 0.7460 - val_loss: 3.2903 - val_accuracy: 0.0323\n",
      "Epoch 348/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6484 - accuracy: 0.6949 - val_loss: 3.2650 - val_accuracy: 0.0323\n",
      "Epoch 349/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6526 - accuracy: 0.7406 - val_loss: 3.2995 - val_accuracy: 0.0323\n",
      "Epoch 350/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6529 - accuracy: 0.6935 - val_loss: 3.3667 - val_accuracy: 0.0323\n",
      "Epoch 351/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6298 - accuracy: 0.7406 - val_loss: 3.3426 - val_accuracy: 0.0323\n",
      "Epoch 352/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6274 - accuracy: 0.7460 - val_loss: 3.3641 - val_accuracy: 0.0323\n",
      "Epoch 353/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6341 - accuracy: 0.7621 - val_loss: 3.4347 - val_accuracy: 0.0323\n",
      "Epoch 354/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6374 - accuracy: 0.7245 - val_loss: 3.4180 - val_accuracy: 0.0323\n",
      "Epoch 355/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6334 - accuracy: 0.7433 - val_loss: 3.4515 - val_accuracy: 0.0323\n",
      "Epoch 356/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6273 - accuracy: 0.7366 - val_loss: 3.4793 - val_accuracy: 0.0323\n",
      "Epoch 357/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6319 - accuracy: 0.7392 - val_loss: 3.5077 - val_accuracy: 0.0323\n",
      "Epoch 358/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6137 - accuracy: 0.7419 - val_loss: 3.5388 - val_accuracy: 0.0323\n",
      "Epoch 359/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6146 - accuracy: 0.7513 - val_loss: 3.5716 - val_accuracy: 0.0323\n",
      "Epoch 360/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6216 - accuracy: 0.7379 - val_loss: 3.6356 - val_accuracy: 0.0323\n",
      "Epoch 361/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5931 - accuracy: 0.7446 - val_loss: 3.6773 - val_accuracy: 0.0323\n",
      "Epoch 362/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6251 - accuracy: 0.7419 - val_loss: 3.6915 - val_accuracy: 0.0323\n",
      "Epoch 363/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5927 - accuracy: 0.7527 - val_loss: 3.7395 - val_accuracy: 0.0323\n",
      "Epoch 364/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5965 - accuracy: 0.7473 - val_loss: 3.8089 - val_accuracy: 0.0323\n",
      "Epoch 365/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6076 - accuracy: 0.7487 - val_loss: 3.7891 - val_accuracy: 0.0323\n",
      "Epoch 366/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5814 - accuracy: 0.7608 - val_loss: 3.8193 - val_accuracy: 0.0323\n",
      "Epoch 367/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5859 - accuracy: 0.7527 - val_loss: 3.8493 - val_accuracy: 0.0323\n",
      "Epoch 368/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6087 - accuracy: 0.7487 - val_loss: 3.8794 - val_accuracy: 0.0323\n",
      "Epoch 369/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5959 - accuracy: 0.7554 - val_loss: 3.8783 - val_accuracy: 0.0323\n",
      "Epoch 370/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5910 - accuracy: 0.7540 - val_loss: 3.8964 - val_accuracy: 0.0323\n",
      "Epoch 371/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5870 - accuracy: 0.7527 - val_loss: 3.9198 - val_accuracy: 0.0323\n",
      "Epoch 372/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6048 - accuracy: 0.7419 - val_loss: 3.9400 - val_accuracy: 0.0323\n",
      "Epoch 373/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5956 - accuracy: 0.7473 - val_loss: 3.9412 - val_accuracy: 0.0323\n",
      "Epoch 374/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5950 - accuracy: 0.7487 - val_loss: 3.9279 - val_accuracy: 0.0323\n",
      "Epoch 375/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5773 - accuracy: 0.7742 - val_loss: 3.9721 - val_accuracy: 0.0323\n",
      "Epoch 376/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5922 - accuracy: 0.7366 - val_loss: 3.9969 - val_accuracy: 0.0323\n",
      "Epoch 377/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6050 - accuracy: 0.7406 - val_loss: 3.9981 - val_accuracy: 0.0323\n",
      "Epoch 378/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5890 - accuracy: 0.7554 - val_loss: 3.9683 - val_accuracy: 0.0323\n",
      "Epoch 379/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5955 - accuracy: 0.7742 - val_loss: 3.9777 - val_accuracy: 0.0323\n",
      "Epoch 380/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5869 - accuracy: 0.7473 - val_loss: 4.0750 - val_accuracy: 0.0323\n",
      "Epoch 381/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6018 - accuracy: 0.7581 - val_loss: 4.0498 - val_accuracy: 0.0323\n",
      "Epoch 382/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5793 - accuracy: 0.7540 - val_loss: 4.0444 - val_accuracy: 0.0323\n",
      "Epoch 383/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5834 - accuracy: 0.7527 - val_loss: 4.0991 - val_accuracy: 0.0323\n",
      "Epoch 384/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5973 - accuracy: 0.7473 - val_loss: 4.1580 - val_accuracy: 0.0323\n",
      "Epoch 385/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5793 - accuracy: 0.7554 - val_loss: 4.1126 - val_accuracy: 0.0323\n",
      "Epoch 386/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5724 - accuracy: 0.7567 - val_loss: 4.1234 - val_accuracy: 0.0323\n",
      "Epoch 387/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5946 - accuracy: 0.7594 - val_loss: 4.1385 - val_accuracy: 0.0323\n",
      "Epoch 388/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5709 - accuracy: 0.7702 - val_loss: 4.1597 - val_accuracy: 0.0323\n",
      "Epoch 389/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5674 - accuracy: 0.7675 - val_loss: 4.1573 - val_accuracy: 0.0323\n",
      "Epoch 390/1000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.5870 - accuracy: 0.7648 - val_loss: 4.1737 - val_accuracy: 0.0323\n",
      "Epoch 391/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5586 - accuracy: 0.7823 - val_loss: 4.2285 - val_accuracy: 0.0323\n",
      "Epoch 392/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5651 - accuracy: 0.7755 - val_loss: 4.2169 - val_accuracy: 0.0323\n",
      "Epoch 393/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5685 - accuracy: 0.7688 - val_loss: 4.2172 - val_accuracy: 0.0323\n",
      "Epoch 394/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5776 - accuracy: 0.7702 - val_loss: 4.2355 - val_accuracy: 0.0323\n",
      "Epoch 395/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5539 - accuracy: 0.7634 - val_loss: 4.2227 - val_accuracy: 0.0323\n",
      "Epoch 396/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5775 - accuracy: 0.7688 - val_loss: 4.2755 - val_accuracy: 0.0323\n",
      "Epoch 397/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5678 - accuracy: 0.7728 - val_loss: 4.2375 - val_accuracy: 0.0323\n",
      "Epoch 398/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5746 - accuracy: 0.7527 - val_loss: 4.2234 - val_accuracy: 0.0323\n",
      "Epoch 399/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5854 - accuracy: 0.7581 - val_loss: 4.2636 - val_accuracy: 0.0323\n",
      "Epoch 400/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5648 - accuracy: 0.7742 - val_loss: 4.3112 - val_accuracy: 0.0323\n",
      "Epoch 401/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5760 - accuracy: 0.7688 - val_loss: 4.2814 - val_accuracy: 0.0323\n",
      "Epoch 402/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5730 - accuracy: 0.7554 - val_loss: 4.3005 - val_accuracy: 0.0323\n",
      "Epoch 403/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5620 - accuracy: 0.7715 - val_loss: 4.3090 - val_accuracy: 0.0323\n",
      "Epoch 404/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5580 - accuracy: 0.7608 - val_loss: 4.2689 - val_accuracy: 0.0323\n",
      "Epoch 405/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5704 - accuracy: 0.7755 - val_loss: 4.2385 - val_accuracy: 0.0323\n",
      "Epoch 406/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5788 - accuracy: 0.7540 - val_loss: 4.2636 - val_accuracy: 0.0323\n",
      "Epoch 407/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5685 - accuracy: 0.7702 - val_loss: 4.2982 - val_accuracy: 0.0323\n",
      "Epoch 408/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5639 - accuracy: 0.7769 - val_loss: 4.3225 - val_accuracy: 0.0323\n",
      "Epoch 409/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5820 - accuracy: 0.7648 - val_loss: 4.3271 - val_accuracy: 0.0323\n",
      "Epoch 410/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5589 - accuracy: 0.7742 - val_loss: 4.3184 - val_accuracy: 0.0323\n",
      "Epoch 411/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5618 - accuracy: 0.7742 - val_loss: 4.2903 - val_accuracy: 0.0323\n",
      "Epoch 412/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5657 - accuracy: 0.7608 - val_loss: 4.3352 - val_accuracy: 0.0323\n",
      "Epoch 413/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5537 - accuracy: 0.7728 - val_loss: 4.3387 - val_accuracy: 0.0323\n",
      "Epoch 414/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5665 - accuracy: 0.7634 - val_loss: 4.3226 - val_accuracy: 0.0323\n",
      "Epoch 415/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5649 - accuracy: 0.7702 - val_loss: 4.3157 - val_accuracy: 0.0323\n",
      "Epoch 416/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5608 - accuracy: 0.7500 - val_loss: 4.3333 - val_accuracy: 0.0323\n",
      "Epoch 417/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5586 - accuracy: 0.7728 - val_loss: 4.3054 - val_accuracy: 0.0323\n",
      "Epoch 418/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5484 - accuracy: 0.7634 - val_loss: 4.3580 - val_accuracy: 0.0323\n",
      "Epoch 419/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5327 - accuracy: 0.7796 - val_loss: 4.3564 - val_accuracy: 0.0323\n",
      "Epoch 420/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5557 - accuracy: 0.7769 - val_loss: 4.3525 - val_accuracy: 0.0323\n",
      "Epoch 421/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5655 - accuracy: 0.7728 - val_loss: 4.3835 - val_accuracy: 0.0323\n",
      "Epoch 422/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5595 - accuracy: 0.7688 - val_loss: 4.3641 - val_accuracy: 0.0323\n",
      "Epoch 423/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5459 - accuracy: 0.7621 - val_loss: 4.3968 - val_accuracy: 0.0323\n",
      "Epoch 424/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5556 - accuracy: 0.7608 - val_loss: 4.4424 - val_accuracy: 0.0323\n",
      "Epoch 425/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5465 - accuracy: 0.7809 - val_loss: 4.3813 - val_accuracy: 0.0323\n",
      "Epoch 426/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5511 - accuracy: 0.7769 - val_loss: 4.3302 - val_accuracy: 0.0323\n",
      "Epoch 427/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5419 - accuracy: 0.7769 - val_loss: 4.4016 - val_accuracy: 0.0323\n",
      "Epoch 428/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5498 - accuracy: 0.7634 - val_loss: 4.4361 - val_accuracy: 0.0323\n",
      "Epoch 429/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5401 - accuracy: 0.7849 - val_loss: 4.3978 - val_accuracy: 0.0323\n",
      "Epoch 430/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5489 - accuracy: 0.7849 - val_loss: 4.3980 - val_accuracy: 0.0323\n",
      "Epoch 431/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5458 - accuracy: 0.7581 - val_loss: 4.4113 - val_accuracy: 0.0323\n",
      "Epoch 432/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5514 - accuracy: 0.7782 - val_loss: 4.3951 - val_accuracy: 0.0323\n",
      "Epoch 433/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5417 - accuracy: 0.7675 - val_loss: 4.4189 - val_accuracy: 0.0323\n",
      "Epoch 434/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5404 - accuracy: 0.7581 - val_loss: 4.4654 - val_accuracy: 0.0323\n",
      "Epoch 435/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5402 - accuracy: 0.7742 - val_loss: 4.4382 - val_accuracy: 0.0323\n",
      "Epoch 436/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5384 - accuracy: 0.7809 - val_loss: 4.4490 - val_accuracy: 0.0323\n",
      "Epoch 437/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5213 - accuracy: 0.7984 - val_loss: 4.4641 - val_accuracy: 0.0323\n",
      "Epoch 438/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5618 - accuracy: 0.7634 - val_loss: 4.5220 - val_accuracy: 0.0323\n",
      "Epoch 439/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5421 - accuracy: 0.7769 - val_loss: 4.4751 - val_accuracy: 0.0323\n",
      "Epoch 440/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5364 - accuracy: 0.7715 - val_loss: 4.4307 - val_accuracy: 0.0323\n",
      "Epoch 441/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5457 - accuracy: 0.7742 - val_loss: 4.4310 - val_accuracy: 0.0323\n",
      "Epoch 442/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5422 - accuracy: 0.7702 - val_loss: 4.4920 - val_accuracy: 0.0323\n",
      "Epoch 443/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5272 - accuracy: 0.7917 - val_loss: 4.4755 - val_accuracy: 0.0323\n",
      "Epoch 444/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5388 - accuracy: 0.7796 - val_loss: 4.4856 - val_accuracy: 0.0323\n",
      "Epoch 445/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5386 - accuracy: 0.7863 - val_loss: 4.4882 - val_accuracy: 0.0323\n",
      "Epoch 446/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5466 - accuracy: 0.7863 - val_loss: 4.4926 - val_accuracy: 0.0323\n",
      "Epoch 447/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5234 - accuracy: 0.7809 - val_loss: 4.5070 - val_accuracy: 0.0323\n",
      "Epoch 448/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5358 - accuracy: 0.7742 - val_loss: 4.5108 - val_accuracy: 0.0323\n",
      "Epoch 449/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5226 - accuracy: 0.7769 - val_loss: 4.4917 - val_accuracy: 0.0323\n",
      "Epoch 450/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5488 - accuracy: 0.7702 - val_loss: 4.5126 - val_accuracy: 0.0323\n",
      "Epoch 451/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5137 - accuracy: 0.7957 - val_loss: 4.4889 - val_accuracy: 0.0323\n",
      "Epoch 452/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5226 - accuracy: 0.7890 - val_loss: 4.4998 - val_accuracy: 0.0323\n",
      "Epoch 453/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5442 - accuracy: 0.7688 - val_loss: 4.5467 - val_accuracy: 0.0323\n",
      "Epoch 454/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5535 - accuracy: 0.7608 - val_loss: 4.4916 - val_accuracy: 0.0323\n",
      "Epoch 455/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5290 - accuracy: 0.7782 - val_loss: 4.4569 - val_accuracy: 0.0323\n",
      "Epoch 456/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5338 - accuracy: 0.7823 - val_loss: 4.5154 - val_accuracy: 0.0323\n",
      "Epoch 457/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5418 - accuracy: 0.7796 - val_loss: 4.5638 - val_accuracy: 0.0323\n",
      "Epoch 458/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5282 - accuracy: 0.7849 - val_loss: 4.5251 - val_accuracy: 0.0323\n",
      "Epoch 459/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5328 - accuracy: 0.7903 - val_loss: 4.5344 - val_accuracy: 0.0323\n",
      "Epoch 460/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5317 - accuracy: 0.7728 - val_loss: 4.5014 - val_accuracy: 0.0323\n",
      "Epoch 461/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5267 - accuracy: 0.7782 - val_loss: 4.5304 - val_accuracy: 0.0323\n",
      "Epoch 462/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5246 - accuracy: 0.7755 - val_loss: 4.5751 - val_accuracy: 0.0323\n",
      "Epoch 463/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5351 - accuracy: 0.7742 - val_loss: 4.5439 - val_accuracy: 0.0323\n",
      "Epoch 464/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5328 - accuracy: 0.7755 - val_loss: 4.5599 - val_accuracy: 0.0323\n",
      "Epoch 465/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5330 - accuracy: 0.7782 - val_loss: 4.5697 - val_accuracy: 0.0323\n",
      "Epoch 466/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5366 - accuracy: 0.7796 - val_loss: 4.5670 - val_accuracy: 0.0323\n",
      "Epoch 467/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5132 - accuracy: 0.7809 - val_loss: 4.5408 - val_accuracy: 0.0323\n",
      "Epoch 468/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5322 - accuracy: 0.7849 - val_loss: 4.5753 - val_accuracy: 0.0323\n",
      "Epoch 469/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5178 - accuracy: 0.7863 - val_loss: 4.5971 - val_accuracy: 0.0323\n",
      "Epoch 470/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5231 - accuracy: 0.7930 - val_loss: 4.5595 - val_accuracy: 0.0323\n",
      "Epoch 471/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5180 - accuracy: 0.7876 - val_loss: 4.5372 - val_accuracy: 0.0323\n",
      "Epoch 472/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5164 - accuracy: 0.7863 - val_loss: 4.6088 - val_accuracy: 0.0323\n",
      "Epoch 473/1000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.5121 - accuracy: 0.7836 - val_loss: 4.6382 - val_accuracy: 0.0323\n",
      "Epoch 474/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5316 - accuracy: 0.7876 - val_loss: 4.6056 - val_accuracy: 0.0323\n",
      "Epoch 475/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5337 - accuracy: 0.7836 - val_loss: 4.5833 - val_accuracy: 0.0323\n",
      "Epoch 476/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5206 - accuracy: 0.7890 - val_loss: 4.6266 - val_accuracy: 0.0323\n",
      "Epoch 477/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5119 - accuracy: 0.7890 - val_loss: 4.6317 - val_accuracy: 0.0323\n",
      "Epoch 478/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5232 - accuracy: 0.7903 - val_loss: 4.6219 - val_accuracy: 0.0323\n",
      "Epoch 479/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5239 - accuracy: 0.7890 - val_loss: 4.6305 - val_accuracy: 0.0323\n",
      "Epoch 480/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5147 - accuracy: 0.7782 - val_loss: 4.6786 - val_accuracy: 0.0323\n",
      "Epoch 481/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5208 - accuracy: 0.7755 - val_loss: 4.6780 - val_accuracy: 0.0323\n",
      "Epoch 482/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5270 - accuracy: 0.7540 - val_loss: 4.6932 - val_accuracy: 0.0323\n",
      "Epoch 483/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5251 - accuracy: 0.7782 - val_loss: 4.6333 - val_accuracy: 0.0323\n",
      "Epoch 484/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5091 - accuracy: 0.7702 - val_loss: 4.6758 - val_accuracy: 0.0323\n",
      "Epoch 485/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5122 - accuracy: 0.7782 - val_loss: 4.6909 - val_accuracy: 0.0323\n",
      "Epoch 486/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5209 - accuracy: 0.7755 - val_loss: 4.7451 - val_accuracy: 0.0323\n",
      "Epoch 487/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5361 - accuracy: 0.7581 - val_loss: 4.6793 - val_accuracy: 0.0323\n",
      "Epoch 488/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5014 - accuracy: 0.7782 - val_loss: 4.6740 - val_accuracy: 0.0323\n",
      "Epoch 489/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4977 - accuracy: 0.7796 - val_loss: 4.7566 - val_accuracy: 0.0323\n",
      "Epoch 490/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5089 - accuracy: 0.7728 - val_loss: 4.7469 - val_accuracy: 0.0323\n",
      "Epoch 491/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5069 - accuracy: 0.7823 - val_loss: 4.6916 - val_accuracy: 0.0323\n",
      "Epoch 492/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4975 - accuracy: 0.7836 - val_loss: 4.6481 - val_accuracy: 0.0323\n",
      "Epoch 493/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5046 - accuracy: 0.7755 - val_loss: 4.7514 - val_accuracy: 0.0323\n",
      "Epoch 494/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5013 - accuracy: 0.7970 - val_loss: 4.7584 - val_accuracy: 0.0323\n",
      "Epoch 495/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4952 - accuracy: 0.7769 - val_loss: 4.6890 - val_accuracy: 0.0323\n",
      "Epoch 496/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5232 - accuracy: 0.7970 - val_loss: 4.7301 - val_accuracy: 0.0323\n",
      "Epoch 497/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5274 - accuracy: 0.7849 - val_loss: 4.8013 - val_accuracy: 0.0323\n",
      "Epoch 498/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5071 - accuracy: 0.8011 - val_loss: 4.8048 - val_accuracy: 0.0323\n",
      "Epoch 499/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4906 - accuracy: 0.7944 - val_loss: 4.7878 - val_accuracy: 0.0323\n",
      "Epoch 500/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5138 - accuracy: 0.7944 - val_loss: 4.7870 - val_accuracy: 0.0323\n",
      "Epoch 501/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4968 - accuracy: 0.8051 - val_loss: 4.8034 - val_accuracy: 0.0323\n",
      "Epoch 502/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5035 - accuracy: 0.7997 - val_loss: 4.8110 - val_accuracy: 0.0323\n",
      "Epoch 503/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4973 - accuracy: 0.7849 - val_loss: 4.8356 - val_accuracy: 0.0323\n",
      "Epoch 504/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5061 - accuracy: 0.7688 - val_loss: 4.8475 - val_accuracy: 0.0323\n",
      "Epoch 505/1000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4881 - accuracy: 0.7796 - val_loss: 4.7993 - val_accuracy: 0.0323\n",
      "Epoch 506/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5076 - accuracy: 0.7782 - val_loss: 4.7877 - val_accuracy: 0.0323\n",
      "Epoch 507/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5101 - accuracy: 0.7728 - val_loss: 4.8299 - val_accuracy: 0.0323\n",
      "Epoch 508/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4946 - accuracy: 0.7823 - val_loss: 4.9136 - val_accuracy: 0.0323\n",
      "Epoch 509/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4951 - accuracy: 0.7742 - val_loss: 4.8373 - val_accuracy: 0.0323\n",
      "Epoch 510/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4976 - accuracy: 0.7782 - val_loss: 4.8497 - val_accuracy: 0.0323\n",
      "Epoch 511/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5038 - accuracy: 0.7702 - val_loss: 4.8563 - val_accuracy: 0.0323\n",
      "Epoch 512/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4864 - accuracy: 0.7903 - val_loss: 4.8853 - val_accuracy: 0.0323\n",
      "Epoch 513/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4883 - accuracy: 0.7997 - val_loss: 4.9229 - val_accuracy: 0.0323\n",
      "Epoch 514/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5054 - accuracy: 0.7890 - val_loss: 4.8766 - val_accuracy: 0.0323\n",
      "Epoch 515/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5097 - accuracy: 0.7769 - val_loss: 4.8989 - val_accuracy: 0.0323\n",
      "Epoch 516/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4920 - accuracy: 0.7849 - val_loss: 4.9010 - val_accuracy: 0.0323\n",
      "Epoch 517/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5077 - accuracy: 0.7823 - val_loss: 4.8666 - val_accuracy: 0.0323\n",
      "Epoch 518/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5061 - accuracy: 0.7769 - val_loss: 4.9327 - val_accuracy: 0.0323\n",
      "Epoch 519/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4929 - accuracy: 0.7769 - val_loss: 4.9192 - val_accuracy: 0.0323\n",
      "Epoch 520/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4801 - accuracy: 0.7823 - val_loss: 4.9552 - val_accuracy: 0.0323\n",
      "Epoch 521/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4713 - accuracy: 0.7917 - val_loss: 4.9862 - val_accuracy: 0.0323\n",
      "Epoch 522/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4884 - accuracy: 0.7769 - val_loss: 4.9639 - val_accuracy: 0.0323\n",
      "Epoch 523/1000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4778 - accuracy: 0.7917 - val_loss: 4.9519 - val_accuracy: 0.0323\n",
      "Epoch 524/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4880 - accuracy: 0.7970 - val_loss: 4.9454 - val_accuracy: 0.0323\n",
      "Epoch 525/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5001 - accuracy: 0.7876 - val_loss: 4.9888 - val_accuracy: 0.0323\n",
      "Epoch 526/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4819 - accuracy: 0.7970 - val_loss: 4.9993 - val_accuracy: 0.0323\n",
      "Epoch 527/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4769 - accuracy: 0.7903 - val_loss: 5.0036 - val_accuracy: 0.0323\n",
      "Epoch 528/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5230 - accuracy: 0.7742 - val_loss: 4.9439 - val_accuracy: 0.0323\n",
      "Epoch 529/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4839 - accuracy: 0.7944 - val_loss: 4.9655 - val_accuracy: 0.0323\n",
      "Epoch 530/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4912 - accuracy: 0.7997 - val_loss: 5.0099 - val_accuracy: 0.0323\n",
      "Epoch 531/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4823 - accuracy: 0.7863 - val_loss: 4.9788 - val_accuracy: 0.0323\n",
      "Epoch 532/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5000 - accuracy: 0.7876 - val_loss: 4.9654 - val_accuracy: 0.0323\n",
      "Epoch 533/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5011 - accuracy: 0.7755 - val_loss: 4.9971 - val_accuracy: 0.0323\n",
      "Epoch 534/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4867 - accuracy: 0.8051 - val_loss: 5.0259 - val_accuracy: 0.0323\n",
      "Epoch 535/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4840 - accuracy: 0.8038 - val_loss: 5.0586 - val_accuracy: 0.0323\n",
      "Epoch 536/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4804 - accuracy: 0.8239 - val_loss: 5.1122 - val_accuracy: 0.0323\n",
      "Epoch 537/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4891 - accuracy: 0.8159 - val_loss: 5.0843 - val_accuracy: 0.0323\n",
      "Epoch 538/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4881 - accuracy: 0.8132 - val_loss: 5.0449 - val_accuracy: 0.0323\n",
      "Epoch 539/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4871 - accuracy: 0.8159 - val_loss: 5.0474 - val_accuracy: 0.0323\n",
      "Epoch 540/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4877 - accuracy: 0.8145 - val_loss: 5.0615 - val_accuracy: 0.0323\n",
      "Epoch 541/1000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4728 - accuracy: 0.8333 - val_loss: 5.1194 - val_accuracy: 0.0323\n",
      "Epoch 542/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4697 - accuracy: 0.8266 - val_loss: 5.0691 - val_accuracy: 0.0323\n",
      "Epoch 543/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4831 - accuracy: 0.8253 - val_loss: 5.0882 - val_accuracy: 0.0323\n",
      "Epoch 544/1000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4795 - accuracy: 0.8199 - val_loss: 5.1267 - val_accuracy: 0.0323\n",
      "Epoch 545/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4839 - accuracy: 0.8226 - val_loss: 5.2000 - val_accuracy: 0.0323\n",
      "Epoch 546/1000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4818 - accuracy: 0.8239 - val_loss: 5.0995 - val_accuracy: 0.0323\n",
      "Epoch 547/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4838 - accuracy: 0.8118 - val_loss: 5.1046 - val_accuracy: 0.0323\n",
      "Epoch 548/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4748 - accuracy: 0.8280 - val_loss: 5.0999 - val_accuracy: 0.0323\n",
      "Epoch 549/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5011 - accuracy: 0.8118 - val_loss: 5.1724 - val_accuracy: 0.0323\n",
      "Epoch 550/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4776 - accuracy: 0.8280 - val_loss: 5.1335 - val_accuracy: 0.0323\n",
      "Epoch 551/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4782 - accuracy: 0.8118 - val_loss: 5.1455 - val_accuracy: 0.0323\n",
      "Epoch 552/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4980 - accuracy: 0.8078 - val_loss: 5.0818 - val_accuracy: 0.0323\n",
      "Epoch 553/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4839 - accuracy: 0.7984 - val_loss: 5.1655 - val_accuracy: 0.0323\n",
      "Epoch 554/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4983 - accuracy: 0.8011 - val_loss: 5.1720 - val_accuracy: 0.0323\n",
      "Epoch 555/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4636 - accuracy: 0.8253 - val_loss: 5.2277 - val_accuracy: 0.0323\n",
      "Epoch 556/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4892 - accuracy: 0.8172 - val_loss: 5.1860 - val_accuracy: 0.0323\n",
      "Epoch 557/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4501 - accuracy: 0.8320 - val_loss: 5.2219 - val_accuracy: 0.0323\n",
      "Epoch 558/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5013 - accuracy: 0.8038 - val_loss: 5.1886 - val_accuracy: 0.0323\n",
      "Epoch 559/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4838 - accuracy: 0.8118 - val_loss: 5.1867 - val_accuracy: 0.0323\n",
      "Epoch 560/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4655 - accuracy: 0.8172 - val_loss: 5.1881 - val_accuracy: 0.0323\n",
      "Epoch 561/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4688 - accuracy: 0.8185 - val_loss: 5.2986 - val_accuracy: 0.0323\n",
      "Epoch 562/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4858 - accuracy: 0.8212 - val_loss: 5.2657 - val_accuracy: 0.0323\n",
      "Epoch 563/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4916 - accuracy: 0.8226 - val_loss: 5.2183 - val_accuracy: 0.0323\n",
      "Epoch 564/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4860 - accuracy: 0.8199 - val_loss: 5.2424 - val_accuracy: 0.0323\n",
      "Epoch 565/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4891 - accuracy: 0.8239 - val_loss: 5.2990 - val_accuracy: 0.0323\n",
      "Epoch 566/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4593 - accuracy: 0.8374 - val_loss: 5.2809 - val_accuracy: 0.0323\n",
      "Epoch 567/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4838 - accuracy: 0.8132 - val_loss: 5.2085 - val_accuracy: 0.0323\n",
      "Epoch 568/1000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4862 - accuracy: 0.8172 - val_loss: 5.1687 - val_accuracy: 0.0323\n",
      "Epoch 569/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4884 - accuracy: 0.8038 - val_loss: 5.2610 - val_accuracy: 0.0323\n",
      "Epoch 570/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4731 - accuracy: 0.8253 - val_loss: 5.3181 - val_accuracy: 0.0323\n",
      "Epoch 571/1000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4592 - accuracy: 0.8320 - val_loss: 5.3679 - val_accuracy: 0.0323\n",
      "Epoch 572/1000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4664 - accuracy: 0.8253 - val_loss: 5.2892 - val_accuracy: 0.0323\n",
      "Epoch 573/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4716 - accuracy: 0.8212 - val_loss: 5.2923 - val_accuracy: 0.0323\n",
      "Epoch 574/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4519 - accuracy: 0.8387 - val_loss: 5.2648 - val_accuracy: 0.0323\n",
      "Epoch 575/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4683 - accuracy: 0.8266 - val_loss: 5.3257 - val_accuracy: 0.0323\n",
      "Epoch 576/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4623 - accuracy: 0.8360 - val_loss: 5.3551 - val_accuracy: 0.0323\n",
      "Epoch 577/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4838 - accuracy: 0.8145 - val_loss: 5.2788 - val_accuracy: 0.0323\n",
      "Epoch 578/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4647 - accuracy: 0.8266 - val_loss: 5.2279 - val_accuracy: 0.0323\n",
      "Epoch 579/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4846 - accuracy: 0.8078 - val_loss: 5.2813 - val_accuracy: 0.0323\n",
      "Epoch 580/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4681 - accuracy: 0.8253 - val_loss: 5.3319 - val_accuracy: 0.0323\n",
      "Epoch 581/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4704 - accuracy: 0.8293 - val_loss: 5.3416 - val_accuracy: 0.0323\n",
      "Epoch 582/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4578 - accuracy: 0.8320 - val_loss: 5.3317 - val_accuracy: 0.0323\n",
      "Epoch 583/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4698 - accuracy: 0.8306 - val_loss: 5.3016 - val_accuracy: 0.0323\n",
      "Epoch 584/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4530 - accuracy: 0.8387 - val_loss: 5.3992 - val_accuracy: 0.0323\n",
      "Epoch 585/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4634 - accuracy: 0.8253 - val_loss: 5.3949 - val_accuracy: 0.0323\n",
      "Epoch 586/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4719 - accuracy: 0.8239 - val_loss: 5.4404 - val_accuracy: 0.0323\n",
      "Epoch 587/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4619 - accuracy: 0.8280 - val_loss: 5.3585 - val_accuracy: 0.0323\n",
      "Epoch 588/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4815 - accuracy: 0.8105 - val_loss: 5.4078 - val_accuracy: 0.0323\n",
      "Epoch 589/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4747 - accuracy: 0.8132 - val_loss: 5.4206 - val_accuracy: 0.0323\n",
      "Epoch 590/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4791 - accuracy: 0.8226 - val_loss: 5.4447 - val_accuracy: 0.0323\n",
      "Epoch 591/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4618 - accuracy: 0.8266 - val_loss: 5.4567 - val_accuracy: 0.0323\n",
      "Epoch 592/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4720 - accuracy: 0.8360 - val_loss: 5.4294 - val_accuracy: 0.0323\n",
      "Epoch 593/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4600 - accuracy: 0.8427 - val_loss: 5.4168 - val_accuracy: 0.0323\n",
      "Epoch 594/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4595 - accuracy: 0.8320 - val_loss: 5.4620 - val_accuracy: 0.0323\n",
      "Epoch 595/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4723 - accuracy: 0.8239 - val_loss: 5.4321 - val_accuracy: 0.0323\n",
      "Epoch 596/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4797 - accuracy: 0.8091 - val_loss: 5.4366 - val_accuracy: 0.0323\n",
      "Epoch 597/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4450 - accuracy: 0.8562 - val_loss: 5.4314 - val_accuracy: 0.0323\n",
      "Epoch 598/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4669 - accuracy: 0.8212 - val_loss: 5.4856 - val_accuracy: 0.0323\n",
      "Epoch 599/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4676 - accuracy: 0.8185 - val_loss: 5.4439 - val_accuracy: 0.0323\n",
      "Epoch 600/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4443 - accuracy: 0.8333 - val_loss: 5.4615 - val_accuracy: 0.0323\n",
      "Epoch 601/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4593 - accuracy: 0.8280 - val_loss: 5.4515 - val_accuracy: 0.0323\n",
      "Epoch 602/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4694 - accuracy: 0.8199 - val_loss: 5.5242 - val_accuracy: 0.0323\n",
      "Epoch 603/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4469 - accuracy: 0.8454 - val_loss: 5.5138 - val_accuracy: 0.0323\n",
      "Epoch 604/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4672 - accuracy: 0.8199 - val_loss: 5.5168 - val_accuracy: 0.0323\n",
      "Epoch 605/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4660 - accuracy: 0.8306 - val_loss: 5.4746 - val_accuracy: 0.0323\n",
      "Epoch 606/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4505 - accuracy: 0.8333 - val_loss: 5.5217 - val_accuracy: 0.0323\n",
      "Epoch 607/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4646 - accuracy: 0.8145 - val_loss: 5.4711 - val_accuracy: 0.0323\n",
      "Epoch 608/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4739 - accuracy: 0.8293 - val_loss: 5.4926 - val_accuracy: 0.0323\n",
      "Epoch 609/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4466 - accuracy: 0.8347 - val_loss: 5.5754 - val_accuracy: 0.0323\n",
      "Epoch 610/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4593 - accuracy: 0.8441 - val_loss: 5.5797 - val_accuracy: 0.0323\n",
      "Epoch 611/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4450 - accuracy: 0.8387 - val_loss: 5.5491 - val_accuracy: 0.0323\n",
      "Epoch 612/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4398 - accuracy: 0.8387 - val_loss: 5.5358 - val_accuracy: 0.0323\n",
      "Epoch 613/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4412 - accuracy: 0.8360 - val_loss: 5.5012 - val_accuracy: 0.0323\n",
      "Epoch 614/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4714 - accuracy: 0.8212 - val_loss: 5.5442 - val_accuracy: 0.0323\n",
      "Epoch 615/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4573 - accuracy: 0.8306 - val_loss: 5.5965 - val_accuracy: 0.0323\n",
      "Epoch 616/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4584 - accuracy: 0.8333 - val_loss: 5.6285 - val_accuracy: 0.0323\n",
      "Epoch 617/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4477 - accuracy: 0.8374 - val_loss: 5.5970 - val_accuracy: 0.0323\n",
      "Epoch 618/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4650 - accuracy: 0.8374 - val_loss: 5.5711 - val_accuracy: 0.0323\n",
      "Epoch 619/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4398 - accuracy: 0.8427 - val_loss: 5.5959 - val_accuracy: 0.0323\n",
      "Epoch 620/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4599 - accuracy: 0.8347 - val_loss: 5.6244 - val_accuracy: 0.0323\n",
      "Epoch 621/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4542 - accuracy: 0.8387 - val_loss: 5.6643 - val_accuracy: 0.0323\n",
      "Epoch 622/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4581 - accuracy: 0.8280 - val_loss: 5.6728 - val_accuracy: 0.0323\n",
      "Epoch 623/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4726 - accuracy: 0.8172 - val_loss: 5.6799 - val_accuracy: 0.0323\n",
      "Epoch 624/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4502 - accuracy: 0.8414 - val_loss: 5.6141 - val_accuracy: 0.0323\n",
      "Epoch 625/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4265 - accuracy: 0.8401 - val_loss: 5.6056 - val_accuracy: 0.0323\n",
      "Epoch 626/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4455 - accuracy: 0.8454 - val_loss: 5.6857 - val_accuracy: 0.0323\n",
      "Epoch 627/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4503 - accuracy: 0.8387 - val_loss: 5.6604 - val_accuracy: 0.0323\n",
      "Epoch 628/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4394 - accuracy: 0.8535 - val_loss: 5.6291 - val_accuracy: 0.0323\n",
      "Epoch 629/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4525 - accuracy: 0.8374 - val_loss: 5.6756 - val_accuracy: 0.0323\n",
      "Epoch 630/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4440 - accuracy: 0.8401 - val_loss: 5.6520 - val_accuracy: 0.0323\n",
      "Epoch 631/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4334 - accuracy: 0.8481 - val_loss: 5.7142 - val_accuracy: 0.0323\n",
      "Epoch 632/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4431 - accuracy: 0.8414 - val_loss: 5.7798 - val_accuracy: 0.0323\n",
      "Epoch 633/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4579 - accuracy: 0.8360 - val_loss: 5.7913 - val_accuracy: 0.0323\n",
      "Epoch 634/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4426 - accuracy: 0.8401 - val_loss: 5.7486 - val_accuracy: 0.0323\n",
      "Epoch 635/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4553 - accuracy: 0.8320 - val_loss: 5.7235 - val_accuracy: 0.0323\n",
      "Epoch 636/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4479 - accuracy: 0.8333 - val_loss: 5.7385 - val_accuracy: 0.0323\n",
      "Epoch 637/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4494 - accuracy: 0.8347 - val_loss: 5.7470 - val_accuracy: 0.0323\n",
      "Epoch 638/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4253 - accuracy: 0.8347 - val_loss: 5.7212 - val_accuracy: 0.0323\n",
      "Epoch 639/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4347 - accuracy: 0.8333 - val_loss: 5.7487 - val_accuracy: 0.0323\n",
      "Epoch 640/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4354 - accuracy: 0.8441 - val_loss: 5.7071 - val_accuracy: 0.0323\n",
      "Epoch 641/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4602 - accuracy: 0.8306 - val_loss: 5.7782 - val_accuracy: 0.0323\n",
      "Epoch 642/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4504 - accuracy: 0.8374 - val_loss: 5.7527 - val_accuracy: 0.0323\n",
      "Epoch 643/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4481 - accuracy: 0.8414 - val_loss: 5.7810 - val_accuracy: 0.0323\n",
      "Epoch 644/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4477 - accuracy: 0.8401 - val_loss: 5.7122 - val_accuracy: 0.0323\n",
      "Epoch 645/1000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4374 - accuracy: 0.8481 - val_loss: 5.6847 - val_accuracy: 0.0323\n",
      "Epoch 646/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4364 - accuracy: 0.8441 - val_loss: 5.6986 - val_accuracy: 0.0323\n",
      "Epoch 647/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4444 - accuracy: 0.8548 - val_loss: 5.7998 - val_accuracy: 0.0323\n",
      "Epoch 648/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4503 - accuracy: 0.8347 - val_loss: 5.8310 - val_accuracy: 0.0323\n",
      "Epoch 649/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4302 - accuracy: 0.8589 - val_loss: 5.8602 - val_accuracy: 0.0323\n",
      "Epoch 650/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4422 - accuracy: 0.8320 - val_loss: 5.8324 - val_accuracy: 0.0323\n",
      "Epoch 651/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4230 - accuracy: 0.8562 - val_loss: 5.8216 - val_accuracy: 0.0323\n",
      "Epoch 652/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4537 - accuracy: 0.8293 - val_loss: 5.8897 - val_accuracy: 0.0323\n",
      "Epoch 653/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4416 - accuracy: 0.8495 - val_loss: 5.8532 - val_accuracy: 0.0323\n",
      "Epoch 654/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4226 - accuracy: 0.8401 - val_loss: 5.8716 - val_accuracy: 0.0323\n",
      "Epoch 655/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4193 - accuracy: 0.8522 - val_loss: 5.8235 - val_accuracy: 0.0323\n",
      "Epoch 656/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4222 - accuracy: 0.8427 - val_loss: 5.8836 - val_accuracy: 0.0323\n",
      "Epoch 657/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4406 - accuracy: 0.8374 - val_loss: 5.8607 - val_accuracy: 0.0323\n",
      "Epoch 658/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4116 - accuracy: 0.8535 - val_loss: 5.8689 - val_accuracy: 0.0323\n",
      "Epoch 659/1000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4434 - accuracy: 0.8320 - val_loss: 5.8190 - val_accuracy: 0.0323\n",
      "Epoch 660/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4264 - accuracy: 0.8508 - val_loss: 5.9293 - val_accuracy: 0.0323\n",
      "Epoch 661/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4229 - accuracy: 0.8481 - val_loss: 5.9399 - val_accuracy: 0.0323\n",
      "Epoch 662/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4197 - accuracy: 0.8454 - val_loss: 6.0016 - val_accuracy: 0.0323\n",
      "Epoch 663/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4383 - accuracy: 0.8481 - val_loss: 5.8483 - val_accuracy: 0.0323\n",
      "Epoch 664/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4204 - accuracy: 0.8441 - val_loss: 5.7675 - val_accuracy: 0.0323\n",
      "Epoch 665/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4271 - accuracy: 0.8602 - val_loss: 5.7777 - val_accuracy: 0.0323\n",
      "Epoch 666/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4233 - accuracy: 0.8575 - val_loss: 5.9259 - val_accuracy: 0.0323\n",
      "Epoch 667/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4510 - accuracy: 0.8427 - val_loss: 5.9370 - val_accuracy: 0.0323\n",
      "Epoch 668/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4245 - accuracy: 0.8589 - val_loss: 5.8814 - val_accuracy: 0.0323\n",
      "Epoch 669/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4303 - accuracy: 0.8589 - val_loss: 5.9141 - val_accuracy: 0.0323\n",
      "Epoch 670/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4260 - accuracy: 0.8602 - val_loss: 5.9408 - val_accuracy: 0.0323\n",
      "Epoch 671/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4421 - accuracy: 0.8468 - val_loss: 5.9838 - val_accuracy: 0.0323\n",
      "Epoch 672/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4226 - accuracy: 0.8629 - val_loss: 5.9945 - val_accuracy: 0.0323\n",
      "Epoch 673/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4272 - accuracy: 0.8602 - val_loss: 5.9964 - val_accuracy: 0.0323\n",
      "Epoch 674/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4399 - accuracy: 0.8575 - val_loss: 6.0411 - val_accuracy: 0.0323\n",
      "Epoch 675/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4247 - accuracy: 0.8562 - val_loss: 5.9745 - val_accuracy: 0.0323\n",
      "Epoch 676/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4277 - accuracy: 0.8602 - val_loss: 6.0410 - val_accuracy: 0.0323\n",
      "Epoch 677/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4201 - accuracy: 0.8589 - val_loss: 6.0387 - val_accuracy: 0.0323\n",
      "Epoch 678/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4142 - accuracy: 0.8642 - val_loss: 6.0123 - val_accuracy: 0.0323\n",
      "Epoch 679/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3989 - accuracy: 0.8804 - val_loss: 6.0263 - val_accuracy: 0.0323\n",
      "Epoch 680/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4248 - accuracy: 0.8414 - val_loss: 6.0272 - val_accuracy: 0.0323\n",
      "Epoch 681/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4054 - accuracy: 0.8763 - val_loss: 6.0513 - val_accuracy: 0.0323\n",
      "Epoch 682/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4123 - accuracy: 0.8710 - val_loss: 6.0686 - val_accuracy: 0.0323\n",
      "Epoch 683/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4256 - accuracy: 0.8750 - val_loss: 6.1053 - val_accuracy: 0.0323\n",
      "Epoch 684/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4248 - accuracy: 0.8683 - val_loss: 6.1317 - val_accuracy: 0.0323\n",
      "Epoch 685/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4199 - accuracy: 0.8669 - val_loss: 6.0983 - val_accuracy: 0.0323\n",
      "Epoch 686/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4225 - accuracy: 0.8508 - val_loss: 6.0272 - val_accuracy: 0.0323\n",
      "Epoch 687/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4031 - accuracy: 0.8508 - val_loss: 5.9679 - val_accuracy: 0.0323\n",
      "Epoch 688/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4263 - accuracy: 0.8804 - val_loss: 6.0231 - val_accuracy: 0.0323\n",
      "Epoch 689/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4266 - accuracy: 0.8616 - val_loss: 6.1806 - val_accuracy: 0.0323\n",
      "Epoch 690/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4182 - accuracy: 0.8548 - val_loss: 6.1570 - val_accuracy: 0.0323\n",
      "Epoch 691/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4191 - accuracy: 0.8508 - val_loss: 6.0595 - val_accuracy: 0.0323\n",
      "Epoch 692/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4310 - accuracy: 0.8589 - val_loss: 6.0283 - val_accuracy: 0.0323\n",
      "Epoch 693/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4316 - accuracy: 0.8522 - val_loss: 6.0812 - val_accuracy: 0.0323\n",
      "Epoch 694/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4173 - accuracy: 0.8602 - val_loss: 6.1244 - val_accuracy: 0.0323\n",
      "Epoch 695/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4236 - accuracy: 0.8642 - val_loss: 6.1085 - val_accuracy: 0.0323\n",
      "Epoch 696/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4037 - accuracy: 0.8763 - val_loss: 6.1975 - val_accuracy: 0.0323\n",
      "Epoch 697/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4320 - accuracy: 0.8468 - val_loss: 6.2156 - val_accuracy: 0.0323\n",
      "Epoch 698/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4101 - accuracy: 0.8602 - val_loss: 6.1945 - val_accuracy: 0.0323\n",
      "Epoch 699/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4217 - accuracy: 0.8683 - val_loss: 6.2138 - val_accuracy: 0.0323\n",
      "Epoch 700/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4191 - accuracy: 0.8589 - val_loss: 6.2490 - val_accuracy: 0.0323\n",
      "Epoch 701/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4279 - accuracy: 0.8468 - val_loss: 6.2779 - val_accuracy: 0.0323\n",
      "Epoch 702/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4146 - accuracy: 0.8589 - val_loss: 6.1728 - val_accuracy: 0.0323\n",
      "Epoch 703/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4073 - accuracy: 0.8642 - val_loss: 6.2152 - val_accuracy: 0.0323\n",
      "Epoch 704/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4199 - accuracy: 0.8669 - val_loss: 6.2919 - val_accuracy: 0.0323\n",
      "Epoch 705/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4050 - accuracy: 0.8602 - val_loss: 6.3134 - val_accuracy: 0.0323\n",
      "Epoch 706/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4383 - accuracy: 0.8401 - val_loss: 6.1968 - val_accuracy: 0.0323\n",
      "Epoch 707/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4319 - accuracy: 0.8589 - val_loss: 6.1879 - val_accuracy: 0.0323\n",
      "Epoch 708/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4179 - accuracy: 0.8710 - val_loss: 6.2220 - val_accuracy: 0.0323\n",
      "Epoch 709/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4012 - accuracy: 0.8642 - val_loss: 6.2062 - val_accuracy: 0.0323\n",
      "Epoch 710/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4211 - accuracy: 0.8656 - val_loss: 6.2308 - val_accuracy: 0.0323\n",
      "Epoch 711/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3874 - accuracy: 0.8763 - val_loss: 6.2350 - val_accuracy: 0.0323\n",
      "Epoch 712/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4062 - accuracy: 0.8710 - val_loss: 6.3002 - val_accuracy: 0.0323\n",
      "Epoch 713/1000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4200 - accuracy: 0.8629 - val_loss: 6.2944 - val_accuracy: 0.0323\n",
      "Epoch 714/1000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.3974 - accuracy: 0.8750 - val_loss: 6.2735 - val_accuracy: 0.0323\n",
      "Epoch 715/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3970 - accuracy: 0.8656 - val_loss: 6.3473 - val_accuracy: 0.0323\n",
      "Epoch 716/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4254 - accuracy: 0.8495 - val_loss: 6.3503 - val_accuracy: 0.0323\n",
      "Epoch 717/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4162 - accuracy: 0.8454 - val_loss: 6.4106 - val_accuracy: 0.0323\n",
      "Epoch 718/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3812 - accuracy: 0.8763 - val_loss: 6.3784 - val_accuracy: 0.0323\n",
      "Epoch 719/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4096 - accuracy: 0.8548 - val_loss: 6.4323 - val_accuracy: 0.0323\n",
      "Epoch 720/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4086 - accuracy: 0.8710 - val_loss: 6.4412 - val_accuracy: 0.0323\n",
      "Epoch 721/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4429 - accuracy: 0.8427 - val_loss: 6.4922 - val_accuracy: 0.0323\n",
      "Epoch 722/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4268 - accuracy: 0.8427 - val_loss: 6.4086 - val_accuracy: 0.0323\n",
      "Epoch 723/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3966 - accuracy: 0.8710 - val_loss: 6.3027 - val_accuracy: 0.0323\n",
      "Epoch 724/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3945 - accuracy: 0.8804 - val_loss: 6.3366 - val_accuracy: 0.0323\n",
      "Epoch 725/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3840 - accuracy: 0.8737 - val_loss: 6.3888 - val_accuracy: 0.0323\n",
      "Epoch 726/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3769 - accuracy: 0.8925 - val_loss: 6.4045 - val_accuracy: 0.0323\n",
      "Epoch 727/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4001 - accuracy: 0.8790 - val_loss: 6.4553 - val_accuracy: 0.0323\n",
      "Epoch 728/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3900 - accuracy: 0.8817 - val_loss: 6.5621 - val_accuracy: 0.0323\n",
      "Epoch 729/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4113 - accuracy: 0.8562 - val_loss: 6.5393 - val_accuracy: 0.0323\n",
      "Epoch 730/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4180 - accuracy: 0.8616 - val_loss: 6.4804 - val_accuracy: 0.0323\n",
      "Epoch 731/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4116 - accuracy: 0.8575 - val_loss: 6.5299 - val_accuracy: 0.0323\n",
      "Epoch 732/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4022 - accuracy: 0.8683 - val_loss: 6.4555 - val_accuracy: 0.0323\n",
      "Epoch 733/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4161 - accuracy: 0.8696 - val_loss: 6.4221 - val_accuracy: 0.0323\n",
      "Epoch 734/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3730 - accuracy: 0.8884 - val_loss: 6.5029 - val_accuracy: 0.0323\n",
      "Epoch 735/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3928 - accuracy: 0.8777 - val_loss: 6.5530 - val_accuracy: 0.0323\n",
      "Epoch 736/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3951 - accuracy: 0.8763 - val_loss: 6.5261 - val_accuracy: 0.0323\n",
      "Epoch 737/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4124 - accuracy: 0.8723 - val_loss: 6.4582 - val_accuracy: 0.0323\n",
      "Epoch 738/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3945 - accuracy: 0.8763 - val_loss: 6.4587 - val_accuracy: 0.0323\n",
      "Epoch 739/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3754 - accuracy: 0.8938 - val_loss: 6.5221 - val_accuracy: 0.0323\n",
      "Epoch 740/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3944 - accuracy: 0.8804 - val_loss: 6.5605 - val_accuracy: 0.0323\n",
      "Epoch 741/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3965 - accuracy: 0.8737 - val_loss: 6.6560 - val_accuracy: 0.0323\n",
      "Epoch 742/1000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.3935 - accuracy: 0.8642 - val_loss: 6.6661 - val_accuracy: 0.0323\n",
      "Epoch 743/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4230 - accuracy: 0.8575 - val_loss: 6.5639 - val_accuracy: 0.0323\n",
      "Epoch 744/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4083 - accuracy: 0.8723 - val_loss: 6.5921 - val_accuracy: 0.0323\n",
      "Epoch 745/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4137 - accuracy: 0.8656 - val_loss: 6.5967 - val_accuracy: 0.0323\n",
      "Epoch 746/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3998 - accuracy: 0.8669 - val_loss: 6.6332 - val_accuracy: 0.0323\n",
      "Epoch 747/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3934 - accuracy: 0.8750 - val_loss: 6.6473 - val_accuracy: 0.0323\n",
      "Epoch 748/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4191 - accuracy: 0.8575 - val_loss: 6.6587 - val_accuracy: 0.0323\n",
      "Epoch 749/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3937 - accuracy: 0.8656 - val_loss: 6.5745 - val_accuracy: 0.0323\n",
      "Epoch 750/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4058 - accuracy: 0.8683 - val_loss: 6.5714 - val_accuracy: 0.0323\n",
      "Epoch 751/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4032 - accuracy: 0.8669 - val_loss: 6.6793 - val_accuracy: 0.0323\n",
      "Epoch 752/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3966 - accuracy: 0.8737 - val_loss: 6.6130 - val_accuracy: 0.0323\n",
      "Epoch 753/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3952 - accuracy: 0.8683 - val_loss: 6.6342 - val_accuracy: 0.0323\n",
      "Epoch 754/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4047 - accuracy: 0.8696 - val_loss: 6.6417 - val_accuracy: 0.0323\n",
      "Epoch 755/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3917 - accuracy: 0.8723 - val_loss: 6.6313 - val_accuracy: 0.0323\n",
      "Epoch 756/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3871 - accuracy: 0.8790 - val_loss: 6.6509 - val_accuracy: 0.0323\n",
      "Epoch 757/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3781 - accuracy: 0.8858 - val_loss: 6.6802 - val_accuracy: 0.0323\n",
      "Epoch 758/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3964 - accuracy: 0.8710 - val_loss: 6.6636 - val_accuracy: 0.0323\n",
      "Epoch 759/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4182 - accuracy: 0.8535 - val_loss: 6.6746 - val_accuracy: 0.0323\n",
      "Epoch 760/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4063 - accuracy: 0.8562 - val_loss: 6.6745 - val_accuracy: 0.0323\n",
      "Epoch 761/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4403 - accuracy: 0.8508 - val_loss: 6.7065 - val_accuracy: 0.0323\n",
      "Epoch 762/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4110 - accuracy: 0.8616 - val_loss: 6.7271 - val_accuracy: 0.0323\n",
      "Epoch 763/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4099 - accuracy: 0.8656 - val_loss: 6.7283 - val_accuracy: 0.0323\n",
      "Epoch 764/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3871 - accuracy: 0.8817 - val_loss: 6.7256 - val_accuracy: 0.0323\n",
      "Epoch 765/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3976 - accuracy: 0.8642 - val_loss: 6.7637 - val_accuracy: 0.0323\n",
      "Epoch 766/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3989 - accuracy: 0.8696 - val_loss: 6.8220 - val_accuracy: 0.0323\n",
      "Epoch 767/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4052 - accuracy: 0.8710 - val_loss: 6.8131 - val_accuracy: 0.0323\n",
      "Epoch 768/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3916 - accuracy: 0.8723 - val_loss: 6.7882 - val_accuracy: 0.0323\n",
      "Epoch 769/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4003 - accuracy: 0.8669 - val_loss: 6.8298 - val_accuracy: 0.0323\n",
      "Epoch 770/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3707 - accuracy: 0.8790 - val_loss: 6.8476 - val_accuracy: 0.0323\n",
      "Epoch 771/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4021 - accuracy: 0.8683 - val_loss: 6.8154 - val_accuracy: 0.0323\n",
      "Epoch 772/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3855 - accuracy: 0.8750 - val_loss: 6.8112 - val_accuracy: 0.0323\n",
      "Epoch 773/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3793 - accuracy: 0.8844 - val_loss: 6.8615 - val_accuracy: 0.0323\n",
      "Epoch 774/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3909 - accuracy: 0.8817 - val_loss: 6.8986 - val_accuracy: 0.0323\n",
      "Epoch 775/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3923 - accuracy: 0.8696 - val_loss: 6.8947 - val_accuracy: 0.0323\n",
      "Epoch 776/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3805 - accuracy: 0.8804 - val_loss: 6.8430 - val_accuracy: 0.0323\n",
      "Epoch 777/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3884 - accuracy: 0.8710 - val_loss: 6.8673 - val_accuracy: 0.0323\n",
      "Epoch 778/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3883 - accuracy: 0.8737 - val_loss: 6.8853 - val_accuracy: 0.0323\n",
      "Epoch 779/1000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.3964 - accuracy: 0.8629 - val_loss: 6.8638 - val_accuracy: 0.0323\n",
      "Epoch 780/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3904 - accuracy: 0.8858 - val_loss: 6.9263 - val_accuracy: 0.0323\n",
      "Epoch 781/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4000 - accuracy: 0.8737 - val_loss: 6.9845 - val_accuracy: 0.0323\n",
      "Epoch 782/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3778 - accuracy: 0.8871 - val_loss: 6.9441 - val_accuracy: 0.0323\n",
      "Epoch 783/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3758 - accuracy: 0.8790 - val_loss: 6.9580 - val_accuracy: 0.0323\n",
      "Epoch 784/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3875 - accuracy: 0.8817 - val_loss: 6.9782 - val_accuracy: 0.0323\n",
      "Epoch 785/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3973 - accuracy: 0.8669 - val_loss: 6.9483 - val_accuracy: 0.0323\n",
      "Epoch 786/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4038 - accuracy: 0.8696 - val_loss: 6.8788 - val_accuracy: 0.0323\n",
      "Epoch 787/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3718 - accuracy: 0.8790 - val_loss: 6.8234 - val_accuracy: 0.0323\n",
      "Epoch 788/1000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.4150 - accuracy: 0.8548 - val_loss: 6.9000 - val_accuracy: 0.0430\n",
      "Epoch 789/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3981 - accuracy: 0.8629 - val_loss: 6.9586 - val_accuracy: 0.0323\n",
      "Epoch 790/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4006 - accuracy: 0.8562 - val_loss: 7.0324 - val_accuracy: 0.0323\n",
      "Epoch 791/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4019 - accuracy: 0.8642 - val_loss: 6.9656 - val_accuracy: 0.0323\n",
      "Epoch 792/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3703 - accuracy: 0.8884 - val_loss: 6.9749 - val_accuracy: 0.0323\n",
      "Epoch 793/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3901 - accuracy: 0.8696 - val_loss: 6.9823 - val_accuracy: 0.0323\n",
      "Epoch 794/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3924 - accuracy: 0.8723 - val_loss: 7.0273 - val_accuracy: 0.0323\n",
      "Epoch 795/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3493 - accuracy: 0.8871 - val_loss: 7.0338 - val_accuracy: 0.0323\n",
      "Epoch 796/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3736 - accuracy: 0.8763 - val_loss: 7.0272 - val_accuracy: 0.0323\n",
      "Epoch 797/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3974 - accuracy: 0.8589 - val_loss: 7.0038 - val_accuracy: 0.0323\n",
      "Epoch 798/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4000 - accuracy: 0.8629 - val_loss: 7.0605 - val_accuracy: 0.0323\n",
      "Epoch 799/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3898 - accuracy: 0.8710 - val_loss: 7.0806 - val_accuracy: 0.0323\n",
      "Epoch 800/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3677 - accuracy: 0.8858 - val_loss: 7.0559 - val_accuracy: 0.0323\n",
      "Epoch 801/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3834 - accuracy: 0.8763 - val_loss: 7.1052 - val_accuracy: 0.0484\n",
      "Epoch 802/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3759 - accuracy: 0.8884 - val_loss: 7.1775 - val_accuracy: 0.0323\n",
      "Epoch 803/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3665 - accuracy: 0.8790 - val_loss: 7.1874 - val_accuracy: 0.0323\n",
      "Epoch 804/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3992 - accuracy: 0.8656 - val_loss: 7.1255 - val_accuracy: 0.0323\n",
      "Epoch 805/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3800 - accuracy: 0.8831 - val_loss: 7.1118 - val_accuracy: 0.0323\n",
      "Epoch 806/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3800 - accuracy: 0.8710 - val_loss: 7.1526 - val_accuracy: 0.0323\n",
      "Epoch 807/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3857 - accuracy: 0.8642 - val_loss: 7.1797 - val_accuracy: 0.0323\n",
      "Epoch 808/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3796 - accuracy: 0.8723 - val_loss: 7.1422 - val_accuracy: 0.0323\n",
      "Epoch 809/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3964 - accuracy: 0.8710 - val_loss: 7.0909 - val_accuracy: 0.0430\n",
      "Epoch 810/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3908 - accuracy: 0.8763 - val_loss: 7.1172 - val_accuracy: 0.0323\n",
      "Epoch 811/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3812 - accuracy: 0.8737 - val_loss: 7.3051 - val_accuracy: 0.0323\n",
      "Epoch 812/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4043 - accuracy: 0.8562 - val_loss: 7.2390 - val_accuracy: 0.0323\n",
      "Epoch 813/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4028 - accuracy: 0.8616 - val_loss: 7.1106 - val_accuracy: 0.0430\n",
      "Epoch 814/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4080 - accuracy: 0.8616 - val_loss: 7.0901 - val_accuracy: 0.0323\n",
      "Epoch 815/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3749 - accuracy: 0.8737 - val_loss: 7.0969 - val_accuracy: 0.0323\n",
      "Epoch 816/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3947 - accuracy: 0.8683 - val_loss: 7.1683 - val_accuracy: 0.0323\n",
      "Epoch 817/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4073 - accuracy: 0.8562 - val_loss: 7.1660 - val_accuracy: 0.0323\n",
      "Epoch 818/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3855 - accuracy: 0.8737 - val_loss: 7.1470 - val_accuracy: 0.0323\n",
      "Epoch 819/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3882 - accuracy: 0.8656 - val_loss: 7.1476 - val_accuracy: 0.0484\n",
      "Epoch 820/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3476 - accuracy: 0.8952 - val_loss: 7.1683 - val_accuracy: 0.0323\n",
      "Epoch 821/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3792 - accuracy: 0.8683 - val_loss: 7.2126 - val_accuracy: 0.0538\n",
      "Epoch 822/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3899 - accuracy: 0.8710 - val_loss: 7.2187 - val_accuracy: 0.0323\n",
      "Epoch 823/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3843 - accuracy: 0.8683 - val_loss: 7.1499 - val_accuracy: 0.0484\n",
      "Epoch 824/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3911 - accuracy: 0.8710 - val_loss: 7.1701 - val_accuracy: 0.0323\n",
      "Epoch 825/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3732 - accuracy: 0.8737 - val_loss: 7.2352 - val_accuracy: 0.0323\n",
      "Epoch 826/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3646 - accuracy: 0.8925 - val_loss: 7.2600 - val_accuracy: 0.0323\n",
      "Epoch 827/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4074 - accuracy: 0.8602 - val_loss: 7.3131 - val_accuracy: 0.0323\n",
      "Epoch 828/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3474 - accuracy: 0.8911 - val_loss: 7.3144 - val_accuracy: 0.0323\n",
      "Epoch 829/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4003 - accuracy: 0.8683 - val_loss: 7.3237 - val_accuracy: 0.0323\n",
      "Epoch 830/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4019 - accuracy: 0.8522 - val_loss: 7.3986 - val_accuracy: 0.0323\n",
      "Epoch 831/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4071 - accuracy: 0.8535 - val_loss: 7.3776 - val_accuracy: 0.0430\n",
      "Epoch 832/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3774 - accuracy: 0.8696 - val_loss: 7.3176 - val_accuracy: 0.0323\n",
      "Epoch 833/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3844 - accuracy: 0.8656 - val_loss: 7.2870 - val_accuracy: 0.0484\n",
      "Epoch 834/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4037 - accuracy: 0.8710 - val_loss: 7.3339 - val_accuracy: 0.0323\n",
      "Epoch 835/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3734 - accuracy: 0.8669 - val_loss: 7.3510 - val_accuracy: 0.0430\n",
      "Epoch 836/1000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.3907 - accuracy: 0.8548 - val_loss: 7.3855 - val_accuracy: 0.0323\n",
      "Epoch 837/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3660 - accuracy: 0.8831 - val_loss: 7.3777 - val_accuracy: 0.0430\n",
      "Epoch 838/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3738 - accuracy: 0.8790 - val_loss: 7.3935 - val_accuracy: 0.0376\n",
      "Epoch 839/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3986 - accuracy: 0.8669 - val_loss: 7.3732 - val_accuracy: 0.0538\n",
      "Epoch 840/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3853 - accuracy: 0.8696 - val_loss: 7.3720 - val_accuracy: 0.0323\n",
      "Epoch 841/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4006 - accuracy: 0.8401 - val_loss: 7.4287 - val_accuracy: 0.0323\n",
      "Epoch 842/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3918 - accuracy: 0.8696 - val_loss: 7.3895 - val_accuracy: 0.0323\n",
      "Epoch 843/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3861 - accuracy: 0.8562 - val_loss: 7.3095 - val_accuracy: 0.0538\n",
      "Epoch 844/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3626 - accuracy: 0.8831 - val_loss: 7.3341 - val_accuracy: 0.0323\n",
      "Epoch 845/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3705 - accuracy: 0.8763 - val_loss: 7.4299 - val_accuracy: 0.0323\n",
      "Epoch 846/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4048 - accuracy: 0.8616 - val_loss: 7.5194 - val_accuracy: 0.0484\n",
      "Epoch 847/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3859 - accuracy: 0.8683 - val_loss: 7.4950 - val_accuracy: 0.0323\n",
      "Epoch 848/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3678 - accuracy: 0.8629 - val_loss: 7.5066 - val_accuracy: 0.0323\n",
      "Epoch 849/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3905 - accuracy: 0.8602 - val_loss: 7.5118 - val_accuracy: 0.0323\n",
      "Epoch 850/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3852 - accuracy: 0.8669 - val_loss: 7.5331 - val_accuracy: 0.0323\n",
      "Epoch 851/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3717 - accuracy: 0.8656 - val_loss: 7.5385 - val_accuracy: 0.0323\n",
      "Epoch 852/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3672 - accuracy: 0.8804 - val_loss: 7.6185 - val_accuracy: 0.0323\n",
      "Epoch 853/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3797 - accuracy: 0.8589 - val_loss: 7.5033 - val_accuracy: 0.0323\n",
      "Epoch 854/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3562 - accuracy: 0.8831 - val_loss: 7.3576 - val_accuracy: 0.0484\n",
      "Epoch 855/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3789 - accuracy: 0.8696 - val_loss: 7.4684 - val_accuracy: 0.0323\n",
      "Epoch 856/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3648 - accuracy: 0.8737 - val_loss: 7.5682 - val_accuracy: 0.0323\n",
      "Epoch 857/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3619 - accuracy: 0.8616 - val_loss: 7.6349 - val_accuracy: 0.0323\n",
      "Epoch 858/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3713 - accuracy: 0.8737 - val_loss: 7.6538 - val_accuracy: 0.0323\n",
      "Epoch 859/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3938 - accuracy: 0.8642 - val_loss: 7.6819 - val_accuracy: 0.0323\n",
      "Epoch 860/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3868 - accuracy: 0.8629 - val_loss: 7.6480 - val_accuracy: 0.0430\n",
      "Epoch 861/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3636 - accuracy: 0.8831 - val_loss: 7.6132 - val_accuracy: 0.0323\n",
      "Epoch 862/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3811 - accuracy: 0.8804 - val_loss: 7.6187 - val_accuracy: 0.0323\n",
      "Epoch 863/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3551 - accuracy: 0.8777 - val_loss: 7.6946 - val_accuracy: 0.0323\n",
      "Epoch 864/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3670 - accuracy: 0.8669 - val_loss: 7.6555 - val_accuracy: 0.0323\n",
      "Epoch 865/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3682 - accuracy: 0.8750 - val_loss: 7.6041 - val_accuracy: 0.0323\n",
      "Epoch 866/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3647 - accuracy: 0.8763 - val_loss: 7.6028 - val_accuracy: 0.0430\n",
      "Epoch 867/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3892 - accuracy: 0.8602 - val_loss: 7.5890 - val_accuracy: 0.0323\n",
      "Epoch 868/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3810 - accuracy: 0.8616 - val_loss: 7.6833 - val_accuracy: 0.0323\n",
      "Epoch 869/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3551 - accuracy: 0.8790 - val_loss: 7.6287 - val_accuracy: 0.0484\n",
      "Epoch 870/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3579 - accuracy: 0.8723 - val_loss: 7.6660 - val_accuracy: 0.0323\n",
      "Epoch 871/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3619 - accuracy: 0.8723 - val_loss: 7.7055 - val_accuracy: 0.0484\n",
      "Epoch 872/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3944 - accuracy: 0.8683 - val_loss: 7.6378 - val_accuracy: 0.0323\n",
      "Epoch 873/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.4155 - accuracy: 0.8508 - val_loss: 7.6049 - val_accuracy: 0.0699\n",
      "Epoch 874/1000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.3779 - accuracy: 0.8750 - val_loss: 7.6611 - val_accuracy: 0.0323\n",
      "Epoch 875/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3444 - accuracy: 0.8817 - val_loss: 7.6861 - val_accuracy: 0.0484\n",
      "Epoch 876/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3809 - accuracy: 0.8629 - val_loss: 7.6097 - val_accuracy: 0.0323\n",
      "Epoch 877/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3683 - accuracy: 0.8683 - val_loss: 7.6342 - val_accuracy: 0.0484\n",
      "Epoch 878/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3717 - accuracy: 0.8696 - val_loss: 7.7952 - val_accuracy: 0.0323\n",
      "Epoch 879/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3659 - accuracy: 0.8575 - val_loss: 7.7951 - val_accuracy: 0.0323\n",
      "Epoch 880/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3716 - accuracy: 0.8790 - val_loss: 7.6719 - val_accuracy: 0.0484\n",
      "Epoch 881/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3560 - accuracy: 0.8804 - val_loss: 7.6546 - val_accuracy: 0.0484\n",
      "Epoch 882/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3976 - accuracy: 0.8642 - val_loss: 7.7889 - val_accuracy: 0.0484\n",
      "Epoch 883/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3830 - accuracy: 0.8562 - val_loss: 7.8943 - val_accuracy: 0.0323\n",
      "Epoch 884/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3629 - accuracy: 0.8723 - val_loss: 7.8478 - val_accuracy: 0.0484\n",
      "Epoch 885/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3365 - accuracy: 0.8992 - val_loss: 7.8480 - val_accuracy: 0.0323\n",
      "Epoch 886/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3918 - accuracy: 0.8495 - val_loss: 7.9361 - val_accuracy: 0.0323\n",
      "Epoch 887/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3762 - accuracy: 0.8616 - val_loss: 7.8017 - val_accuracy: 0.0323\n",
      "Epoch 888/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3703 - accuracy: 0.8602 - val_loss: 7.6936 - val_accuracy: 0.0323\n",
      "Epoch 889/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3845 - accuracy: 0.8535 - val_loss: 7.7652 - val_accuracy: 0.0376\n",
      "Epoch 890/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3784 - accuracy: 0.8723 - val_loss: 7.8555 - val_accuracy: 0.0323\n",
      "Epoch 891/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3839 - accuracy: 0.8602 - val_loss: 7.9122 - val_accuracy: 0.0323\n",
      "Epoch 892/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3580 - accuracy: 0.8710 - val_loss: 7.9645 - val_accuracy: 0.0430\n",
      "Epoch 893/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3550 - accuracy: 0.8723 - val_loss: 8.1227 - val_accuracy: 0.0323\n",
      "Epoch 894/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3606 - accuracy: 0.8602 - val_loss: 8.0303 - val_accuracy: 0.0376\n",
      "Epoch 895/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3650 - accuracy: 0.8642 - val_loss: 7.9078 - val_accuracy: 0.0323\n",
      "Epoch 896/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3694 - accuracy: 0.8683 - val_loss: 7.8267 - val_accuracy: 0.0484\n",
      "Epoch 897/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3560 - accuracy: 0.8763 - val_loss: 7.8591 - val_accuracy: 0.0430\n",
      "Epoch 898/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3608 - accuracy: 0.8656 - val_loss: 7.8414 - val_accuracy: 0.0323\n",
      "Epoch 899/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3702 - accuracy: 0.8562 - val_loss: 7.8993 - val_accuracy: 0.0323\n",
      "Epoch 900/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3635 - accuracy: 0.8723 - val_loss: 7.9221 - val_accuracy: 0.0323\n",
      "Epoch 901/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3619 - accuracy: 0.8683 - val_loss: 7.9572 - val_accuracy: 0.0484\n",
      "Epoch 902/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3546 - accuracy: 0.8723 - val_loss: 8.0863 - val_accuracy: 0.0323\n",
      "Epoch 903/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3838 - accuracy: 0.8589 - val_loss: 8.0950 - val_accuracy: 0.0323\n",
      "Epoch 904/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3397 - accuracy: 0.8804 - val_loss: 7.9524 - val_accuracy: 0.0323\n",
      "Epoch 905/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3870 - accuracy: 0.8575 - val_loss: 7.8932 - val_accuracy: 0.0323\n",
      "Epoch 906/1000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.3667 - accuracy: 0.8669 - val_loss: 7.9475 - val_accuracy: 0.0430\n",
      "Epoch 907/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3594 - accuracy: 0.8737 - val_loss: 8.0149 - val_accuracy: 0.0323\n",
      "Epoch 908/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3732 - accuracy: 0.8589 - val_loss: 7.9972 - val_accuracy: 0.0484\n",
      "Epoch 909/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3861 - accuracy: 0.8642 - val_loss: 8.0741 - val_accuracy: 0.0323\n",
      "Epoch 910/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3475 - accuracy: 0.8710 - val_loss: 8.1674 - val_accuracy: 0.0323\n",
      "Epoch 911/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3559 - accuracy: 0.8737 - val_loss: 8.1983 - val_accuracy: 0.0323\n",
      "Epoch 912/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3774 - accuracy: 0.8562 - val_loss: 8.1261 - val_accuracy: 0.0323\n",
      "Epoch 913/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3658 - accuracy: 0.8669 - val_loss: 8.0359 - val_accuracy: 0.0323\n",
      "Epoch 914/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3713 - accuracy: 0.8696 - val_loss: 7.9428 - val_accuracy: 0.0430\n",
      "Epoch 915/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3462 - accuracy: 0.8737 - val_loss: 7.9750 - val_accuracy: 0.0323\n",
      "Epoch 916/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3608 - accuracy: 0.8710 - val_loss: 8.1050 - val_accuracy: 0.0323\n",
      "Epoch 917/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3913 - accuracy: 0.8468 - val_loss: 8.1352 - val_accuracy: 0.0484\n",
      "Epoch 918/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3656 - accuracy: 0.8642 - val_loss: 8.1671 - val_accuracy: 0.0323\n",
      "Epoch 919/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3799 - accuracy: 0.8548 - val_loss: 8.2644 - val_accuracy: 0.0323\n",
      "Epoch 920/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3511 - accuracy: 0.8750 - val_loss: 8.2478 - val_accuracy: 0.0430\n",
      "Epoch 921/1000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.3686 - accuracy: 0.8548 - val_loss: 8.1297 - val_accuracy: 0.0430\n",
      "Epoch 922/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3695 - accuracy: 0.8575 - val_loss: 8.1153 - val_accuracy: 0.0430\n",
      "Epoch 923/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3609 - accuracy: 0.8616 - val_loss: 8.2464 - val_accuracy: 0.0323\n",
      "Epoch 924/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3621 - accuracy: 0.8710 - val_loss: 8.2181 - val_accuracy: 0.0323\n",
      "Epoch 925/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3508 - accuracy: 0.8723 - val_loss: 8.2400 - val_accuracy: 0.0376\n",
      "Epoch 926/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3803 - accuracy: 0.8602 - val_loss: 8.2991 - val_accuracy: 0.0484\n",
      "Epoch 927/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3292 - accuracy: 0.8804 - val_loss: 8.3608 - val_accuracy: 0.0323\n",
      "Epoch 928/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3650 - accuracy: 0.8629 - val_loss: 8.4095 - val_accuracy: 0.0323\n",
      "Epoch 929/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3663 - accuracy: 0.8642 - val_loss: 8.3450 - val_accuracy: 0.0323\n",
      "Epoch 930/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3417 - accuracy: 0.8656 - val_loss: 8.3237 - val_accuracy: 0.0484\n",
      "Epoch 931/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3744 - accuracy: 0.8656 - val_loss: 8.2782 - val_accuracy: 0.0323\n",
      "Epoch 932/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3735 - accuracy: 0.8616 - val_loss: 8.3441 - val_accuracy: 0.0323\n",
      "Epoch 933/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3684 - accuracy: 0.8669 - val_loss: 8.3256 - val_accuracy: 0.0538\n",
      "Epoch 934/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3530 - accuracy: 0.8750 - val_loss: 8.3573 - val_accuracy: 0.0323\n",
      "Epoch 935/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3827 - accuracy: 0.8522 - val_loss: 8.4111 - val_accuracy: 0.0323\n",
      "Epoch 936/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3596 - accuracy: 0.8656 - val_loss: 8.4135 - val_accuracy: 0.0484\n",
      "Epoch 937/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3688 - accuracy: 0.8669 - val_loss: 8.4063 - val_accuracy: 0.0323\n",
      "Epoch 938/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3538 - accuracy: 0.8629 - val_loss: 8.4843 - val_accuracy: 0.0430\n",
      "Epoch 939/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3728 - accuracy: 0.8602 - val_loss: 8.4525 - val_accuracy: 0.0323\n",
      "Epoch 940/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3824 - accuracy: 0.8441 - val_loss: 8.3610 - val_accuracy: 0.0484\n",
      "Epoch 941/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3665 - accuracy: 0.8656 - val_loss: 8.2977 - val_accuracy: 0.0430\n",
      "Epoch 942/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3508 - accuracy: 0.8790 - val_loss: 8.3322 - val_accuracy: 0.0323\n",
      "Epoch 943/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3906 - accuracy: 0.8522 - val_loss: 8.3297 - val_accuracy: 0.0484\n",
      "Epoch 944/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3572 - accuracy: 0.8790 - val_loss: 8.2614 - val_accuracy: 0.0323\n",
      "Epoch 945/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3338 - accuracy: 0.8804 - val_loss: 8.3034 - val_accuracy: 0.0430\n",
      "Epoch 946/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3496 - accuracy: 0.8831 - val_loss: 8.2444 - val_accuracy: 0.0538\n",
      "Epoch 947/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3680 - accuracy: 0.8642 - val_loss: 8.2606 - val_accuracy: 0.0323\n",
      "Epoch 948/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3544 - accuracy: 0.8817 - val_loss: 8.2703 - val_accuracy: 0.0645\n",
      "Epoch 949/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3561 - accuracy: 0.8804 - val_loss: 8.3254 - val_accuracy: 0.0323\n",
      "Epoch 950/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3530 - accuracy: 0.8669 - val_loss: 8.3169 - val_accuracy: 0.0484\n",
      "Epoch 951/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3432 - accuracy: 0.8831 - val_loss: 8.3295 - val_accuracy: 0.0484\n",
      "Epoch 952/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3685 - accuracy: 0.8750 - val_loss: 8.4739 - val_accuracy: 0.0323\n",
      "Epoch 953/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3531 - accuracy: 0.8723 - val_loss: 8.5903 - val_accuracy: 0.0323\n",
      "Epoch 954/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3451 - accuracy: 0.8817 - val_loss: 8.5600 - val_accuracy: 0.0323\n",
      "Epoch 955/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3639 - accuracy: 0.8589 - val_loss: 8.5420 - val_accuracy: 0.0484\n",
      "Epoch 956/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3548 - accuracy: 0.8656 - val_loss: 8.6274 - val_accuracy: 0.0323\n",
      "Epoch 957/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3883 - accuracy: 0.8548 - val_loss: 8.6724 - val_accuracy: 0.0323\n",
      "Epoch 958/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3456 - accuracy: 0.8723 - val_loss: 8.5649 - val_accuracy: 0.0484\n",
      "Epoch 959/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3519 - accuracy: 0.8858 - val_loss: 8.6572 - val_accuracy: 0.0484\n",
      "Epoch 960/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3527 - accuracy: 0.8683 - val_loss: 8.6304 - val_accuracy: 0.0430\n",
      "Epoch 961/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3566 - accuracy: 0.8669 - val_loss: 8.4473 - val_accuracy: 0.0538\n",
      "Epoch 962/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3540 - accuracy: 0.8629 - val_loss: 8.4237 - val_accuracy: 0.0484\n",
      "Epoch 963/1000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.3392 - accuracy: 0.8871 - val_loss: 8.5609 - val_accuracy: 0.0538\n",
      "Epoch 964/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3447 - accuracy: 0.8696 - val_loss: 8.6733 - val_accuracy: 0.0323\n",
      "Epoch 965/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3641 - accuracy: 0.8656 - val_loss: 8.6597 - val_accuracy: 0.0484\n",
      "Epoch 966/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3640 - accuracy: 0.8669 - val_loss: 8.5818 - val_accuracy: 0.0484\n",
      "Epoch 967/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3472 - accuracy: 0.8656 - val_loss: 8.5355 - val_accuracy: 0.0323\n",
      "Epoch 968/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3609 - accuracy: 0.8696 - val_loss: 8.5473 - val_accuracy: 0.0484\n",
      "Epoch 969/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3421 - accuracy: 0.8710 - val_loss: 8.7174 - val_accuracy: 0.0323\n",
      "Epoch 970/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3622 - accuracy: 0.8683 - val_loss: 8.7786 - val_accuracy: 0.0323\n",
      "Epoch 971/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3675 - accuracy: 0.8562 - val_loss: 8.8045 - val_accuracy: 0.0376\n",
      "Epoch 972/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3559 - accuracy: 0.8710 - val_loss: 8.6901 - val_accuracy: 0.0484\n",
      "Epoch 973/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3527 - accuracy: 0.8804 - val_loss: 8.6520 - val_accuracy: 0.0484\n",
      "Epoch 974/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3556 - accuracy: 0.8737 - val_loss: 8.7472 - val_accuracy: 0.0430\n",
      "Epoch 975/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3206 - accuracy: 0.8790 - val_loss: 8.7670 - val_accuracy: 0.0323\n",
      "Epoch 976/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3538 - accuracy: 0.8750 - val_loss: 8.7805 - val_accuracy: 0.0484\n",
      "Epoch 977/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3815 - accuracy: 0.8481 - val_loss: 8.8842 - val_accuracy: 0.0323\n",
      "Epoch 978/1000\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.3606 - accuracy: 0.8696 - val_loss: 8.7712 - val_accuracy: 0.0538\n",
      "Epoch 979/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3224 - accuracy: 0.8871 - val_loss: 8.7703 - val_accuracy: 0.0323\n",
      "Epoch 980/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3618 - accuracy: 0.8777 - val_loss: 8.7537 - val_accuracy: 0.0591\n",
      "Epoch 981/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3204 - accuracy: 0.8925 - val_loss: 8.9165 - val_accuracy: 0.0323\n",
      "Epoch 982/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3378 - accuracy: 0.8858 - val_loss: 8.8746 - val_accuracy: 0.0538\n",
      "Epoch 983/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3712 - accuracy: 0.8696 - val_loss: 8.8340 - val_accuracy: 0.0323\n",
      "Epoch 984/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3599 - accuracy: 0.8710 - val_loss: 8.7991 - val_accuracy: 0.0484\n",
      "Epoch 985/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3550 - accuracy: 0.8723 - val_loss: 8.7737 - val_accuracy: 0.0323\n",
      "Epoch 986/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3546 - accuracy: 0.8710 - val_loss: 8.7644 - val_accuracy: 0.0323\n",
      "Epoch 987/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3457 - accuracy: 0.8723 - val_loss: 8.8039 - val_accuracy: 0.0323\n",
      "Epoch 988/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3575 - accuracy: 0.8562 - val_loss: 8.8114 - val_accuracy: 0.0430\n",
      "Epoch 989/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3687 - accuracy: 0.8548 - val_loss: 8.9473 - val_accuracy: 0.0376\n",
      "Epoch 990/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3486 - accuracy: 0.8696 - val_loss: 8.9045 - val_accuracy: 0.0538\n",
      "Epoch 991/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3774 - accuracy: 0.8629 - val_loss: 8.9138 - val_accuracy: 0.0323\n",
      "Epoch 992/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3317 - accuracy: 0.8871 - val_loss: 8.7105 - val_accuracy: 0.0806\n",
      "Epoch 993/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3519 - accuracy: 0.8777 - val_loss: 8.8406 - val_accuracy: 0.0323\n",
      "Epoch 994/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3513 - accuracy: 0.8750 - val_loss: 8.8825 - val_accuracy: 0.0484\n",
      "Epoch 995/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3628 - accuracy: 0.8616 - val_loss: 8.8580 - val_accuracy: 0.0323\n",
      "Epoch 996/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3472 - accuracy: 0.8763 - val_loss: 8.8010 - val_accuracy: 0.0484\n",
      "Epoch 997/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3456 - accuracy: 0.8642 - val_loss: 8.9547 - val_accuracy: 0.0323\n",
      "Epoch 998/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3411 - accuracy: 0.8763 - val_loss: 8.9092 - val_accuracy: 0.0484\n",
      "Epoch 999/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3484 - accuracy: 0.8737 - val_loss: 8.9524 - val_accuracy: 0.0430\n",
      "Epoch 1000/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3353 - accuracy: 0.8884 - val_loss: 8.9649 - val_accuracy: 0.0484\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0b09877290>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpointer = keras.callbacks.ModelCheckpoint(monitor='val_loss', verbose=1,)\n",
    "nn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# nn_model.fit(x=X_train_nn, y=Y_train, batch_size=100, epochs=1000, verbose=2, validation_data=(X_test_nn, Y_test))\n",
    "nn_model.fit(x=X_Train, y=Y_Train_nn, batch_size=128, epochs=1000, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step - loss: 0.4609 - accuracy: 0.8600\n",
      "5/5 [==============================] - 0s 846us/step\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 1 3 3 3 3 3 3 3 3 1\n",
      " 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "nn_model.evaluate(x=X_Test, y=Y_Test_nn, verbose=1)\n",
    "# nn_model_pred=nn_model.predict_classes(x=X_test_nn, verbose=1)\n",
    "y_prob = nn_model.predict(x=X_Test, verbose=1)\n",
    "nn_model_pred = y_prob.argmax(axis=-1)\n",
    "print(nn_model_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_nn_class = label_convert(Y_Test_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(Y_Test_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "print(Y_test_nn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "NN_df_Y_pred = nn_model_pred.reshape(5,30)\n",
    "result_ann = pd.DataFrame(NN_df_Y_pred, index=row_indices, columns = column_names)\n",
    "result_ann\n",
    "result_ann.replace(0, 'Can', inplace=True)\n",
    "result_ann.replace(1, 'Glass', inplace=True)\n",
    "result_ann.replace(2, 'Plastic', inplace=True)\n",
    "result_ann.replace(3, 'Paper', inplace=True)\n",
    "result_ann.to_csv('/home/joongho/Intelligent_Radar/gui/result/ann_result.csv', sep=',', na_rep='NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy score with neural networks : 0.8600\n"
     ]
    }
   ],
   "source": [
    "print('Model accuracy score with neural networks : {0:0.4f}'. format(accuracy_score(Y_test_nn_class, nn_model_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[60  0  0  0]\n",
      " [ 0 30  0  0]\n",
      " [ 0  0 30  0]\n",
      " [ 0 11 10  9]]\n"
     ]
    }
   ],
   "source": [
    "ANN_cm = confusion_matrix(Y_test_nn_class, nn_model_pred)\n",
    "print(ANN_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(matrix,title):\n",
    "    df=DF(matrix,index=[\"Can\",\"Glass\",\"Plastic\",\"Paper\"],columns=[\"Can\",\"Glass\",\"Plastic\",\"Paper\"])\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.heatmap(df, annot=True)\n",
    "    plt.tick_params(axis='x', top=True, labeltop = True,bottom=False, labelbottom=False)\n",
    "    plt.xticks(np.arange(0.5, len(df.columns), 1), df.columns)\n",
    "    plt.yticks(np.arange(0.5, len(df.index), 1), df.index)\n",
    "    plt.xlabel(\"Prediction\",position = (0.5,1.0+0.05))\n",
    "    plt.ylabel(\"Object\")\n",
    "    plt.title(title) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "title = 'Artificial Neural Network'\n",
    "heatmap(ANN_cm, title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d1fa6df8db067a31b8fd23415ff137a5e0cd08c592e5ad42cba215b538b35518"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('acconeer': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
