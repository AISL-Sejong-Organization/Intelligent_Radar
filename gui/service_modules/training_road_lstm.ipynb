{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-04 08:00:23.867965: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import copy\n",
    "from math import pi\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from pandas import DataFrame as DF\n",
    "from models import AP_ResNet, ResNet, VGG_branch, ResNetLSTM\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from classifier import preprocessing as prep\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['asphalt','bicycle','block','floor','ground']\n",
    "bound = 414"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(matrix,title):\n",
    "    df=DF(matrix, index = class_names, columns = class_names)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.heatmap(df, annot=True)\n",
    "    plt.tick_params(axis='x', top=True, labeltop = True,bottom=False, labelbottom=False)\n",
    "    plt.xticks(np.arange(0.5, len(df.columns), 1), df.columns)\n",
    "    plt.yticks(np.arange(0.5, len(df.index), 1), df.index)\n",
    "    plt.xlabel(\"Prediction\",position = (0.5,1.0+0.05))\n",
    "    plt.ylabel(\"Object\")\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2idx_Dict = {\n",
    "                'asphalt' : 0,\n",
    "                'bicycle' : 1,\n",
    "                'block' : 2,\n",
    "                'floor' : 3,\n",
    "                'ground' : 4,\n",
    "            }\n",
    "\n",
    "idx2label_Dict = {\n",
    "    0 : 'asphalt',\n",
    "    1 : 'bicycle',\n",
    "    2 : 'block',\n",
    "    3 : 'floor',\n",
    "    4 : 'ground',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = './road_data'\n",
    "def readNpy(dir_path):\n",
    "    class_num = len(idx2label_Dict)\n",
    "\n",
    "    Asphalt = list()\n",
    "    Bicycle = list()\n",
    "    Block = list()\n",
    "    Floor = list()\n",
    "    Ground = list()\n",
    "    Asphalt = np.array(Asphalt)\n",
    "    Bicycle = np.array(Bicycle)\n",
    "    Block = np.array(Block)\n",
    "    Floor = np.array(Floor)\n",
    "    Ground = np.array(Ground)\n",
    "    \n",
    "\n",
    "    for dir in os.listdir(dir_path):\n",
    "        d_path = os.path.join(dir_path, dir)\n",
    "        file_list = os.listdir(d_path)\n",
    "        for file in file_list:\n",
    "            file_path = os.path.join(d_path, file)\n",
    "            if dir == idx2label_Dict[0] :\n",
    "                if len(Asphalt) == 0:\n",
    "                    Asphalt = np.load(file_path, allow_pickle=True)\n",
    "                else :\n",
    "                    Asphalt = np.append(Asphalt, np.load(file_path), axis = 0)\n",
    "            elif dir == idx2label_Dict[1]:\n",
    "                if len(Bicycle) == 0:\n",
    "                    Bicycle = np.load(file_path, allow_pickle=True)\n",
    "                else :\n",
    "                    Bicycle = np.append(Bicycle, np.load(file_path), axis = 0)\n",
    "            elif dir == idx2label_Dict[2]:\n",
    "                if len(Block) == 0:\n",
    "                    Block = np.load(file_path, allow_pickle=True)\n",
    "                else:\n",
    "                    Block = np.append(Block, np.load(file_path), axis = 0)\n",
    "            elif dir == idx2label_Dict[3]:\n",
    "                if len(Floor) == 0:\n",
    "                    Floor = np.load(file_path, allow_pickle=True)\n",
    "                else:\n",
    "                    Floor = np.append(Floor, np.load(file_path), axis = 0)\n",
    "            elif dir == idx2label_Dict[4]:\n",
    "                if len(Ground) == 0:\n",
    "                    Ground = np.load(file_path, allow_pickle=True)\n",
    "                else:\n",
    "                    Ground = np.append(Ground, np.load(file_path), axis = 0)\n",
    "\n",
    "    bound = Asphalt.shape[1]\n",
    "\n",
    "    Ground_label = np.full((Ground.shape[0], class_num), np.eye(len(label2idx_Dict))[label2idx_Dict['ground']])\n",
    "    Asphalt_label = np.full((Asphalt.shape[0],class_num), np.eye(len(label2idx_Dict))[label2idx_Dict['asphalt']])\n",
    "    Bicycle_label = np.full((Bicycle.shape[0],class_num), np.eye(len(label2idx_Dict))[label2idx_Dict['bicycle']])\n",
    "    Block_label = np.full((Block.shape[0],class_num), np.eye(len(label2idx_Dict))[label2idx_Dict['block']])\n",
    "    Floor_label = np.full((Floor.shape[0],class_num), np.eye(len(label2idx_Dict))[label2idx_Dict['floor']])\n",
    "\n",
    "    Ground = np.concatenate((Ground, Ground_label), axis=1)\n",
    "    Asphalt = np.concatenate((Asphalt, Asphalt_label), axis=1)\n",
    "    Bicycle = np.concatenate((Bicycle, Bicycle_label), axis=1)\n",
    "    Block = np.concatenate((Block, Block_label), axis=1)\n",
    "    Floor = np.concatenate((Floor, Floor_label), axis=1)\n",
    "    \n",
    "    array = Asphalt\n",
    "    array = np.append(array, Bicycle, axis = 0)\n",
    "    array = np.append(array, Block, axis = 0)\n",
    "    array = np.append(array, Floor, axis = 0)\n",
    "    array = np.append(array, Ground, axis = 0)\n",
    "    s = np.arange(array.shape[0])\n",
    "    np.random.shuffle(s)\n",
    "    array_s = array[s]\n",
    "\n",
    "    X = array_s[:,:bound]\n",
    "    Y = np.real(array_s[:,bound:])\n",
    "    return copy.deepcopy(X), copy.deepcopy(Y)\n",
    "\n",
    "X, Y = readNpy(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperater(arr):\n",
    "    # global maximum\n",
    "    pre_data = arr\n",
    "    amp = np.abs(pre_data)\n",
    "    # amp = amp / maximum\n",
    "    phs = np.angle(pre_data)\n",
    "    # phs = (phs - (- pi)) / (pi - (- pi))\n",
    "    sin = np.sin(phs)\n",
    "    sin = (sin + 1) / 2\n",
    "    seperated_data = np.stack((amp.T,sin.T), axis=0)\n",
    "    seperated_data = np.expand_dims(seperated_data, axis=0)\n",
    "    return np.array(seperated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataSeperator(arr):\n",
    "    temp = copy.deepcopy(seperater(arr[0]))\n",
    "    for i in range(1, len(arr)):\n",
    "        temp = np.concatenate((temp, seperater(arr[i])), axis=0)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Split_X = dataSeperator(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = 'road_detection_lstm'\n",
    "callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = './model/' + checkpoint_filepath,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only = True,\n",
    "    save_weigths_only = False,\n",
    ")\n",
    "log_dir = './logs/fit/'\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir, histogram_freq = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for i in range(len(data[0])-seq_length-1):\n",
    "        x = data[0][:][i:(i+seq_length)]\n",
    "        y = data[1][i+seq_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 10\n",
    "X, Y = create_sequences((Split_X, Y), seq_length = seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2389, 10, 2, 414) (2389, 5)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X, Y, history_dict=None):\n",
    "    Epoch = 100\n",
    "    callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath = './model/' + 'Resnet_LSTM',\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only = True,\n",
    "        save_weigths_only = False,\n",
    "    )\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "    model.compile(optimizer = optimizer , loss = loss, \n",
    "                metrics = ['accuracy', 'categorical_crossentropy'])\n",
    "    history = model.fit(X, Y,  batch_size = 64, epochs = Epoch,\n",
    "                callbacks = callback, validation_split = 0.3)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-04 08:00:30.099096: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-04 08:00:30.617355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22304 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:d8:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-04 08:00:34.737010: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8301\n",
      "2022-08-04 08:00:36.406935: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - ETA: 0s - loss: 3.0238 - accuracy: 0.1866 - categorical_crossentropy: 3.0238"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 18). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/Resnet_LSTM/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/Resnet_LSTM/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 17s 414ms/step - loss: 3.0238 - accuracy: 0.1866 - categorical_crossentropy: 3.0238 - val_loss: 1.6470 - val_accuracy: 0.2357 - val_categorical_crossentropy: 1.6470\n",
      "Epoch 2/100\n",
      "27/27 [==============================] - 3s 97ms/step - loss: 2.7640 - accuracy: 0.2141 - categorical_crossentropy: 2.7640 - val_loss: 1.6577 - val_accuracy: 0.2357 - val_categorical_crossentropy: 1.6577\n",
      "Epoch 3/100\n",
      "27/27 [==============================] - 3s 95ms/step - loss: 2.4903 - accuracy: 0.2321 - categorical_crossentropy: 2.4903 - val_loss: 1.6449 - val_accuracy: 0.2357 - val_categorical_crossentropy: 1.6449\n",
      "Epoch 4/100\n",
      "27/27 [==============================] - ETA: 0s - loss: 2.3764 - accuracy: 0.2434 - categorical_crossentropy: 2.3764"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 18). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/Resnet_LSTM/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/Resnet_LSTM/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 10s 390ms/step - loss: 2.3764 - accuracy: 0.2434 - categorical_crossentropy: 2.3764 - val_loss: 1.6269 - val_accuracy: 0.2371 - val_categorical_crossentropy: 1.6269\n",
      "Epoch 5/100\n",
      "27/27 [==============================] - ETA: 0s - loss: 2.2525 - accuracy: 0.2422 - categorical_crossentropy: 2.2525"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 18). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/Resnet_LSTM/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/Resnet_LSTM/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 10s 395ms/step - loss: 2.2525 - accuracy: 0.2422 - categorical_crossentropy: 2.2525 - val_loss: 1.6235 - val_accuracy: 0.2385 - val_categorical_crossentropy: 1.6235\n",
      "Epoch 6/100\n",
      "27/27 [==============================] - 2s 93ms/step - loss: 2.2161 - accuracy: 0.2255 - categorical_crossentropy: 2.2161 - val_loss: 1.6260 - val_accuracy: 0.2301 - val_categorical_crossentropy: 1.6260\n",
      "Epoch 7/100\n",
      "27/27 [==============================] - 3s 95ms/step - loss: 2.1459 - accuracy: 0.2207 - categorical_crossentropy: 2.1459 - val_loss: 1.6102 - val_accuracy: 0.2301 - val_categorical_crossentropy: 1.6102\n",
      "Epoch 8/100\n",
      "27/27 [==============================] - 3s 98ms/step - loss: 2.0281 - accuracy: 0.2255 - categorical_crossentropy: 2.0281 - val_loss: 1.6043 - val_accuracy: 0.2287 - val_categorical_crossentropy: 1.6043\n",
      "Epoch 9/100\n",
      "27/27 [==============================] - 3s 95ms/step - loss: 1.9329 - accuracy: 0.2392 - categorical_crossentropy: 1.9329 - val_loss: 1.6067 - val_accuracy: 0.2329 - val_categorical_crossentropy: 1.6067\n",
      "Epoch 10/100\n",
      "27/27 [==============================] - 3s 94ms/step - loss: 1.8665 - accuracy: 0.2231 - categorical_crossentropy: 1.8665 - val_loss: 1.6024 - val_accuracy: 0.2357 - val_categorical_crossentropy: 1.6024\n",
      "Epoch 11/100\n",
      "27/27 [==============================] - ETA: 0s - loss: 1.8389 - accuracy: 0.2458 - categorical_crossentropy: 1.8389"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 18). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/Resnet_LSTM/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/Resnet_LSTM/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 10s 389ms/step - loss: 1.8389 - accuracy: 0.2458 - categorical_crossentropy: 1.8389 - val_loss: 1.5975 - val_accuracy: 0.2469 - val_categorical_crossentropy: 1.5975\n",
      "Epoch 12/100\n",
      "27/27 [==============================] - 3s 99ms/step - loss: 1.8550 - accuracy: 0.2177 - categorical_crossentropy: 1.8550 - val_loss: 1.5938 - val_accuracy: 0.2455 - val_categorical_crossentropy: 1.5938\n",
      "Epoch 13/100\n",
      "27/27 [==============================] - 3s 94ms/step - loss: 1.8003 - accuracy: 0.2380 - categorical_crossentropy: 1.8003 - val_loss: 1.5940 - val_accuracy: 0.2385 - val_categorical_crossentropy: 1.5940\n",
      "Epoch 14/100\n",
      "27/27 [==============================] - 3s 96ms/step - loss: 1.7446 - accuracy: 0.2255 - categorical_crossentropy: 1.7446 - val_loss: 1.5910 - val_accuracy: 0.2385 - val_categorical_crossentropy: 1.5910\n",
      "Epoch 15/100\n",
      "27/27 [==============================] - 3s 97ms/step - loss: 1.7131 - accuracy: 0.2416 - categorical_crossentropy: 1.7131 - val_loss: 1.5879 - val_accuracy: 0.2413 - val_categorical_crossentropy: 1.5879\n",
      "Epoch 16/100\n",
      "27/27 [==============================] - 3s 96ms/step - loss: 1.7048 - accuracy: 0.2464 - categorical_crossentropy: 1.7048 - val_loss: 1.5799 - val_accuracy: 0.2399 - val_categorical_crossentropy: 1.5799\n",
      "Epoch 17/100\n",
      "27/27 [==============================] - 3s 97ms/step - loss: 1.6776 - accuracy: 0.2303 - categorical_crossentropy: 1.6776 - val_loss: 1.5802 - val_accuracy: 0.2343 - val_categorical_crossentropy: 1.5802\n",
      "Epoch 18/100\n",
      "27/27 [==============================] - 3s 96ms/step - loss: 1.6720 - accuracy: 0.2231 - categorical_crossentropy: 1.6720 - val_loss: 1.5804 - val_accuracy: 0.2357 - val_categorical_crossentropy: 1.5804\n",
      "Epoch 19/100\n",
      "27/27 [==============================] - 3s 97ms/step - loss: 1.6818 - accuracy: 0.2434 - categorical_crossentropy: 1.6818 - val_loss: 1.5822 - val_accuracy: 0.2343 - val_categorical_crossentropy: 1.5822\n",
      "Epoch 20/100\n",
      "27/27 [==============================] - 3s 96ms/step - loss: 1.6287 - accuracy: 0.2488 - categorical_crossentropy: 1.6287 - val_loss: 1.5800 - val_accuracy: 0.2343 - val_categorical_crossentropy: 1.5800\n",
      "Epoch 21/100\n",
      "27/27 [==============================] - 3s 93ms/step - loss: 1.6258 - accuracy: 0.2368 - categorical_crossentropy: 1.6258 - val_loss: 1.5789 - val_accuracy: 0.2343 - val_categorical_crossentropy: 1.5789\n",
      "Epoch 22/100\n",
      "27/27 [==============================] - 3s 97ms/step - loss: 1.6220 - accuracy: 0.2452 - categorical_crossentropy: 1.6220 - val_loss: 1.5784 - val_accuracy: 0.2343 - val_categorical_crossentropy: 1.5784\n",
      "Epoch 23/100\n",
      "27/27 [==============================] - 3s 94ms/step - loss: 1.6235 - accuracy: 0.2255 - categorical_crossentropy: 1.6235 - val_loss: 1.5789 - val_accuracy: 0.2343 - val_categorical_crossentropy: 1.5789\n",
      "Epoch 24/100\n",
      "27/27 [==============================] - 3s 96ms/step - loss: 1.6192 - accuracy: 0.2344 - categorical_crossentropy: 1.6192 - val_loss: 1.5788 - val_accuracy: 0.2343 - val_categorical_crossentropy: 1.5788\n",
      "Epoch 25/100\n",
      "27/27 [==============================] - 3s 99ms/step - loss: 1.6033 - accuracy: 0.2614 - categorical_crossentropy: 1.6033 - val_loss: 1.5799 - val_accuracy: 0.2343 - val_categorical_crossentropy: 1.5799\n",
      "Epoch 26/100\n",
      "27/27 [==============================] - 3s 93ms/step - loss: 1.5886 - accuracy: 0.2632 - categorical_crossentropy: 1.5886 - val_loss: 1.5825 - val_accuracy: 0.2413 - val_categorical_crossentropy: 1.5825\n",
      "Epoch 27/100\n",
      "27/27 [==============================] - ETA: 0s - loss: 1.5956 - accuracy: 0.2560 - categorical_crossentropy: 1.5956"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 18). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/Resnet_LSTM/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/Resnet_LSTM/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 11s 402ms/step - loss: 1.5956 - accuracy: 0.2560 - categorical_crossentropy: 1.5956 - val_loss: 1.5824 - val_accuracy: 0.2692 - val_categorical_crossentropy: 1.5824\n",
      "Epoch 28/100\n",
      "27/27 [==============================] - 3s 95ms/step - loss: 1.5928 - accuracy: 0.2309 - categorical_crossentropy: 1.5928 - val_loss: 1.5816 - val_accuracy: 0.2245 - val_categorical_crossentropy: 1.5816\n",
      "Epoch 29/100\n",
      "27/27 [==============================] - 3s 99ms/step - loss: 1.5744 - accuracy: 0.2566 - categorical_crossentropy: 1.5744 - val_loss: 1.5808 - val_accuracy: 0.2497 - val_categorical_crossentropy: 1.5808\n",
      "Epoch 30/100\n",
      "27/27 [==============================] - 3s 99ms/step - loss: 1.5779 - accuracy: 0.2500 - categorical_crossentropy: 1.5779 - val_loss: 1.5786 - val_accuracy: 0.2469 - val_categorical_crossentropy: 1.5786\n",
      "Epoch 31/100\n",
      "27/27 [==============================] - 3s 94ms/step - loss: 1.5858 - accuracy: 0.2614 - categorical_crossentropy: 1.5858 - val_loss: 1.5790 - val_accuracy: 0.2399 - val_categorical_crossentropy: 1.5790\n",
      "Epoch 32/100\n",
      "27/27 [==============================] - 3s 99ms/step - loss: 1.5791 - accuracy: 0.2494 - categorical_crossentropy: 1.5791 - val_loss: 1.5794 - val_accuracy: 0.2455 - val_categorical_crossentropy: 1.5794\n",
      "Epoch 33/100\n",
      "27/27 [==============================] - 3s 101ms/step - loss: 1.5759 - accuracy: 0.2566 - categorical_crossentropy: 1.5759 - val_loss: 1.5781 - val_accuracy: 0.2510 - val_categorical_crossentropy: 1.5781\n",
      "Epoch 34/100\n",
      "27/27 [==============================] - 3s 97ms/step - loss: 1.5716 - accuracy: 0.2578 - categorical_crossentropy: 1.5716 - val_loss: 1.5781 - val_accuracy: 0.2594 - val_categorical_crossentropy: 1.5781\n",
      "Epoch 35/100\n",
      "27/27 [==============================] - 3s 99ms/step - loss: 1.5748 - accuracy: 0.2679 - categorical_crossentropy: 1.5748 - val_loss: 1.5774 - val_accuracy: 0.2455 - val_categorical_crossentropy: 1.5774\n",
      "Epoch 36/100\n",
      "27/27 [==============================] - 3s 97ms/step - loss: 1.5822 - accuracy: 0.2356 - categorical_crossentropy: 1.5822 - val_loss: 1.5772 - val_accuracy: 0.2455 - val_categorical_crossentropy: 1.5772\n",
      "Epoch 37/100\n",
      "27/27 [==============================] - 3s 101ms/step - loss: 1.5729 - accuracy: 0.2590 - categorical_crossentropy: 1.5729 - val_loss: 1.5776 - val_accuracy: 0.2441 - val_categorical_crossentropy: 1.5776\n",
      "Epoch 38/100\n",
      "27/27 [==============================] - 3s 100ms/step - loss: 1.5652 - accuracy: 0.2488 - categorical_crossentropy: 1.5652 - val_loss: 1.5774 - val_accuracy: 0.2413 - val_categorical_crossentropy: 1.5774\n",
      "Epoch 39/100\n",
      "27/27 [==============================] - 3s 94ms/step - loss: 1.5772 - accuracy: 0.2542 - categorical_crossentropy: 1.5772 - val_loss: 1.5774 - val_accuracy: 0.2399 - val_categorical_crossentropy: 1.5774\n",
      "Epoch 40/100\n",
      "27/27 [==============================] - 3s 95ms/step - loss: 1.5711 - accuracy: 0.2518 - categorical_crossentropy: 1.5711 - val_loss: 1.5778 - val_accuracy: 0.2385 - val_categorical_crossentropy: 1.5778\n",
      "Epoch 41/100\n",
      "27/27 [==============================] - 3s 98ms/step - loss: 1.5670 - accuracy: 0.2458 - categorical_crossentropy: 1.5670 - val_loss: 1.5771 - val_accuracy: 0.2413 - val_categorical_crossentropy: 1.5771\n",
      "Epoch 42/100\n",
      "27/27 [==============================] - 3s 98ms/step - loss: 1.5687 - accuracy: 0.2398 - categorical_crossentropy: 1.5687 - val_loss: 1.5758 - val_accuracy: 0.2385 - val_categorical_crossentropy: 1.5758\n",
      "Epoch 43/100\n",
      "27/27 [==============================] - 3s 96ms/step - loss: 1.5728 - accuracy: 0.2697 - categorical_crossentropy: 1.5728 - val_loss: 1.5756 - val_accuracy: 0.2441 - val_categorical_crossentropy: 1.5756\n",
      "Epoch 44/100\n",
      "27/27 [==============================] - 3s 97ms/step - loss: 1.5611 - accuracy: 0.2679 - categorical_crossentropy: 1.5611 - val_loss: 1.5754 - val_accuracy: 0.2441 - val_categorical_crossentropy: 1.5754\n",
      "Epoch 45/100\n",
      "27/27 [==============================] - 3s 96ms/step - loss: 1.5779 - accuracy: 0.2374 - categorical_crossentropy: 1.5779 - val_loss: 1.5759 - val_accuracy: 0.2413 - val_categorical_crossentropy: 1.5759\n",
      "Epoch 46/100\n",
      "27/27 [==============================] - 3s 98ms/step - loss: 1.5623 - accuracy: 0.2602 - categorical_crossentropy: 1.5623 - val_loss: 1.5770 - val_accuracy: 0.2176 - val_categorical_crossentropy: 1.5770\n",
      "Epoch 47/100\n",
      "27/27 [==============================] - 3s 99ms/step - loss: 1.5642 - accuracy: 0.2691 - categorical_crossentropy: 1.5642 - val_loss: 1.5770 - val_accuracy: 0.2287 - val_categorical_crossentropy: 1.5770\n",
      "Epoch 48/100\n",
      "27/27 [==============================] - 3s 96ms/step - loss: 1.5673 - accuracy: 0.2482 - categorical_crossentropy: 1.5673 - val_loss: 1.5763 - val_accuracy: 0.2287 - val_categorical_crossentropy: 1.5763\n",
      "Epoch 49/100\n",
      "27/27 [==============================] - 3s 94ms/step - loss: 1.5695 - accuracy: 0.2584 - categorical_crossentropy: 1.5695 - val_loss: 1.5754 - val_accuracy: 0.2427 - val_categorical_crossentropy: 1.5754\n",
      "Epoch 50/100\n",
      "27/27 [==============================] - 3s 93ms/step - loss: 1.5703 - accuracy: 0.2482 - categorical_crossentropy: 1.5703 - val_loss: 1.5755 - val_accuracy: 0.2357 - val_categorical_crossentropy: 1.5755\n",
      "Epoch 51/100\n",
      "27/27 [==============================] - 3s 98ms/step - loss: 1.5714 - accuracy: 0.2512 - categorical_crossentropy: 1.5714 - val_loss: 1.5752 - val_accuracy: 0.2343 - val_categorical_crossentropy: 1.5752\n",
      "Epoch 52/100\n",
      "27/27 [==============================] - 3s 97ms/step - loss: 1.5695 - accuracy: 0.2398 - categorical_crossentropy: 1.5695 - val_loss: 1.5755 - val_accuracy: 0.2455 - val_categorical_crossentropy: 1.5755\n",
      "Epoch 53/100\n",
      "27/27 [==============================] - 3s 99ms/step - loss: 1.5691 - accuracy: 0.2446 - categorical_crossentropy: 1.5691 - val_loss: 1.5750 - val_accuracy: 0.2552 - val_categorical_crossentropy: 1.5750\n",
      "Epoch 54/100\n",
      "27/27 [==============================] - 3s 99ms/step - loss: 1.5671 - accuracy: 0.2691 - categorical_crossentropy: 1.5671 - val_loss: 1.5742 - val_accuracy: 0.2441 - val_categorical_crossentropy: 1.5742\n",
      "Epoch 55/100\n",
      "27/27 [==============================] - 3s 99ms/step - loss: 1.5640 - accuracy: 0.2679 - categorical_crossentropy: 1.5640 - val_loss: 1.5736 - val_accuracy: 0.2580 - val_categorical_crossentropy: 1.5736\n",
      "Epoch 56/100\n",
      "27/27 [==============================] - 3s 99ms/step - loss: 1.5661 - accuracy: 0.2614 - categorical_crossentropy: 1.5661 - val_loss: 1.5721 - val_accuracy: 0.2510 - val_categorical_crossentropy: 1.5721\n",
      "Epoch 57/100\n",
      "27/27 [==============================] - 3s 98ms/step - loss: 1.5678 - accuracy: 0.2494 - categorical_crossentropy: 1.5678 - val_loss: 1.5723 - val_accuracy: 0.2524 - val_categorical_crossentropy: 1.5723\n",
      "Epoch 58/100\n",
      "27/27 [==============================] - 3s 97ms/step - loss: 1.5689 - accuracy: 0.2422 - categorical_crossentropy: 1.5689 - val_loss: 1.5721 - val_accuracy: 0.2441 - val_categorical_crossentropy: 1.5721\n",
      "Epoch 59/100\n",
      "27/27 [==============================] - 3s 96ms/step - loss: 1.5696 - accuracy: 0.2362 - categorical_crossentropy: 1.5696 - val_loss: 1.5724 - val_accuracy: 0.2455 - val_categorical_crossentropy: 1.5724\n",
      "Epoch 60/100\n",
      "27/27 [==============================] - 3s 93ms/step - loss: 1.5679 - accuracy: 0.2536 - categorical_crossentropy: 1.5679 - val_loss: 1.5720 - val_accuracy: 0.2441 - val_categorical_crossentropy: 1.5720\n",
      "Epoch 61/100\n",
      "27/27 [==============================] - 2s 93ms/step - loss: 1.5634 - accuracy: 0.2548 - categorical_crossentropy: 1.5634 - val_loss: 1.5722 - val_accuracy: 0.2441 - val_categorical_crossentropy: 1.5722\n",
      "Epoch 62/100\n",
      "27/27 [==============================] - 2s 90ms/step - loss: 1.5642 - accuracy: 0.2548 - categorical_crossentropy: 1.5642 - val_loss: 1.5723 - val_accuracy: 0.2301 - val_categorical_crossentropy: 1.5723\n",
      "Epoch 63/100\n",
      "27/27 [==============================] - 3s 93ms/step - loss: 1.5698 - accuracy: 0.2638 - categorical_crossentropy: 1.5698 - val_loss: 1.5719 - val_accuracy: 0.2441 - val_categorical_crossentropy: 1.5719\n",
      "Epoch 64/100\n",
      "27/27 [==============================] - 2s 93ms/step - loss: 1.5647 - accuracy: 0.2626 - categorical_crossentropy: 1.5647 - val_loss: 1.5711 - val_accuracy: 0.2315 - val_categorical_crossentropy: 1.5711\n",
      "Epoch 65/100\n",
      "27/27 [==============================] - 3s 94ms/step - loss: 1.5663 - accuracy: 0.2548 - categorical_crossentropy: 1.5663 - val_loss: 1.5713 - val_accuracy: 0.2343 - val_categorical_crossentropy: 1.5713\n",
      "Epoch 66/100\n",
      "27/27 [==============================] - 3s 94ms/step - loss: 1.5682 - accuracy: 0.2452 - categorical_crossentropy: 1.5682 - val_loss: 1.5718 - val_accuracy: 0.2399 - val_categorical_crossentropy: 1.5718\n",
      "Epoch 67/100\n",
      "27/27 [==============================] - 3s 100ms/step - loss: 1.5674 - accuracy: 0.2524 - categorical_crossentropy: 1.5674 - val_loss: 1.5719 - val_accuracy: 0.2441 - val_categorical_crossentropy: 1.5719\n",
      "Epoch 68/100\n",
      "27/27 [==============================] - 3s 97ms/step - loss: 1.5678 - accuracy: 0.2416 - categorical_crossentropy: 1.5678 - val_loss: 1.5726 - val_accuracy: 0.2441 - val_categorical_crossentropy: 1.5726\n",
      "Epoch 69/100\n",
      "27/27 [==============================] - 3s 98ms/step - loss: 1.5634 - accuracy: 0.2566 - categorical_crossentropy: 1.5634 - val_loss: 1.5723 - val_accuracy: 0.2441 - val_categorical_crossentropy: 1.5723\n",
      "Epoch 70/100\n",
      "27/27 [==============================] - 3s 98ms/step - loss: 1.5626 - accuracy: 0.2566 - categorical_crossentropy: 1.5626 - val_loss: 1.5731 - val_accuracy: 0.2441 - val_categorical_crossentropy: 1.5731\n",
      "Epoch 71/100\n",
      "27/27 [==============================] - 3s 94ms/step - loss: 1.5626 - accuracy: 0.2542 - categorical_crossentropy: 1.5626 - val_loss: 1.5727 - val_accuracy: 0.2441 - val_categorical_crossentropy: 1.5727\n",
      "Epoch 72/100\n",
      "27/27 [==============================] - 3s 99ms/step - loss: 1.5657 - accuracy: 0.2584 - categorical_crossentropy: 1.5657 - val_loss: 1.5726 - val_accuracy: 0.2441 - val_categorical_crossentropy: 1.5726\n",
      "Epoch 73/100\n",
      "27/27 [==============================] - 3s 97ms/step - loss: 1.5719 - accuracy: 0.2333 - categorical_crossentropy: 1.5719 - val_loss: 1.5725 - val_accuracy: 0.2441 - val_categorical_crossentropy: 1.5725\n",
      "Epoch 74/100\n",
      "27/27 [==============================] - 3s 99ms/step - loss: 1.5632 - accuracy: 0.2488 - categorical_crossentropy: 1.5632 - val_loss: 1.5723 - val_accuracy: 0.2441 - val_categorical_crossentropy: 1.5723\n",
      "Epoch 75/100\n",
      "27/27 [==============================] - 3s 96ms/step - loss: 1.5674 - accuracy: 0.2536 - categorical_crossentropy: 1.5674 - val_loss: 1.5721 - val_accuracy: 0.2371 - val_categorical_crossentropy: 1.5721\n",
      "Epoch 76/100\n",
      "27/27 [==============================] - 3s 97ms/step - loss: 1.5692 - accuracy: 0.2560 - categorical_crossentropy: 1.5692 - val_loss: 1.5716 - val_accuracy: 0.2371 - val_categorical_crossentropy: 1.5716\n",
      "Epoch 77/100\n",
      "27/27 [==============================] - 3s 103ms/step - loss: 1.5661 - accuracy: 0.2476 - categorical_crossentropy: 1.5661 - val_loss: 1.5715 - val_accuracy: 0.2343 - val_categorical_crossentropy: 1.5715\n",
      "Epoch 78/100\n",
      "27/27 [==============================] - 3s 104ms/step - loss: 1.5616 - accuracy: 0.2554 - categorical_crossentropy: 1.5616 - val_loss: 1.5717 - val_accuracy: 0.2441 - val_categorical_crossentropy: 1.5717\n",
      "Epoch 79/100\n",
      "27/27 [==============================] - 3s 102ms/step - loss: 1.5647 - accuracy: 0.2626 - categorical_crossentropy: 1.5647 - val_loss: 1.5715 - val_accuracy: 0.2441 - val_categorical_crossentropy: 1.5715\n",
      "Epoch 80/100\n",
      "27/27 [==============================] - 3s 99ms/step - loss: 1.5638 - accuracy: 0.2518 - categorical_crossentropy: 1.5638 - val_loss: 1.5719 - val_accuracy: 0.2413 - val_categorical_crossentropy: 1.5719\n",
      "Epoch 81/100\n",
      "27/27 [==============================] - 3s 97ms/step - loss: 1.5644 - accuracy: 0.2512 - categorical_crossentropy: 1.5644 - val_loss: 1.5722 - val_accuracy: 0.2413 - val_categorical_crossentropy: 1.5722\n",
      "Epoch 82/100\n",
      "27/27 [==============================] - 3s 94ms/step - loss: 1.5598 - accuracy: 0.2458 - categorical_crossentropy: 1.5598 - val_loss: 1.5715 - val_accuracy: 0.2329 - val_categorical_crossentropy: 1.5715\n",
      "Epoch 83/100\n",
      "27/27 [==============================] - 3s 101ms/step - loss: 1.5698 - accuracy: 0.2344 - categorical_crossentropy: 1.5698 - val_loss: 1.5719 - val_accuracy: 0.2566 - val_categorical_crossentropy: 1.5719\n",
      "Epoch 84/100\n",
      "27/27 [==============================] - 3s 98ms/step - loss: 1.5701 - accuracy: 0.2374 - categorical_crossentropy: 1.5701 - val_loss: 1.5717 - val_accuracy: 0.2483 - val_categorical_crossentropy: 1.5717\n",
      "Epoch 85/100\n",
      "27/27 [==============================] - 3s 97ms/step - loss: 1.5673 - accuracy: 0.2542 - categorical_crossentropy: 1.5673 - val_loss: 1.5716 - val_accuracy: 0.2594 - val_categorical_crossentropy: 1.5716\n",
      "Epoch 86/100\n",
      "27/27 [==============================] - 3s 94ms/step - loss: 1.5665 - accuracy: 0.2721 - categorical_crossentropy: 1.5665 - val_loss: 1.5713 - val_accuracy: 0.2497 - val_categorical_crossentropy: 1.5713\n",
      "Epoch 87/100\n",
      "27/27 [==============================] - 3s 94ms/step - loss: 1.5666 - accuracy: 0.2578 - categorical_crossentropy: 1.5666 - val_loss: 1.5712 - val_accuracy: 0.2510 - val_categorical_crossentropy: 1.5712\n",
      "Epoch 88/100\n",
      "27/27 [==============================] - 3s 98ms/step - loss: 1.5637 - accuracy: 0.2518 - categorical_crossentropy: 1.5637 - val_loss: 1.5712 - val_accuracy: 0.2427 - val_categorical_crossentropy: 1.5712\n",
      "Epoch 89/100\n",
      "27/27 [==============================] - 3s 100ms/step - loss: 1.5708 - accuracy: 0.2590 - categorical_crossentropy: 1.5708 - val_loss: 1.5714 - val_accuracy: 0.2385 - val_categorical_crossentropy: 1.5714\n",
      "Epoch 90/100\n",
      "27/27 [==============================] - 3s 98ms/step - loss: 1.5693 - accuracy: 0.2356 - categorical_crossentropy: 1.5693 - val_loss: 1.5716 - val_accuracy: 0.2441 - val_categorical_crossentropy: 1.5716\n",
      "Epoch 91/100\n",
      "27/27 [==============================] - 3s 98ms/step - loss: 1.5631 - accuracy: 0.2482 - categorical_crossentropy: 1.5631 - val_loss: 1.5711 - val_accuracy: 0.2441 - val_categorical_crossentropy: 1.5711\n",
      "Epoch 92/100\n",
      "27/27 [==============================] - 3s 98ms/step - loss: 1.5630 - accuracy: 0.2416 - categorical_crossentropy: 1.5630 - val_loss: 1.5715 - val_accuracy: 0.2329 - val_categorical_crossentropy: 1.5715\n",
      "Epoch 93/100\n",
      "27/27 [==============================] - 3s 101ms/step - loss: 1.5631 - accuracy: 0.2518 - categorical_crossentropy: 1.5631 - val_loss: 1.5715 - val_accuracy: 0.2441 - val_categorical_crossentropy: 1.5715\n",
      "Epoch 94/100\n",
      "27/27 [==============================] - 3s 95ms/step - loss: 1.5631 - accuracy: 0.2739 - categorical_crossentropy: 1.5631 - val_loss: 1.5716 - val_accuracy: 0.2441 - val_categorical_crossentropy: 1.5716\n",
      "Epoch 95/100\n",
      "27/27 [==============================] - 3s 99ms/step - loss: 1.5710 - accuracy: 0.2225 - categorical_crossentropy: 1.5710 - val_loss: 1.5717 - val_accuracy: 0.2441 - val_categorical_crossentropy: 1.5717\n",
      "Epoch 96/100\n",
      "27/27 [==============================] - 3s 96ms/step - loss: 1.5633 - accuracy: 0.2470 - categorical_crossentropy: 1.5633 - val_loss: 1.5717 - val_accuracy: 0.2441 - val_categorical_crossentropy: 1.5717\n",
      "Epoch 97/100\n",
      "27/27 [==============================] - 3s 97ms/step - loss: 1.5595 - accuracy: 0.2721 - categorical_crossentropy: 1.5595 - val_loss: 1.5715 - val_accuracy: 0.2441 - val_categorical_crossentropy: 1.5715\n",
      "Epoch 98/100\n",
      "27/27 [==============================] - 3s 98ms/step - loss: 1.5733 - accuracy: 0.2500 - categorical_crossentropy: 1.5733 - val_loss: 1.5717 - val_accuracy: 0.2441 - val_categorical_crossentropy: 1.5717\n",
      "Epoch 99/100\n",
      "27/27 [==============================] - 3s 94ms/step - loss: 1.5672 - accuracy: 0.2488 - categorical_crossentropy: 1.5672 - val_loss: 1.5711 - val_accuracy: 0.2441 - val_categorical_crossentropy: 1.5711\n",
      "Epoch 100/100\n",
      "27/27 [==============================] - 3s 98ms/step - loss: 1.5659 - accuracy: 0.2452 - categorical_crossentropy: 1.5659 - val_loss: 1.5708 - val_accuracy: 0.2427 - val_categorical_crossentropy: 1.5708\n"
     ]
    }
   ],
   "source": [
    "model = ResNetLSTM()\n",
    "history = train(model, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"res_net_lstm\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             multiple                  32        \n",
      "                                                                 \n",
      " resnet_layer2d (ResnetLayer  multiple                 3120      \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  multiple                 0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " resnet_layer2d_1 (ResnetLay  multiple                 11872     \n",
      " er2D)                                                           \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  multiple                 0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " resnet_layer2d_2 (ResnetLay  multiple                 46272     \n",
      " er2D)                                                           \n",
      "                                                                 \n",
      " flatten (Flatten)           multiple                  0         \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  32896     \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  645       \n",
      "                                                                 \n",
      " lstm (LSTM)                 multiple                  328704    \n",
      "                                                                 \n",
      " reshape (Reshape)           multiple                  0         \n",
      "                                                                 \n",
      " dropout (Dropout)           multiple                  0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 423,541\n",
      "Trainable params: 422,757\n",
      "Non-trainable params: 784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('radar')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dea347e98226a0e23d2c967d9a0ab2a62ac04f9a7895aa5011026f33e6be1d75"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
